"""
é€»è¾‘å›å½’ (Logistic Regression)

é—®é¢˜ï¼šä»å›å½’åˆ°åˆ†ç±» - å¦‚ä½•é¢„æµ‹ç¦»æ•£ç±»åˆ«ï¼Ÿ
ç›®æ ‡ï¼šç†è§£é€»è¾‘å›å½’çš„åŸç†å’Œå®ç°äºŒåˆ†ç±»ä»»åŠ¡

æ ¸å¿ƒæ¦‚å¿µï¼š
1. Sigmoidå‡½æ•°ï¼šå°†çº¿æ€§è¾“å‡º z = wx + b æ˜ å°„åˆ° [0, 1] æ¦‚ç‡
   Ïƒ(z) = 1 / (1 + e^(-z))
2. äº¤å‰ç†µæŸå¤±ï¼šåˆ†ç±»é—®é¢˜çš„æŸå¤±å‡½æ•°ï¼ˆä¸å†æ˜¯MSEï¼‰
   Loss = -[y*log(p) + (1-y)*log(1-p)]
3. å†³ç­–è¾¹ç•Œï¼šåˆ†ç±»çš„åˆ†ç•Œçº¿ï¼Œå½“ p = 0.5 æ—¶çš„è¾¹ç•Œ
"""

import numpy as np
import matplotlib.pyplot as plt
from sklearn.datasets import make_classification
from sklearn.model_selection import train_test_split


# ==================== 1. æ ¸å¿ƒå‡½æ•° ====================
def sigmoid(z):
    """
    Sigmoid æ¿€æ´»å‡½æ•°

    ä½œç”¨ï¼šå°†ä»»æ„å®æ•°æ˜ å°„åˆ° (0, 1) åŒºé—´ï¼Œè¡¨ç¤ºæ¦‚ç‡

    æ€§è´¨ï¼š
    - è¾“å‡ºèŒƒå›´ï¼š(0, 1)
    - åœ¨ z=0 å¤„ï¼ŒÏƒ(0) = 0.5
    - å•è°ƒé€’å¢
    - å¯¼æ•°ï¼šÏƒ'(z) = Ïƒ(z) * (1 - Ïƒ(z))

    ä¾‹å­ï¼š
    - z = 0   â†’ Ïƒ(z) = 0.5   (ä¸ç¡®å®š)
    - z = 5   â†’ Ïƒ(z) â‰ˆ 0.993 (éå¸¸ç¡®å®šæ˜¯æ­£ç±»)
    - z = -5  â†’ Ïƒ(z) â‰ˆ 0.007 (éå¸¸ç¡®å®šæ˜¯è´Ÿç±»)
    """
    return 1 / (1 + np.exp(-z))


def binary_cross_entropy(y_true, y_pred, epsilon=1e-15):
    """
    äºŒåˆ†ç±»äº¤å‰ç†µæŸå¤±

    ä¸ºä»€ä¹ˆä¸ç”¨ MSEï¼Ÿ
    - MSE ç”¨äºå›å½’ï¼Œå¯¹åˆ†ç±»é—®é¢˜ä¼˜åŒ–æ•ˆæœå·®
    - äº¤å‰ç†µèƒ½æ›´å¥½åœ°è¡¡é‡æ¦‚ç‡åˆ†å¸ƒçš„å·®å¼‚

    å…¬å¼ï¼š
    Loss = -1/n * Î£[y*log(p) + (1-y)*log(1-p)]

    å…¶ä¸­ï¼š
    - y: çœŸå®æ ‡ç­¾ (0 æˆ– 1)
    - p: é¢„æµ‹æ¦‚ç‡ (0 åˆ° 1)

    epsilon: é˜²æ­¢ log(0) å‡ºç°ï¼ˆnp.clip è£å‰ªåˆ° [epsilon, 1-epsilon]ï¼‰

    ============================================================================
    ğŸ”‘ æ ¸å¿ƒä¼˜åŠ¿ï¼šäº¤å‰ç†µ vs MSE çš„æ¢¯åº¦å¯¹æ¯”
    ============================================================================

    å¯¹äºé€»è¾‘å›å½’ï¼šz = wx + b, p = sigmoid(z)

    ã€ä½¿ç”¨äº¤å‰ç†µã€‘
    Loss = -[y*log(p) + (1-y)*log(1-p)]

    æ¢¯åº¦æ¨å¯¼ï¼ˆé“¾å¼æ³•åˆ™ï¼‰ï¼š
    âˆ‚Loss/âˆ‚w = (âˆ‚Loss/âˆ‚p) Â· (âˆ‚p/âˆ‚z) Â· (âˆ‚z/âˆ‚w)
             = [(p-y)/(p(1-p))] Â· [p(1-p)] Â· x
             = (p - y) Â· x  â† p(1-p) è¢«çº¦æ‰äº†ï¼

    âœ… ä¼˜åŠ¿ï¼š
    1. æ¢¯åº¦å½¢å¼æç®€ï¼šâˆ‚Loss/âˆ‚w = (p - y) Â· x
    2. é¿å…æ¢¯åº¦æ¶ˆå¤±ï¼šsigmoid å¯¼æ•° p(1-p) è¢«æŠµæ¶ˆ
    3. é”™è¯¯è¶Šå¤§ï¼Œæ¢¯åº¦è¶Šå¤§ï¼Œå­¦ä¹ è¶Šå¿«

    ã€å¦‚æœä½¿ç”¨ MSEã€‘
    Loss = (y - p)Â²

    æ¢¯åº¦æ¨å¯¼ï¼š
    âˆ‚Loss/âˆ‚w = 2(p - y) Â· p(1-p) Â· x  â† ä¿ç•™äº† p(1-p)ï¼

    âŒ é—®é¢˜ï¼š
    1. å½“ p æ¥è¿‘ 0 æˆ– 1 æ—¶ï¼Œp(1-p) â†’ 0
    2. å¯¼è‡´æ¢¯åº¦æ¶ˆå¤±ï¼Œå­¦ä¹ åœæ»
    3. è®­ç»ƒé€Ÿåº¦æ…¢ï¼Œå®¹æ˜“é™·å…¥å±€éƒ¨æœ€ä¼˜

    ã€æ•°å€¼ä¾‹å­ã€‘
    å‡è®¾ï¼šy=1ï¼ˆçœŸå®æ­£ç±»ï¼‰ï¼Œp=0.2ï¼ˆé¢„æµ‹é”™äº†ï¼‰ï¼Œx=2

    äº¤å‰ç†µæ¢¯åº¦ï¼šâˆ‚L/âˆ‚w = (0.2 - 1) Ã— 2 = -1.6ï¼ˆæ¢¯åº¦å¤§ï¼Œå¿«é€Ÿä¿®æ­£ï¼‰
    MSEæ¢¯åº¦ï¼š    âˆ‚L/âˆ‚w = 2(0.2-1) Ã— 0.2Ã—0.8 Ã— 2 = -0.512ï¼ˆæ¢¯åº¦è¢«å‰Šå¼±ï¼ï¼‰

    ç»“è®ºï¼šäº¤å‰ç†µ + Sigmoid æ˜¯åˆ†ç±»é—®é¢˜çš„é»„é‡‘ç»„åˆï¼
    ============================================================================
    """
    # è£å‰ªé¢„æµ‹å€¼ï¼Œé¿å… log(0)
    y_pred = np.clip(y_pred, epsilon, 1 - epsilon)

    # è®¡ç®—äº¤å‰ç†µ
    loss = -np.mean(y_true * np.log(y_pred) + (1 - y_true) * np.log(1 - y_pred))
    return loss


# ==================== 2. é€»è¾‘å›å½’ç±» ====================
class LogisticRegression:
    """ä»é›¶å®ç°é€»è¾‘å›å½’"""

    def __init__(self, learning_rate=0.01, n_epochs=1000, batch_size=32):
        """
        å‚æ•°ï¼š
            learning_rate: å­¦ä¹ ç‡
            n_epochs: è®­ç»ƒè½®æ•°
            batch_size: æ‰¹é‡å¤§å°ï¼ˆä½¿ç”¨ Mini-batch GDï¼‰
        """
        self.lr = learning_rate
        self.n_epochs = n_epochs
        self.batch_size = batch_size
        self.weights = None
        self.bias = None
        self.loss_history = []

    def fit(self, X, y):
        """
        è®­ç»ƒæ¨¡å‹

        æµç¨‹ï¼š
        1. åˆå§‹åŒ–å‚æ•° w, b
        2. å‰å‘ä¼ æ’­ï¼šè®¡ç®—é¢„æµ‹æ¦‚ç‡
        3. è®¡ç®—æŸå¤±
        4. åå‘ä¼ æ’­ï¼šè®¡ç®—æ¢¯åº¦
        5. æ›´æ–°å‚æ•°
        """
        n_samples, n_features = X.shape

        # åˆå§‹åŒ–å‚æ•°
        self.weights = np.zeros(n_features)
        self.bias = 0

        # Mini-batch æ¢¯åº¦ä¸‹é™
        for epoch in range(self.n_epochs):
            # æ‰“ä¹±æ•°æ®
            indices = np.random.permutation(n_samples)

            # åˆ†æ‰¹è®­ç»ƒ
            for start_idx in range(0, n_samples, self.batch_size):
                # è·å–å½“å‰ batch
                batch_indices = indices[start_idx:start_idx + self.batch_size]
                X_batch = X[batch_indices]
                y_batch = y[batch_indices]

                # ========== å‰å‘ä¼ æ’­ ==========
                # 1. çº¿æ€§ç»„åˆ
                z = np.dot(X_batch, self.weights) + self.bias

                # 2. Sigmoid æ¿€æ´»
                y_pred = sigmoid(z)

                # ========== è®¡ç®—æ¢¯åº¦ ==========
                # æ¨å¯¼è¿‡ç¨‹ï¼š
                # Loss = -[y*log(Ïƒ(z)) + (1-y)*log(1-Ïƒ(z))]
                # âˆ‚Loss/âˆ‚w = (Ïƒ(z) - y) * x
                # âˆ‚Loss/âˆ‚b = Ïƒ(z) - y

                batch_size_actual = len(X_batch)
                dw = (1 / batch_size_actual) * np.dot(X_batch.T, (y_pred - y_batch))
                db = (1 / batch_size_actual) * np.sum(y_pred - y_batch)

                # ========== æ›´æ–°å‚æ•° ==========
                self.weights -= self.lr * dw
                self.bias -= self.lr * db

            # è®°å½•æ¯ä¸ª epoch çš„æŸå¤±ï¼ˆç”¨å…¨éƒ¨æ•°æ®è®¡ç®—ï¼‰
            if epoch % 10 == 0:
                z_all = np.dot(X, self.weights) + self.bias
                y_pred_all = sigmoid(z_all)
                loss = binary_cross_entropy(y, y_pred_all)
                self.loss_history.append(loss)

    def predict_proba(self, X):
        """é¢„æµ‹æ¦‚ç‡"""
        z = np.dot(X, self.weights) + self.bias
        return sigmoid(z)

    def predict(self, X, threshold=0.5):
        """
        é¢„æµ‹ç±»åˆ«

        threshold: å†³ç­–é˜ˆå€¼
        - é»˜è®¤ 0.5ï¼šp >= 0.5 â†’ æ­£ç±»ï¼Œp < 0.5 â†’ è´Ÿç±»
        - å¯ä»¥è°ƒæ•´ï¼šå¦‚åƒåœ¾é‚®ä»¶æ£€æµ‹å¯èƒ½ç”¨ 0.7ï¼ˆå®å¯æ¼è¿‡ä¸å¯è¯¯æ€ï¼‰

        ========================================================================
        ğŸ¯ ç²¾ç¡®ç‡ vs å¬å›ç‡ï¼šå†³ç­–é˜ˆå€¼çš„æƒè¡¡
        ========================================================================

        ã€æ··æ·†çŸ©é˜µ (Confusion Matrix)ã€‘

                        é¢„æµ‹ä¸ºæ­£ç±»    é¢„æµ‹ä¸ºè´Ÿç±»
        å®é™…æ˜¯æ­£ç±»        TP           FN
                      (çœŸæ­£ä¾‹)     (å‡è´Ÿä¾‹)
        å®é™…æ˜¯è´Ÿç±»        FP           TN
                      (å‡æ­£ä¾‹)     (çœŸè´Ÿä¾‹)

        TP (True Positive):  é¢„æµ‹ä¸ºæ­£ï¼Œå®é™…ä¸ºæ­£ âœ…
        TN (True Negative):  é¢„æµ‹ä¸ºè´Ÿï¼Œå®é™…ä¸ºè´Ÿ âœ…
        FP (False Positive): é¢„æµ‹ä¸ºæ­£ï¼Œå®é™…ä¸ºè´Ÿ âŒ (è¯¯æŠ¥)
        FN (False Negative): é¢„æµ‹ä¸ºè´Ÿï¼Œå®é™…ä¸ºæ­£ âŒ (æ¼æŠ¥)

        ã€ä¸‰å¤§æŒ‡æ ‡ã€‘

        1. å‡†ç¡®ç‡ (Accuracy)
           = (TP + TN) / (TP + TN + FP + FN)
           = é¢„æµ‹å¯¹çš„ / æ€»æ ·æœ¬
           å«ä¹‰ï¼šæ•´ä½“é¢„æµ‹çš„å‡†ç¡®ç¨‹åº¦

        2. ç²¾ç¡®ç‡ (Precision) - "æŸ¥å‡†ç‡"
           = TP / (TP + FP)
           = çœŸæ­£ä¾‹ / é¢„æµ‹ä¸ºæ­£ç±»çš„æ‰€æœ‰æ ·æœ¬
           å«ä¹‰ï¼šåœ¨æ‰€æœ‰"é¢„æµ‹ä¸ºæ­£"çš„æ ·æœ¬ä¸­ï¼Œæœ‰å¤šå°‘çœŸçš„æ˜¯æ­£ç±»
           é—®é¢˜ï¼šæ¨¡å‹è¯´æ˜¯æ­£ç±»ï¼Œæœ‰å¤šå¤§æŠŠæ¡ï¼Ÿ

        3. å¬å›ç‡ (Recall) - "æŸ¥å…¨ç‡"
           = TP / (TP + FN)
           = çœŸæ­£ä¾‹ / å®é™…ä¸ºæ­£ç±»çš„æ‰€æœ‰æ ·æœ¬
           å«ä¹‰ï¼šåœ¨æ‰€æœ‰"çœŸæ­£çš„æ­£ç±»"ä¸­ï¼Œæ¨¡å‹æ‰¾åˆ°äº†å¤šå°‘
           é—®é¢˜ï¼šæ‰€æœ‰æ­£ç±»ä¸­ï¼Œæ¼æ‰äº†å¤šå°‘ï¼Ÿ

        ã€ç›´è§‰ç†è§£ã€‘

        åœºæ™¯ï¼šåƒåœ¾é‚®ä»¶æ£€æµ‹
        - ç²¾ç¡®ç‡ï¼šè¢«æ ‡è®°ä¸ºåƒåœ¾é‚®ä»¶çš„ï¼Œæœ‰å¤šå°‘çœŸçš„æ˜¯åƒåœ¾ï¼Ÿ
          â†’ é«˜ç²¾ç¡®ç‡ = ä¸ä¼šè¯¯æ€æ­£å¸¸é‚®ä»¶
        - å¬å›ç‡ï¼šæ‰€æœ‰åƒåœ¾é‚®ä»¶ä¸­ï¼ŒæŠ“åˆ°äº†å¤šå°‘ï¼Ÿ
          â†’ é«˜å¬å›ç‡ = ä¸ä¼šæ¼æ‰åƒåœ¾é‚®ä»¶

        åœºæ™¯ï¼šç–¾ç—…è¯Šæ–­
        - ç²¾ç¡®ç‡ï¼šè¯Šæ–­ä¸ºé˜³æ€§çš„ï¼Œæœ‰å¤šå°‘çœŸçš„æœ‰ç—…ï¼Ÿ
          â†’ é«˜ç²¾ç¡®ç‡ = å‡å°‘è¯¯è¯Šï¼ˆå¥åº·äººè¢«è¯Šæ–­ä¸ºæœ‰ç—…ï¼‰
        - å¬å›ç‡ï¼šæ‰€æœ‰æ‚£è€…ä¸­ï¼Œæ£€æµ‹å‡ºäº†å¤šå°‘ï¼Ÿ
          â†’ é«˜å¬å›ç‡ = å‡å°‘æ¼è¯Šï¼ˆæœ‰ç—…ä½†æ²¡æ£€æµ‹å‡ºæ¥ï¼‰

        ã€é˜ˆå€¼çš„å½±å“ã€‘

        é˜ˆå€¼ â†‘ (å¦‚ 0.3 â†’ 0.7)ï¼š
        â”œâ”€ é¢„æµ‹ä¸ºæ­£ç±»çš„æ ·æœ¬ â†“ï¼ˆæ›´ä¸¥æ ¼ï¼‰
        â”œâ”€ TP â†“ï¼ŒFP â†“ï¼ˆè¯¯æŠ¥å°‘äº†ï¼‰
        â”œâ”€ ç²¾ç¡®ç‡ â†‘ï¼ˆè¯´æ˜¯æ­£ç±»æ—¶æ›´å¯é ï¼‰
        â””â”€ å¬å›ç‡ â†“ï¼ˆæ¼æ‰æ›´å¤šæ­£ç±»ï¼‰

        é˜ˆå€¼ â†“ (å¦‚ 0.7 â†’ 0.3)ï¼š
        â”œâ”€ é¢„æµ‹ä¸ºæ­£ç±»çš„æ ·æœ¬ â†‘ï¼ˆæ›´å®½æ¾ï¼‰
        â”œâ”€ TP â†‘ï¼ŒFP â†‘ï¼ˆè¯¯æŠ¥å¤šäº†ï¼‰
        â”œâ”€ å¬å›ç‡ â†‘ï¼ˆæŠ“åˆ°æ›´å¤šæ­£ç±»ï¼‰
        â””â”€ ç²¾ç¡®ç‡ â†“ï¼ˆè¯´æ˜¯æ­£ç±»æ—¶ä¸å¤ªå¯é ï¼‰

        ã€æ ¸å¿ƒæƒè¡¡ã€‘
        ç²¾ç¡®ç‡ â†‘ â‡” å¬å›ç‡ â†“ï¼ˆé€šå¸¸æƒ…å†µï¼‰

        æ— æ³•åŒæ—¶æœ€å¤§åŒ–ï¼éœ€è¦æ ¹æ®ä¸šåŠ¡éœ€æ±‚é€‰æ‹©ï¼š

        â€¢ é‡è§†ç²¾ç¡®ç‡ï¼ˆå®å¯æ¼è¿‡ï¼Œä¸å¯è¯¯æ€ï¼‰
          â†’ é˜ˆå€¼è°ƒé«˜ï¼ˆå¦‚ 0.7ï¼‰
          â†’ æ¡ˆä¾‹ï¼šæ¨èç³»ç»Ÿï¼ˆå®å¯å°‘æ¨èï¼Œä¸æ¨èé”™çš„ï¼‰
                  åƒåœ¾é‚®ä»¶æ£€æµ‹ï¼ˆä¸èƒ½è¯¯åˆ æ­£å¸¸é‚®ä»¶ï¼‰

        â€¢ é‡è§†å¬å›ç‡ï¼ˆå®å¯è¯¯æ€ï¼Œä¸å¯æ¼è¿‡ï¼‰
          â†’ é˜ˆå€¼è°ƒä½ï¼ˆå¦‚ 0.3ï¼‰
          â†’ æ¡ˆä¾‹ï¼šç–¾ç—…ç­›æŸ¥ï¼ˆä¸èƒ½æ¼æ‰æ‚£è€…ï¼‰
                  æ¬ºè¯ˆæ£€æµ‹ï¼ˆä¸èƒ½æ”¾è¿‡æ¬ºè¯ˆäº¤æ˜“ï¼‰

        ã€F1 Score - å¹³è¡¡ä¸¤è€…ã€‘
        F1 = 2 Ã— (Precision Ã— Recall) / (Precision + Recall)

        F1 æ˜¯ç²¾ç¡®ç‡å’Œå¬å›ç‡çš„è°ƒå’Œå¹³å‡æ•°
        å½“ä¸¤è€…éƒ½é«˜æ—¶ï¼ŒF1 æ‰é«˜ï¼ˆå¹³è¡¡æŒ‡æ ‡ï¼‰

        ã€å®é™…ä¾‹å­ã€‘

        å‡è®¾ 100 ä¸ªæ ·æœ¬ï¼š60 ä¸ªæ­£ç±»ï¼Œ40 ä¸ªè´Ÿç±»
        æ¨¡å‹é¢„æµ‹ç»“æœï¼š

        é˜ˆå€¼ = 0.3ï¼ˆå®½æ¾ï¼‰ï¼š
        â”œâ”€ é¢„æµ‹ä¸ºæ­£ï¼š70 ä¸ªï¼ˆTP=55, FP=15ï¼‰
        â”œâ”€ é¢„æµ‹ä¸ºè´Ÿï¼š30 ä¸ªï¼ˆTN=25, FN=5ï¼‰
        â”œâ”€ Precision = 55/70 = 0.786 (78.6%)
        â”œâ”€ Recall = 55/60 = 0.917 (91.7%)
        â””â”€ ç‰¹ç‚¹ï¼šæŠ“åˆ°äº†å¤§éƒ¨åˆ†æ­£ç±»ï¼Œä½†è¯¯æŠ¥å¤š

        é˜ˆå€¼ = 0.7ï¼ˆä¸¥æ ¼ï¼‰ï¼š
        â”œâ”€ é¢„æµ‹ä¸ºæ­£ï¼š35 ä¸ªï¼ˆTP=33, FP=2ï¼‰
        â”œâ”€ é¢„æµ‹ä¸ºè´Ÿï¼š65 ä¸ªï¼ˆTN=38, FN=27ï¼‰
        â”œâ”€ Precision = 33/35 = 0.943 (94.3%)
        â”œâ”€ Recall = 33/60 = 0.550 (55.0%)
        â””â”€ ç‰¹ç‚¹ï¼šé¢„æµ‹ä¸ºæ­£æ—¶å¾ˆå¯é ï¼Œä½†æ¼æ‰å¾ˆå¤š

        é˜ˆå€¼ = 0.5ï¼ˆå¹³è¡¡ï¼‰ï¼š
        â”œâ”€ æ‰¾åˆ°æœ€ä½³å¹³è¡¡ç‚¹
        â””â”€ æ ¹æ®å®é™…æƒ…å†µè°ƒæ•´

        ã€è®°å¿†å£è¯€ã€‘
        ç²¾ç¡®ç‡ï¼šæˆ‘è¯´çš„å¯¹ä¸å¯¹ï¼Ÿï¼ˆé¢„æµ‹å‡†ä¸å‡†ï¼‰
        å¬å›ç‡ï¼šæˆ‘æ‰¾å…¨äº†æ²¡æœ‰ï¼Ÿï¼ˆæ¼äº†å¤šå°‘ï¼‰

        ========================================================================
        """
        probabilities = self.predict_proba(X)
        return (probabilities >= threshold).astype(int)

    def score(self, X, y):
        """è®¡ç®—å‡†ç¡®ç‡"""
        y_pred = self.predict(X)
        return np.mean(y_pred == y)


# ==================== 3. æ•°æ®ç”Ÿæˆ ====================
def generate_binary_data(n_samples=200, n_features=2, random_state=42):
    """
    ç”ŸæˆäºŒåˆ†ç±»æ•°æ®

    ä½¿ç”¨ sklearn çš„ make_classification ç”Ÿæˆçº¿æ€§å¯åˆ†çš„æ•°æ®
    """
    X, y = make_classification(
        n_samples=n_samples,
        n_features=n_features,
        n_informative=2,      # æ‰€æœ‰ç‰¹å¾éƒ½æœ‰ç”¨
        n_redundant=0,        # æ— å†—ä½™ç‰¹å¾
        n_clusters_per_class=1,
        class_sep=1.5,        # ç±»åˆ«åˆ†ç¦»åº¦
        random_state=random_state
    )
    return X, y


def generate_nonlinear_data(n_samples=200):
    """
    ç”Ÿæˆéçº¿æ€§å¯åˆ†çš„æ•°æ®ï¼ˆåœ†å½¢åˆ†å¸ƒï¼‰

    æ¼”ç¤ºé€»è¾‘å›å½’çš„å±€é™æ€§ï¼šåªèƒ½å­¦ä¹ çº¿æ€§å†³ç­–è¾¹ç•Œ
    """
    np.random.seed(42)

    # å†…åœ†ï¼šè´Ÿç±»
    r_inner = np.random.uniform(0, 1, n_samples // 2)
    theta_inner = np.random.uniform(0, 2 * np.pi, n_samples // 2)
    X_inner = np.column_stack([
        r_inner * np.cos(theta_inner),
        r_inner * np.sin(theta_inner)
    ])
    y_inner = np.zeros(n_samples // 2)

    # å¤–åœ†ï¼šæ­£ç±»
    r_outer = np.random.uniform(2, 3, n_samples // 2)
    theta_outer = np.random.uniform(0, 2 * np.pi, n_samples // 2)
    X_outer = np.column_stack([
        r_outer * np.cos(theta_outer),
        r_outer * np.sin(theta_outer)
    ])
    y_outer = np.ones(n_samples // 2)

    X = np.vstack([X_inner, X_outer])
    y = np.concatenate([y_inner, y_outer])

    return X, y


# ==================== 4. å¯è§†åŒ– ====================
def plot_decision_boundary(model, X, y, title="Decision Boundary"):
    """
    ç»˜åˆ¶å†³ç­–è¾¹ç•Œ

    åŸç†ï¼š
    - åœ¨æ•´ä¸ªç‰¹å¾ç©ºé—´ç”Ÿæˆç½‘æ ¼ç‚¹
    - ç”¨æ¨¡å‹é¢„æµ‹æ¯ä¸ªç‚¹çš„ç±»åˆ«
    - ç”¨é¢œè‰²åŒºåˆ†ä¸åŒåŒºåŸŸ
    """
    # è®¾ç½®ç½‘æ ¼èŒƒå›´
    x_min, x_max = X[:, 0].min() - 1, X[:, 0].max() + 1
    y_min, y_max = X[:, 1].min() - 1, X[:, 1].max() + 1

    # ç”Ÿæˆç½‘æ ¼ç‚¹
    xx, yy = np.meshgrid(
        np.linspace(x_min, x_max, 200),
        np.linspace(y_min, y_max, 200)
    )

    # é¢„æµ‹ç½‘æ ¼ç‚¹çš„ç±»åˆ«
    Z = model.predict(np.c_[xx.ravel(), yy.ravel()])
    Z = Z.reshape(xx.shape)

    # ç»˜åˆ¶å†³ç­–è¾¹ç•Œ
    plt.contourf(xx, yy, Z, alpha=0.3, cmap='RdYlBu', levels=1)

    # ç»˜åˆ¶æ•°æ®ç‚¹
    plt.scatter(X[y == 0][:, 0], X[y == 0][:, 1],
                c='blue', marker='o', s=50, edgecolors='k', label='Class 0', alpha=0.7)
    plt.scatter(X[y == 1][:, 0], X[y == 1][:, 1],
                c='red', marker='s', s=50, edgecolors='k', label='Class 1', alpha=0.7)

    plt.xlabel('Feature 1', fontsize=12)
    plt.ylabel('Feature 2', fontsize=12)
    plt.title(title, fontsize=14, fontweight='bold')
    plt.legend(fontsize=10)
    plt.grid(True, alpha=0.3)


def visualize_sigmoid():
    """å¯è§†åŒ– Sigmoid å‡½æ•°"""
    z = np.linspace(-10, 10, 200)
    sigma = sigmoid(z)

    plt.figure(figsize=(10, 6))
    plt.plot(z, sigma, linewidth=3, color='purple')

    # æ ‡æ³¨å…³é”®ç‚¹
    plt.axhline(y=0.5, color='red', linestyle='--', alpha=0.5, label='Decision threshold (0.5)')
    plt.axvline(x=0, color='green', linestyle='--', alpha=0.5, label='z = 0')
    plt.scatter([0, -2, 2], [sigmoid(0), sigmoid(-2), sigmoid(2)],
                s=100, c='red', zorder=5, edgecolors='black', linewidths=2)

    # æ·»åŠ æ³¨é‡Š
    plt.text(0, 0.5, '  (0, 0.5)', fontsize=11, verticalalignment='bottom')
    plt.text(-2, sigmoid(-2), f'  ({-2:.0f}, {sigmoid(-2):.3f})', fontsize=10)
    plt.text(2, sigmoid(2), f'  ({2:.0f}, {sigmoid(2):.3f})', fontsize=10)

    plt.xlabel('z (linear output)', fontsize=12)
    plt.ylabel('Ïƒ(z) (probability)', fontsize=12)
    plt.title('Sigmoid Function: Ïƒ(z) = 1 / (1 + e^(-z))', fontsize=14, fontweight='bold')
    plt.grid(True, alpha=0.3)
    plt.legend(fontsize=11)
    plt.ylim(-0.1, 1.1)

    plt.tight_layout()
    plt.savefig('sigmoid_function.png', dpi=100)
    print("ğŸ“Š Sigmoid å‡½æ•°å›¾å·²ä¿å­˜åˆ°: sigmoid_function.png")
    plt.show()


# ==================== 5. ä¸»ç¨‹åº ====================
def main():
    print("=" * 70)
    print("é€»è¾‘å›å½’ (Logistic Regression) - äºŒåˆ†ç±»ä»»åŠ¡")
    print("=" * 70)

    # ========== 1. å¯è§†åŒ– Sigmoid å‡½æ•° ==========
    print("\n" + "=" * 70)
    print("ğŸ“ˆ ç¬¬ä¸€æ­¥ï¼šç†è§£ Sigmoid å‡½æ•°")
    print("=" * 70)
    print("""
Sigmoid å‡½æ•°æ˜¯é€»è¾‘å›å½’çš„æ ¸å¿ƒï¼š
- ä½œç”¨ï¼šå°†çº¿æ€§è¾“å‡º z = wx + b æ˜ å°„åˆ° [0, 1] æ¦‚ç‡
- å…¬å¼ï¼šÏƒ(z) = 1 / (1 + e^(-z))
- æ€§è´¨ï¼šå•è°ƒé€’å¢ï¼Œåœ¨ z=0 å¤„å€¼ä¸º 0.5
    """)
    visualize_sigmoid()

    # ========== 2. ç”Ÿæˆæ•°æ® ==========
    print("\n" + "=" * 70)
    print("ğŸ“Š ç¬¬äºŒæ­¥ï¼šç”ŸæˆäºŒåˆ†ç±»æ•°æ®")
    print("=" * 70)
    X, y = generate_binary_data(n_samples=300, n_features=2)
    X_train, X_test, y_train, y_test = train_test_split(
        X, y, test_size=0.3, random_state=42
    )
    print(f"è®­ç»ƒé›†å¤§å°: {len(X_train)}")
    print(f"æµ‹è¯•é›†å¤§å°: {len(X_test)}")
    print(f"æ­£ç±»æ ·æœ¬: {np.sum(y_train == 1)}, è´Ÿç±»æ ·æœ¬: {np.sum(y_train == 0)}")

    # ========== 3. è®­ç»ƒæ¨¡å‹ ==========
    print("\n" + "=" * 70)
    print("ğŸ”§ ç¬¬ä¸‰æ­¥ï¼šè®­ç»ƒé€»è¾‘å›å½’æ¨¡å‹")
    print("=" * 70)

    model = LogisticRegression(learning_rate=0.1, n_epochs=200, batch_size=32)
    print(f"è¶…å‚æ•°ï¼šå­¦ä¹ ç‡={model.lr}, è®­ç»ƒè½®æ•°={model.n_epochs}, æ‰¹é‡å¤§å°={model.batch_size}")
    print("\nè®­ç»ƒä¸­...")
    model.fit(X_train, y_train)

    # ========== 4. è¯„ä¼°æ¨¡å‹ ==========
    print("\n" + "=" * 70)
    print("ğŸ“Š ç¬¬å››æ­¥ï¼šè¯„ä¼°æ¨¡å‹æ€§èƒ½")
    print("=" * 70)

    train_acc = model.score(X_train, y_train)
    test_acc = model.score(X_test, y_test)

    print(f"\nè®­ç»ƒé›†å‡†ç¡®ç‡: {train_acc:.4f} ({train_acc * 100:.2f}%)")
    print(f"æµ‹è¯•é›†å‡†ç¡®ç‡: {test_acc:.4f} ({test_acc * 100:.2f}%)")

    # å­¦ä¹ åˆ°çš„å‚æ•°
    print(f"\nå­¦ä¹ åˆ°çš„å‚æ•°ï¼š")
    print(f"  æƒé‡ w = {model.weights}")
    print(f"  åç½® b = {model.bias:.4f}")

    # é¢„æµ‹ç¤ºä¾‹
    print(f"\né¢„æµ‹ç¤ºä¾‹ï¼ˆå‰5ä¸ªæµ‹è¯•æ ·æœ¬ï¼‰ï¼š")
    sample_probs = model.predict_proba(X_test[:5])
    sample_preds = model.predict(X_test[:5])
    for i in range(5):
        print(f"  æ ·æœ¬ {i+1}: çœŸå®={y_test[i]}, é¢„æµ‹={sample_preds[i]}, æ¦‚ç‡={sample_probs[i]:.4f}")

    # ========== 5. å¯è§†åŒ–ç»“æœ ==========
    print("\n" + "=" * 70)
    print("ğŸ¨ ç¬¬äº”æ­¥ï¼šå¯è§†åŒ–å†³ç­–è¾¹ç•Œå’Œè®­ç»ƒè¿‡ç¨‹")
    print("=" * 70)

    fig = plt.figure(figsize=(16, 5))

    # å­å›¾1ï¼šè®­ç»ƒé›†å†³ç­–è¾¹ç•Œ
    plt.subplot(1, 3, 1)
    plot_decision_boundary(model, X_train, y_train,
                          title=f'Training Set\nAccuracy: {train_acc:.2%}')

    # å­å›¾2ï¼šæµ‹è¯•é›†å†³ç­–è¾¹ç•Œ
    plt.subplot(1, 3, 2)
    plot_decision_boundary(model, X_test, y_test,
                          title=f'Test Set\nAccuracy: {test_acc:.2%}')

    # å­å›¾3ï¼šæŸå¤±å‡½æ•°ä¸‹é™æ›²çº¿
    plt.subplot(1, 3, 3)
    plt.plot(range(0, model.n_epochs, 10), model.loss_history,
             linewidth=2, color='blue', marker='o', markersize=4)
    plt.xlabel('Epoch', fontsize=12)
    plt.ylabel('Cross-Entropy Loss', fontsize=12)
    plt.title('Training Loss vs Epoch', fontsize=14, fontweight='bold')
    plt.grid(True, alpha=0.3)

    plt.tight_layout()
    plt.savefig('logistic_regression_result.png', dpi=100)
    print("\nğŸ“Š ç»“æœå›¾å·²ä¿å­˜åˆ°: logistic_regression_result.png")
    plt.show()

    # ========== 6. å¯¹æ¯” sklearn ==========
    print("\n" + "=" * 70)
    print("ğŸ”¬ ç¬¬å…­æ­¥ï¼šä¸ sklearn å¯¹æ¯”éªŒè¯")
    print("=" * 70)

    from sklearn.linear_model import LogisticRegression as SklearnLR

    sklearn_model = SklearnLR(max_iter=1000, random_state=42)
    sklearn_model.fit(X_train, y_train)

    sklearn_train_acc = sklearn_model.score(X_train, y_train)
    sklearn_test_acc = sklearn_model.score(X_test, y_test)

    print(f"\nSklearn é€»è¾‘å›å½’ï¼š")
    print(f"  è®­ç»ƒé›†å‡†ç¡®ç‡: {sklearn_train_acc:.4f} ({sklearn_train_acc * 100:.2f}%)")
    print(f"  æµ‹è¯•é›†å‡†ç¡®ç‡: {sklearn_test_acc:.4f} ({sklearn_test_acc * 100:.2f}%)")
    print(f"  æƒé‡ w = {sklearn_model.coef_[0]}")
    print(f"  åç½® b = {sklearn_model.intercept_[0]:.4f}")

    print(f"\nå¯¹æ¯”ç»“æœï¼š")
    print(f"  å‡†ç¡®ç‡å·®å¼‚ï¼ˆæµ‹è¯•é›†ï¼‰: {abs(test_acc - sklearn_test_acc):.4f}")
    print(f"  âœ… å®ç°åŸºæœ¬æ­£ç¡®ï¼")


# ==================== 6. å®éªŒåŒº ====================
def experiment_decision_threshold():
    """
    å®éªŒï¼šå†³ç­–é˜ˆå€¼çš„å½±å“

    æ¢ç´¢ä¸åŒé˜ˆå€¼å¯¹åˆ†ç±»ç»“æœçš„å½±å“
    """
    print("\n" + "=" * 70)
    print("ğŸ§ª å®éªŒï¼šå†³ç­–é˜ˆå€¼çš„å½±å“")
    print("=" * 70)

    X, y = generate_binary_data(n_samples=300)
    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)

    # è®­ç»ƒæ¨¡å‹
    model = LogisticRegression(learning_rate=0.1, n_epochs=200)
    model.fit(X_train, y_train)

    # æµ‹è¯•ä¸åŒé˜ˆå€¼
    thresholds = [0.3, 0.5, 0.7, 0.9]

    print(f"\n{'é˜ˆå€¼':<10} {'å‡†ç¡®ç‡':<10} {'é¢„æµ‹ä¸ºæ­£ç±»çš„æ¯”ä¾‹':<20}")
    print("-" * 40)

    for threshold in thresholds:
        y_pred = model.predict(X_test, threshold=threshold)
        accuracy = np.mean(y_pred == y_test)
        positive_rate = np.mean(y_pred == 1)
        print(f"{threshold:<10.1f} {accuracy:<10.4f} {positive_rate:<20.2%}")

    print(f"\nğŸ’¡ è§‚å¯Ÿï¼š")
    print(f"  - é˜ˆå€¼è¶Šé«˜ â†’ é¢„æµ‹ä¸ºæ­£ç±»è¶Šä¸¥æ ¼ï¼ˆç²¾ç¡®ç‡é«˜ï¼Œå¬å›ç‡ä½ï¼‰")
    print(f"  - é˜ˆå€¼è¶Šä½ â†’ é¢„æµ‹ä¸ºæ­£ç±»è¶Šå®½æ¾ï¼ˆå¬å›ç‡é«˜ï¼Œç²¾ç¡®ç‡ä½ï¼‰")
    print(f"  - é»˜è®¤ 0.5 é€šå¸¸æ˜¯å¹³è¡¡ç‚¹")


def experiment_nonlinear_data():
    """
    å®éªŒï¼šé€»è¾‘å›å½’åœ¨éçº¿æ€§æ•°æ®ä¸Šçš„å±€é™æ€§

    å±•ç¤ºé€»è¾‘å›å½’åªèƒ½å­¦ä¹ çº¿æ€§å†³ç­–è¾¹ç•Œ
    """
    print("\n" + "=" * 70)
    print("ğŸ§ª å®éªŒï¼šé€»è¾‘å›å½’çš„å±€é™æ€§ - éçº¿æ€§æ•°æ®")
    print("=" * 70)

    # ç”Ÿæˆéçº¿æ€§æ•°æ®ï¼ˆåœ†å½¢åˆ†å¸ƒï¼‰
    X, y = generate_nonlinear_data(n_samples=300)
    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)

    # è®­ç»ƒé€»è¾‘å›å½’
    model = LogisticRegression(learning_rate=0.1, n_epochs=200)
    model.fit(X_train, y_train)

    train_acc = model.score(X_train, y_train)
    test_acc = model.score(X_test, y_test)

    print(f"\næ•°æ®ç‰¹ç‚¹ï¼šå†…åœ†ä¸ºè´Ÿç±»ï¼Œå¤–åœ†ä¸ºæ­£ç±»ï¼ˆéçº¿æ€§å¯åˆ†ï¼‰")
    print(f"è®­ç»ƒé›†å‡†ç¡®ç‡: {train_acc:.4f} ({train_acc * 100:.2f}%)")
    print(f"æµ‹è¯•é›†å‡†ç¡®ç‡: {test_acc:.4f} ({test_acc * 100:.2f}%)")

    # å¯è§†åŒ–
    plt.figure(figsize=(8, 6))
    plot_decision_boundary(model, X_test, y_test,
                          title=f'Nonlinear Data\nLogistic Regression Accuracy: {test_acc:.2%}')
    plt.tight_layout()
    plt.savefig('logistic_regression_nonlinear.png', dpi=100)
    print("\nğŸ“Š éçº¿æ€§æ•°æ®ç»“æœå›¾å·²ä¿å­˜åˆ°: logistic_regression_nonlinear.png")
    plt.show()

    print(f"\nğŸ’¡ ç»“è®ºï¼š")
    print(f"  - é€»è¾‘å›å½’åªèƒ½å­¦ä¹ çº¿æ€§å†³ç­–è¾¹ç•Œ")
    print(f"  - å¯¹äºéçº¿æ€§æ•°æ®ï¼Œå‡†ç¡®ç‡å¾ˆä½")
    print(f"  - è§£å†³æ–¹æ¡ˆï¼šç‰¹å¾å·¥ç¨‹ï¼ˆå¤šé¡¹å¼ç‰¹å¾ï¼‰æˆ–ä½¿ç”¨éçº¿æ€§æ¨¡å‹ï¼ˆç¥ç»ç½‘ç»œï¼‰")


# ==================== 7. æ€»ç»“ ====================
def print_summary():
    print("\n" + "=" * 70)
    print("âœ… æ ¸å¿ƒè¦ç‚¹æ€»ç»“")
    print("=" * 70)
    print("""
1. é€»è¾‘å›å½’ vs çº¿æ€§å›å½’
   çº¿æ€§å›å½’ï¼šé¢„æµ‹è¿ç»­å€¼ï¼ˆæˆ¿ä»·ã€æ¸©åº¦ï¼‰
   é€»è¾‘å›å½’ï¼šé¢„æµ‹ç¦»æ•£ç±»åˆ«ï¼ˆæ˜¯/å¦ã€åƒåœ¾é‚®ä»¶/æ­£å¸¸é‚®ä»¶ï¼‰

2. Sigmoid å‡½æ•°
   - ä½œç”¨ï¼šå°†çº¿æ€§è¾“å‡ºæ˜ å°„åˆ° [0, 1] æ¦‚ç‡
   - å…¬å¼ï¼šÏƒ(z) = 1 / (1 + e^(-z))
   - z = wx + b æ˜¯çº¿æ€§ç»„åˆ

3. äº¤å‰ç†µæŸå¤±
   - ä¸ºä»€ä¹ˆä¸ç”¨ MSEï¼ŸMSE å¯¹åˆ†ç±»é—®é¢˜ä¼˜åŒ–æ•ˆæœå·®
   - å…¬å¼ï¼šLoss = -[y*log(p) + (1-y)*log(1-p)]
   - è¡¡é‡é¢„æµ‹æ¦‚ç‡åˆ†å¸ƒå’ŒçœŸå®åˆ†å¸ƒçš„å·®å¼‚

4. å†³ç­–è¾¹ç•Œ
   - çº¿æ€§è¾¹ç•Œï¼šwx + b = 0 çš„ç›´çº¿/å¹³é¢
   - é˜ˆå€¼ï¼šé»˜è®¤ 0.5ï¼Œå¯æ ¹æ®ä¸šåŠ¡éœ€æ±‚è°ƒæ•´

5. åº”ç”¨åœºæ™¯
   âœ“ åƒåœ¾é‚®ä»¶æ£€æµ‹
   âœ“ ç–¾ç—…è¯Šæ–­ï¼ˆæ˜¯å¦æ‚£ç—…ï¼‰
   âœ“ å®¢æˆ·æµå¤±é¢„æµ‹
   âœ“ ä¿¡ç”¨è¯„åˆ†ï¼ˆæ˜¯å¦è¿çº¦ï¼‰

6. å±€é™æ€§
   âœ— åªèƒ½å­¦ä¹ çº¿æ€§å†³ç­–è¾¹ç•Œ
   âœ— å¯¹éçº¿æ€§æ•°æ®æ•ˆæœå·®
   â†’ è§£å†³æ–¹æ¡ˆï¼šç‰¹å¾å·¥ç¨‹æˆ–ç¥ç»ç½‘ç»œ

7. ä¸ç¥ç»ç½‘ç»œçš„å…³ç³»
   - é€»è¾‘å›å½’ = å•å±‚å•ç¥ç»å…ƒçš„ç¥ç»ç½‘ç»œ
   - Sigmoid = æ¿€æ´»å‡½æ•°
   - äº¤å‰ç†µ = åˆ†ç±»é—®é¢˜çš„æ ‡å‡†æŸå¤±å‡½æ•°
   - é€»è¾‘å›å½’æ˜¯ç†è§£ç¥ç»ç½‘ç»œçš„åŸºç¡€ï¼
    """)

    print("=" * 70)
    print("ğŸ¯ ä¸‹ä¸€æ­¥å­¦ä¹ å»ºè®®")
    print("=" * 70)
    print("""
1. å¤šåˆ†ç±»é—®é¢˜ (Softmax Regression)
   - æ‰©å±•åˆ° 3 ä¸ªæˆ–æ›´å¤šç±»åˆ«
   - Softmax å‡½æ•°ï¼šSigmoid çš„å¤šåˆ†ç±»ç‰ˆæœ¬

2. æ­£åˆ™åŒ– (L1/L2 Regularization)
   - é˜²æ­¢è¿‡æ‹Ÿåˆ
   - ç‰¹å¾é€‰æ‹©

3. æ¨¡å‹è¯„ä¼°æŒ‡æ ‡
   - ç²¾ç¡®ç‡ (Precision)
   - å¬å›ç‡ (Recall)
   - F1 Score
   - ROC æ›²çº¿å’Œ AUC

4. ç¥ç»ç½‘ç»œåŸºç¡€
   - å¤šå±‚æ„ŸçŸ¥æœº (MLP)
   - åå‘ä¼ æ’­ç®—æ³•
   - æ¿€æ´»å‡½æ•°å¯¹æ¯”
    """)


if __name__ == "__main__":
    # ä¸»å®éªŒ
    main()

    # é¢å¤–å®éªŒï¼ˆå–æ¶ˆæ³¨é‡Šè¿è¡Œï¼‰
    experiment_decision_threshold()
    experiment_nonlinear_data()

    # æ€»ç»“
    print_summary()

    print("\nğŸ’¡ ç»ƒä¹ å»ºè®®ï¼š")
    print("  1. ä¿®æ”¹å­¦ä¹ ç‡å’Œè®­ç»ƒè½®æ•°ï¼Œè§‚å¯Ÿå¯¹å‡†ç¡®ç‡çš„å½±å“")
    print("  2. å°è¯•ä¸åŒçš„å†³ç­–é˜ˆå€¼ï¼Œç†è§£ç²¾ç¡®ç‡å’Œå¬å›ç‡çš„æƒè¡¡")
    print("  3. åœ¨éçº¿æ€§æ•°æ®ä¸Šå°è¯•æ·»åŠ å¤šé¡¹å¼ç‰¹å¾ï¼ˆå¦‚ x^2, xy, y^2ï¼‰")
    print("  4. æ€è€ƒï¼šä¸ºä»€ä¹ˆé€»è¾‘å›å½’å«'å›å½’'ä½†å®é™…æ˜¯åˆ†ç±»ç®—æ³•ï¼Ÿ")
