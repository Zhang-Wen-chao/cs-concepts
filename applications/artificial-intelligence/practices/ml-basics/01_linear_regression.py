"""
çº¿æ€§å›å½’ (Linear Regression) - ä»é›¶å®ç°

é—®é¢˜ï¼šæ ¹æ®æˆ¿å­é¢ç§¯é¢„æµ‹æˆ¿ä»·
ç›®æ ‡ï¼šç†è§£ç›‘ç£å­¦ä¹ çš„åŸºæœ¬æµç¨‹å’Œæ¢¯åº¦ä¸‹é™ç®—æ³•

æ ¸å¿ƒæ¦‚å¿µï¼š
1. æ¨¡å‹ï¼šy = w * x + b  (w=æƒé‡, b=åç½®)
2. æŸå¤±å‡½æ•°ï¼šMSE (Mean Squared Error, å‡æ–¹è¯¯å·®) = 1/n * Î£(é¢„æµ‹å€¼ - çœŸå®å€¼)Â²
3. ä¼˜åŒ–ï¼šé€šè¿‡æ¢¯åº¦ä¸‹é™æœ€å°åŒ–æŸå¤±å‡½æ•°
"""

import numpy as np
import matplotlib.pyplot as plt


# ==================== 1. å‡†å¤‡æ•°æ® ====================
def generate_data(n_samples=100):
    """
    ç”Ÿæˆæ¨¡æ‹Ÿæ•°æ®ï¼šæˆ¿å­é¢ç§¯ -> æˆ¿ä»·
    çœŸå®å…³ç³»ï¼šæˆ¿ä»· = 3 * é¢ç§¯ + 2 + å™ªå£°
    """
    np.random.seed(42)
    # é¢ç§¯ï¼š1-5ï¼ˆå½’ä¸€åŒ–åˆ°è¾ƒå°èŒƒå›´ï¼Œé¿å…æ¢¯åº¦çˆ†ç‚¸ï¼‰
    X = np.random.uniform(1, 5, n_samples)
    # æˆ¿ä»· = 3 * é¢ç§¯ + 2 + å™ªå£°
    y = 3 * X + 2 + np.random.normal(0, 0.5, n_samples)
    return X, y


# ==================== 2. çº¿æ€§å›å½’æ¨¡å‹ ====================
class LinearRegression:
    """ä»é›¶å®ç°çš„çº¿æ€§å›å½’"""

    def __init__(self, learning_rate=0.001, n_iterations=1000):
        """
        å‚æ•°ï¼š
            learning_rate: å­¦ä¹ ç‡ï¼ˆæ­¥é•¿ï¼‰
            n_iterations: è®­ç»ƒè¿­ä»£æ¬¡æ•°
        """
        self.lr = learning_rate
        self.n_iterations = n_iterations
        self.w = None  # æƒé‡
        self.b = None  # åç½®
        self.loss_history = []  # è®°å½•æŸå¤±å˜åŒ–

    def fit(self, X, y):
        """
        è®­ç»ƒæ¨¡å‹ï¼šé€šè¿‡æ¢¯åº¦ä¸‹é™æ‰¾åˆ°æœ€ä¼˜çš„ w å’Œ b

        æ•°å­¦åŸç†ï¼š
        - é¢„æµ‹ï¼šy_pred = w * x + b
        - æŸå¤±ï¼šloss = 1/n * Î£(y_pred - y)Â²
        - æ¢¯åº¦ï¼šdw = 2/n * Î£(y_pred - y) * x
        -       db = 2/n * Î£(y_pred - y)
        - æ›´æ–°ï¼šw = w - lr * dw
        -       b = b - lr * db
        """
        n_samples = len(X)

        # åˆå§‹åŒ–å‚æ•°ï¼ˆéšæœºåˆå§‹åŒ–ï¼‰
        self.w = 0.0
        self.b = 0.0

        # æ¢¯åº¦ä¸‹é™è®­ç»ƒ
        for i in range(self.n_iterations):
            # å‰å‘ä¼ æ’­ï¼šè®¡ç®—é¢„æµ‹å€¼
            y_pred = self.w * X + self.b

            # è®¡ç®—æŸå¤±ï¼ˆMSEï¼‰
            loss = np.mean((y_pred - y) ** 2)
            self.loss_history.append(loss)

            # è®¡ç®—æ¢¯åº¦
            dw = (2 / n_samples) * np.sum((y_pred - y) * X)
            db = (2 / n_samples) * np.sum(y_pred - y)

            # æ›´æ–°å‚æ•°
            self.w -= self.lr * dw
            self.b -= self.lr * db

            # æ¯100æ¬¡æ‰“å°ä¸€æ¬¡
            if (i + 1) % 100 == 0:
                print(f"ç¬¬ {i+1} æ¬¡è¿­ä»£ - æŸå¤±: {loss:.2f}, w={self.w:.4f}, b={self.b:.4f}")

    def predict(self, X):
        """é¢„æµ‹"""
        return self.w * X + self.b


# ==================== 3. å¯è§†åŒ– ====================
def plot_results(X, y, model):
    """å¯è§†åŒ–è®­ç»ƒç»“æœ"""
    plt.figure(figsize=(15, 5))

    # å­å›¾1ï¼šæ•°æ®å’Œæ‹Ÿåˆçº¿
    plt.subplot(1, 3, 1)
    plt.scatter(X, y, alpha=0.5, label='Real Data')
    plt.plot(X, model.predict(X), 'r-', linewidth=2, label='Fitted Line')
    plt.xlabel('Area (X)')
    plt.ylabel('Price (Y)')
    plt.title('Linear Regression Result')
    plt.legend()
    plt.grid(True, alpha=0.3)

    # å­å›¾2ï¼šæŸå¤±å‡½æ•°å˜åŒ–
    plt.subplot(1, 3, 2)
    plt.plot(model.loss_history)
    plt.xlabel('Iterations')
    plt.ylabel('Loss (MSE)')
    plt.title('Training: Loss Decreasing')
    plt.grid(True, alpha=0.3)

    # å­å›¾3ï¼šå‰100æ¬¡è¿­ä»£çš„æŸå¤±ï¼ˆæ”¾å¤§çœ‹ï¼‰
    plt.subplot(1, 3, 3)
    plt.plot(model.loss_history[:100])
    plt.xlabel('Iterations')
    plt.ylabel('Loss (MSE)')
    plt.title('First 100 Iterations (Zoomed)')
    plt.grid(True, alpha=0.3)

    plt.tight_layout()
    plt.savefig('linear_regression_result.png', dpi=100)
    print("\nå›¾è¡¨å·²ä¿å­˜åˆ°: linear_regression_result.png")
    plt.show()


# ==================== 4. ä¸»ç¨‹åº ====================
def main():
    print("=" * 60)
    print("çº¿æ€§å›å½’å®è·µï¼šä»é›¶å®ç°")
    print("=" * 60)

    # ç”Ÿæˆæ•°æ®
    print("\n1. ç”Ÿæˆæ¨¡æ‹Ÿæ•°æ®ï¼ˆ100ä¸ªæ ·æœ¬ï¼‰")
    print("   çœŸå®å…³ç³»ï¼šæˆ¿ä»· = 3 * é¢ç§¯ + 2 + å™ªå£°")
    X, y = generate_data(100)
    print(f"   æ•°æ®èŒƒå›´ - é¢ç§¯: {X.min():.2f}-{X.max():.2f}")
    print(f"            æˆ¿ä»·: {y.min():.2f}-{y.max():.2f}")

    # è®­ç»ƒæ¨¡å‹
    print("\n2. è®­ç»ƒçº¿æ€§å›å½’æ¨¡å‹")
    print("   å­¦ä¹ ç‡: 0.001, è¿­ä»£æ¬¡æ•°: 1000")
    print("-" * 60)
    model = LinearRegression(learning_rate=0.001, n_iterations=1000)
    model.fit(X, y)

    # æŸ¥çœ‹å­¦åˆ°çš„å‚æ•°
    print("\n3. æ¨¡å‹å­¦åˆ°çš„å‚æ•°")
    print(f"   æƒé‡ w = {model.w:.4f}  (çœŸå®å€¼çº¦ä¸º 3)")
    print(f"   åç½® b = {model.b:.4f}  (çœŸå®å€¼çº¦ä¸º 2)")
    print(f"   æœ€ç»ˆæŸå¤± = {model.loss_history[-1]:.2f}")

    # é¢„æµ‹æ–°æ ·æœ¬
    print("\n4. ç”¨è®­ç»ƒå¥½çš„æ¨¡å‹é¢„æµ‹")
    test_areas = [2.0, 3.0, 4.0]
    for area in test_areas:
        price = model.predict(np.array([area]))[0]
        expected = 3 * area + 2
        print(f"   é¢ç§¯ {area:.1f} â†’ é¢„æµ‹: {price:.2f}, æœŸæœ›: {expected:.2f}")

    # å¯è§†åŒ–
    print("\n5. ç”Ÿæˆå¯è§†åŒ–å›¾è¡¨...")
    plot_results(X, y, model)

    print("\n" + "=" * 60)
    print("âœ… å®Œæˆï¼ä½ å·²ç»ç†è§£äº†ï¼š")
    print("   1. ç›‘ç£å­¦ä¹ çš„åŸºæœ¬æµç¨‹ï¼ˆæ•°æ® â†’ è®­ç»ƒ â†’ é¢„æµ‹ï¼‰")
    print("   2. æŸå¤±å‡½æ•°çš„æ¦‚å¿µï¼ˆè¡¡é‡é¢„æµ‹è¯¯å·®ï¼‰")
    print("   3. æ¢¯åº¦ä¸‹é™çš„ä¼˜åŒ–è¿‡ç¨‹ï¼ˆé€æ­¥å‡å°æŸå¤±ï¼‰")
    print("=" * 60)


# ==================== 5. å®éªŒåŒº ====================
def experiment():
    """
    å®éªŒï¼šè§‚å¯Ÿä¸åŒå­¦ä¹ ç‡çš„å½±å“

    ç¬¬ä¸€è½®å®éªŒç»“æœï¼ˆ500æ¬¡è¿­ä»£ï¼ŒçœŸå®å€¼ wâ‰ˆ3, bâ‰ˆ2ï¼‰ï¼š
    - lr=0.0001: æŸå¤±15.55, w=2.15, b=0.66 â†’ å¤ªæ…¢ï¼Œæœªæ”¶æ•› âœ—
    - lr=0.001:  æŸå¤±0.35,  w=3.25, b=1.14 â†’ æ”¶æ•›è‰¯å¥½ âœ“
    - lr=0.01:   æŸå¤±0.22,  w=3.04, b=1.85 â†’ æ”¶æ•›æœ€å¥½ âœ“âœ“

    ç¬¬äºŒè½®å®éªŒç»“æœï¼ˆæµ‹è¯•æ›´å¤§å­¦ä¹ ç‡ï¼‰ï¼š
    - lr=0.01:   æŸå¤±0.22,  w=3.04, b=1.85 â†’ ç¨³å®šæ”¶æ•› âœ“
    - lr=0.05:   æŸå¤±0.20,  w=2.94, b=2.16 â†’ æœ€ä¼˜ï¼å¿«é€Ÿä¸”å‡†ç¡® âœ“âœ“âœ“
    - lr=0.1:    æŸå¤±çˆ†ç‚¸,  w=-7e23, b=-2e23 â†’ æ¢¯åº¦çˆ†ç‚¸ï¼Œå®Œå…¨å‘æ•£ âœ—âœ—

    ç»“è®ºï¼šlr=0.05 æ˜¯æœ€ä½³å­¦ä¹ ç‡ï¼ˆå¿«é€Ÿæ”¶æ•›+æœ€æ¥è¿‘çœŸå®å€¼ï¼‰ï¼Œlr=0.1 å¤ªå¤§å¯¼è‡´å‘æ•£

    å°è¯•ä¿®æ”¹è¿™äº›å‚æ•°ï¼Œçœ‹çœ‹ä¼šå‘ç”Ÿä»€ä¹ˆï¼š
    - learning_rate: 0.0001, 0.001, 0.01, 0.1
    - n_iterations: 100, 1000, 5000
    """
    print("\n" + "=" * 60)
    print("ğŸ§ª å®éªŒï¼šä¸åŒå­¦ä¹ ç‡çš„å¯¹æ¯”")
    print("=" * 60)

    X, y = generate_data(100)
    # learning_rates = [0.0001, 0.001, 0.01]  # ç¬¬ä¸€è½®
    learning_rates = [0.01, 0.05, 0.1]  # ç¬¬äºŒè½®ï¼šæµ‹è¯•æ›´å¤§å­¦ä¹ ç‡

    plt.figure(figsize=(12, 4))

    for idx, lr in enumerate(learning_rates, 1):
        model = LinearRegression(learning_rate=lr, n_iterations=500)
        model.fit(X, y)

        plt.subplot(1, 3, idx)
        plt.plot(model.loss_history)
        plt.xlabel('Iterations')
        plt.ylabel('Loss')
        plt.title(f'Learning Rate = {lr}')
        plt.grid(True, alpha=0.3)

        print(f"\nå­¦ä¹ ç‡ {lr}:")
        print(f"  æœ€ç»ˆæŸå¤±: {model.loss_history[-1]:.2f}")
        print(f"  å­¦åˆ°çš„å‚æ•°: w={model.w:.4f}, b={model.b:.4f}")

    plt.tight_layout()
    plt.savefig('learning_rate_comparison.png', dpi=100)
    print("\nå¯¹æ¯”å›¾å·²ä¿å­˜åˆ°: learning_rate_comparison.png")
    plt.show()


def experiment_lr_sweep():
    """
    å®éªŒï¼šå­¦ä¹ ç‡æ‰«æ - æ‰¾åˆ°æœ€ä¼˜å­¦ä¹ ç‡

    åœ¨ä¸€å®šèŒƒå›´å†…æµ‹è¯•å¤šä¸ªå­¦ä¹ ç‡ï¼Œç»˜åˆ¶ï¼š
    1. å­¦ä¹ ç‡ vs æœ€ç»ˆæŸå¤±æ›²çº¿ï¼ˆæ‰¾åˆ°æœ€ä¼˜ç‚¹ï¼‰
    2. ä¸åŒå­¦ä¹ ç‡çš„è®­ç»ƒè¿‡ç¨‹å¯¹æ¯”
    """
    print("\n" + "=" * 60)
    print("ğŸ” å®éªŒï¼šå­¦ä¹ ç‡æ‰«æï¼ˆLearning Rate Sweepï¼‰")
    print("=" * 60)

    X, y = generate_data(100)

    # æµ‹è¯•æ›´å¯†é›†çš„å­¦ä¹ ç‡èŒƒå›´
    learning_rates = [0.005, 0.01, 0.02, 0.03, 0.05, 0.07, 0.1, 0.15]
    n_iterations = 500

    # å­˜å‚¨ç»“æœ
    results = []

    # è®­ç»ƒæ¯ä¸ªå­¦ä¹ ç‡
    for lr in learning_rates:
        model = LinearRegression(learning_rate=lr, n_iterations=n_iterations)
        model.fit(X, y)

        final_loss = model.loss_history[-1]
        results.append({
            'lr': lr,
            'loss': final_loss,
            'w': model.w,
            'b': model.b,
            'history': model.loss_history,
            'converged': final_loss < 100  # åˆ¤æ–­æ˜¯å¦æ”¶æ•›
        })

        status = "âœ“ æ”¶æ•›" if final_loss < 100 else "âœ— å‘æ•£"
        print(f"lr={lr:6.3f}: æŸå¤±={final_loss:12.2f}, w={model.w:8.4f}, b={model.b:8.4f} {status}")

    # æ‰¾åˆ°æœ€ä¼˜å­¦ä¹ ç‡ï¼ˆåœ¨æ”¶æ•›çš„ç»“æœä¸­ï¼‰
    converged_results = [r for r in results if r['converged']]
    if converged_results:
        best = min(converged_results, key=lambda x: x['loss'])
        print(f"\nğŸ¯ æœ€ä¼˜å­¦ä¹ ç‡: {best['lr']}, æŸå¤±={best['loss']:.4f}, w={best['w']:.4f}, b={best['b']:.4f}")

    # ==================== å¯è§†åŒ– ====================
    fig = plt.figure(figsize=(16, 5))

    # å­å›¾1ï¼šå­¦ä¹ ç‡ vs æœ€ç»ˆæŸå¤±ï¼ˆå…³é”®å›¾ï¼‰
    plt.subplot(1, 3, 1)
    lrs = [r['lr'] for r in results]
    losses = [r['loss'] for r in results]

    # åˆ†ç¦»æ”¶æ•›å’Œå‘æ•£çš„ç‚¹
    converged_lrs = [r['lr'] for r in results if r['converged']]
    converged_losses = [r['loss'] for r in results if r['converged']]
    diverged_lrs = [r['lr'] for r in results if not r['converged']]
    diverged_losses = [r['loss'] for r in results if not r['converged']]

    plt.plot(converged_lrs, converged_losses, 'o-', linewidth=2, markersize=8, label='Converged')
    if diverged_lrs:
        plt.plot(diverged_lrs, diverged_losses, 'rx', markersize=10, label='Diverged')

    # æ ‡æ³¨æœ€ä¼˜ç‚¹
    if converged_results:
        plt.plot(best['lr'], best['loss'], 'g*', markersize=20, label=f"Best: lr={best['lr']}")

    plt.xlabel('Learning Rate', fontsize=12)
    plt.ylabel('Final Loss', fontsize=12)
    plt.title('Learning Rate vs Final Loss', fontsize=14, fontweight='bold')
    plt.legend()
    plt.grid(True, alpha=0.3)
    plt.yscale('log')  # å¯¹æ•°åæ ‡ï¼Œæ–¹ä¾¿çœ‹å‘æ•£çš„æƒ…å†µ

    # å­å›¾2ï¼šè®­ç»ƒè¿‡ç¨‹å¯¹æ¯”ï¼ˆåªæ˜¾ç¤ºæ”¶æ•›çš„ï¼‰
    plt.subplot(1, 3, 2)
    for r in converged_results:
        plt.plot(r['history'], label=f"lr={r['lr']}", alpha=0.7)
    plt.xlabel('Iterations')
    plt.ylabel('Loss')
    plt.title('Training Process (Converged Only)')
    plt.legend()
    plt.grid(True, alpha=0.3)

    # å­å›¾3ï¼šå‚æ•°æ”¶æ•›æƒ…å†µï¼ˆw å’Œ bï¼‰
    plt.subplot(1, 3, 3)
    true_w, true_b = 3, 2  # çœŸå®å€¼
    ws = [r['w'] for r in converged_results]
    bs = [r['b'] for r in converged_results]
    lrs_conv = [r['lr'] for r in converged_results]

    plt.plot(lrs_conv, ws, 'o-', label='Learned w', markersize=8)
    plt.axhline(y=true_w, color='b', linestyle='--', alpha=0.5, label='True w=3')
    plt.plot(lrs_conv, bs, 's-', label='Learned b', markersize=8)
    plt.axhline(y=true_b, color='orange', linestyle='--', alpha=0.5, label='True b=2')
    plt.xlabel('Learning Rate')
    plt.ylabel('Parameter Value')
    plt.title('Parameters vs Learning Rate')
    plt.legend()
    plt.grid(True, alpha=0.3)

    plt.tight_layout()
    plt.savefig('learning_rate_sweep.png', dpi=100)
    print("\nğŸ“Š å›¾è¡¨å·²ä¿å­˜åˆ°: learning_rate_sweep.png")
    plt.show()


if __name__ == "__main__":
    # åŸºç¡€è®­ç»ƒ
    main()

    # å¯é€‰ï¼šè¿è¡Œå®éªŒï¼ˆå–æ¶ˆæ³¨é‡Šä¸‹é¢çš„è¡Œï¼‰
    # experiment()  # å¯¹æ¯”å‡ ä¸ªå­¦ä¹ ç‡çš„è®­ç»ƒè¿‡ç¨‹
    experiment_lr_sweep()  # å­¦ä¹ ç‡æ‰«æå®éªŒï¼ˆæ¨èï¼ï¼‰

    print("\nğŸ’¡ æç¤ºï¼š")
    print("   - ä¿®æ”¹ learning_rate å’Œ n_iterations è§‚å¯Ÿå˜åŒ–")
    print("   - experiment(): å¯¹æ¯”å‡ ä¸ªå­¦ä¹ ç‡çš„è®­ç»ƒè¿‡ç¨‹")
    print("   - experiment_lr_sweep(): æ‰«æå­¦ä¹ ç‡èŒƒå›´ï¼Œæ‰¾æœ€ä¼˜å€¼ï¼ˆæ¨èï¼‰")
    print("   - å°è¯•è‡ªå·±æ·»åŠ æ–°çš„å®éªŒï¼")
