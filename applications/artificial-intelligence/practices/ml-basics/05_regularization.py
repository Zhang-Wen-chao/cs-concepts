"""
æ­£åˆ™åŒ– (Regularization)

é—®é¢˜ï¼šæ¨¡å‹åœ¨è®­ç»ƒé›†ä¸Šè¡¨ç°å¾ˆå¥½ï¼Œä½†åœ¨æµ‹è¯•é›†ä¸Šè¡¨ç°å·®ï¼ˆè¿‡æ‹Ÿåˆï¼‰
ç›®æ ‡ï¼šé€šè¿‡æ­£åˆ™åŒ–æŠ€æœ¯é˜²æ­¢è¿‡æ‹Ÿåˆï¼Œæé«˜æ¨¡å‹çš„æ³›åŒ–èƒ½åŠ›

æ ¸å¿ƒæ¦‚å¿µï¼š
1. è¿‡æ‹Ÿåˆ (Overfitting)ï¼šæ¨¡å‹è¿‡åº¦æ‹Ÿåˆè®­ç»ƒæ•°æ®ï¼ŒåŒ…æ‹¬å™ªå£°
2. æ¬ æ‹Ÿåˆ (Underfitting)ï¼šæ¨¡å‹å¤ªç®€å•ï¼Œæ— æ³•æ•æ‰æ•°æ®çš„è§„å¾‹
3. L1 æ­£åˆ™åŒ– (Lasso)ï¼šæƒ©ç½šæƒé‡çš„ç»å¯¹å€¼ï¼Œäº§ç”Ÿç¨€ç–è§£
4. L2 æ­£åˆ™åŒ– (Ridge)ï¼šæƒ©ç½šæƒé‡çš„å¹³æ–¹ï¼Œæƒé‡è¡°å‡
5. æ­£åˆ™åŒ–å¼ºåº¦ Î» (lambda)ï¼šæ§åˆ¶æ­£åˆ™åŒ–çš„ç¨‹åº¦

å…³é”®æ€æƒ³ï¼šåœ¨æŸå¤±å‡½æ•°ä¸­æ·»åŠ å¯¹æƒé‡å¤§å°çš„æƒ©ç½š
"""

import numpy as np
import matplotlib.pyplot as plt
from sklearn.datasets import make_regression
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler, PolynomialFeatures


# ==================== 1. æ ¸å¿ƒæ¦‚å¿µ ====================
def add_polynomial_features(X, degree):
    """
    æ·»åŠ å¤šé¡¹å¼ç‰¹å¾

    ä¾‹å¦‚ï¼šX = [x1, x2]
    degree=1: [x1, x2]
    degree=2: [x1, x2, x1Â², x1*x2, x2Â²]
    degree=3: [x1, x2, x1Â², x1*x2, x2Â², x1Â³, x1Â²*x2, x1*x2Â², x2Â³]

    å¤šé¡¹å¼ç‰¹å¾å¯ä»¥è®©çº¿æ€§æ¨¡å‹æ‹Ÿåˆéçº¿æ€§å…³ç³»
    ä½†ä¹Ÿå®¹æ˜“å¯¼è‡´è¿‡æ‹Ÿåˆï¼ˆç‰¹å¾å¤ªå¤šï¼‰
    """
    poly = PolynomialFeatures(degree, include_bias=False)
    return poly.fit_transform(X)


# ==================== 2. å¸¦æ­£åˆ™åŒ–çš„çº¿æ€§å›å½’ ====================
class RegularizedLinearRegression:
    """
    å¸¦æ­£åˆ™åŒ–çš„çº¿æ€§å›å½’

    æ”¯æŒä¸‰ç§æ¨¡å¼ï¼š
    1. æ— æ­£åˆ™åŒ–ï¼ˆæ™®é€šçº¿æ€§å›å½’ï¼‰
    2. L1 æ­£åˆ™åŒ–ï¼ˆLassoï¼‰
    3. L2 æ­£åˆ™åŒ–ï¼ˆRidgeï¼‰
    """

    def __init__(self, regularization='none', lambda_=0.1, learning_rate=0.01,
                 n_epochs=1000, batch_size=32):
        """
        å‚æ•°ï¼š
            regularization: 'none', 'l1', 'l2'
            lambda_: æ­£åˆ™åŒ–å¼ºåº¦ï¼ˆÎ»ï¼‰
            learning_rate: å­¦ä¹ ç‡
            n_epochs: è®­ç»ƒè½®æ•°
            batch_size: æ‰¹é‡å¤§å°
        """
        self.regularization = regularization
        self.lambda_ = lambda_
        self.lr = learning_rate
        self.n_epochs = n_epochs
        self.batch_size = batch_size
        self.weights = None
        self.bias = None
        self.loss_history = []

    def fit(self, X, y):
        """
        è®­ç»ƒæ¨¡å‹

        ====================================================================
        ğŸ”‘ æ­£åˆ™åŒ–çš„æ•°å­¦åŸç†
        ====================================================================

        ã€æ— æ­£åˆ™åŒ–ã€‘
        Loss = MSE = 1/n * Î£(y - Å·)Â²

        ã€L2 æ­£åˆ™åŒ– (Ridge)ã€‘
        Loss = MSE + Î» * Î£wÂ²
             = 1/n * Î£(y - Å·)Â² + Î» * ||W||Â²

        æƒ©ç½šæƒé‡çš„å¹³æ–¹å’Œ
        - Î» = 0ï¼šæ— æ­£åˆ™åŒ–
        - Î» å¤§ï¼šæƒé‡è¢«å‹ç¼©åˆ°æ¥è¿‘ 0
        - æ‰€æœ‰æƒé‡éƒ½å˜å°ï¼Œä½†ä¸ä¼šå˜æˆ 0
        - ä¹Ÿå«"æƒé‡è¡°å‡"ï¼ˆWeight Decayï¼‰

        æ¢¯åº¦ï¼š
        âˆ‚Loss/âˆ‚w = âˆ‚MSE/âˆ‚w + 2Î»w
                 = (Å· - y) * x + 2Î»w

        ã€L1 æ­£åˆ™åŒ– (Lasso)ã€‘
        Loss = MSE + Î» * Î£|w|
             = 1/n * Î£(y - Å·)Â² + Î» * ||W||â‚

        æƒ©ç½šæƒé‡çš„ç»å¯¹å€¼
        - Î» = 0ï¼šæ— æ­£åˆ™åŒ–
        - Î» å¤§ï¼šå¾ˆå¤šæƒé‡è¢«å‹ç¼©åˆ° 0ï¼ˆç¨€ç–è§£ï¼‰
        - å¯ä»¥ç”¨äºç‰¹å¾é€‰æ‹©ï¼ˆæƒé‡ä¸º 0 = ç‰¹å¾ä¸é‡è¦ï¼‰

        æ¢¯åº¦ï¼š
        âˆ‚Loss/âˆ‚w = âˆ‚MSE/âˆ‚w + Î» * sign(w)
                 = (Å· - y) * x + Î» * sign(w)

        å…¶ä¸­ sign(w) = +1 if w>0, -1 if w<0, 0 if w=0

        ====================================================================
        ğŸ”‘ L1 vs L2 çš„ç›´è§‰ç†è§£
        ====================================================================

        æƒ³è±¡åœ¨äºŒç»´ç©ºé—´ä¸­æ‰¾æœ€ä¼˜æƒé‡ (w1, w2)

        æ— æ­£åˆ™åŒ–ï¼š
        - æŸå¤±å‡½æ•°æ˜¯ä¸ªç¢—å½¢
        - æœ€ä¼˜ç‚¹åœ¨ç¢—åº•

        L2 æ­£åˆ™åŒ–ï¼š
        - æŸå¤±å‡½æ•° = åŸå§‹ç¢— + ä»¥åŸç‚¹ä¸ºä¸­å¿ƒçš„åœ†å½¢"å±±ä¸˜"
        - æœ€ä¼˜ç‚¹è¢«"æ¨"å‘åŸç‚¹
        - æƒé‡å˜å°ï¼Œä½†ä¸ä¼šå˜ 0
        - ç­‰é«˜çº¿æ˜¯åœ†å½¢

        L1 æ­£åˆ™åŒ–ï¼š
        - æŸå¤±å‡½æ•° = åŸå§‹ç¢— + ä»¥åŸç‚¹ä¸ºä¸­å¿ƒçš„è±å½¢"å±±ä¸˜"
        - è±å½¢çš„å°–è§’åœ¨åæ ‡è½´ä¸Š
        - æœ€ä¼˜ç‚¹å®¹æ˜“è½åœ¨åæ ‡è½´ä¸Šï¼ˆæŸä¸ªæƒé‡ = 0ï¼‰
        - ç­‰é«˜çº¿æ˜¯è±å½¢

        ====================================================================
        ğŸ”‘ ä¸ºä»€ä¹ˆæ­£åˆ™åŒ–å¯ä»¥é˜²æ­¢è¿‡æ‹Ÿåˆï¼Ÿ
        ====================================================================

        è¿‡æ‹Ÿåˆçš„ç‰¹å¾ï¼š
        - æƒé‡å¾ˆå¤§ï¼ˆå¯¹è®­ç»ƒæ•°æ®çš„å°å˜åŒ–éå¸¸æ•æ„Ÿï¼‰
        - æ¨¡å‹è¿‡äºå¤æ‚ï¼ˆé«˜æ¬¡å¤šé¡¹å¼ï¼Œç‰¹å¾å¾ˆå¤šï¼‰

        æ­£åˆ™åŒ–çš„ä½œç”¨ï¼š
        1. é™åˆ¶æƒé‡å¤§å° â†’ æ¨¡å‹æ›´å¹³æ»‘
        2. L1 è®©ä¸€äº›æƒé‡ä¸º 0 â†’ ç®€åŒ–æ¨¡å‹
        3. å¼ºåˆ¶æ¨¡å‹å…³æ³¨é‡è¦ç‰¹å¾ â†’ å‡å°‘å™ªå£°å½±å“

        ç±»æ¯”ï¼š
        - æ— æ­£åˆ™åŒ– = è®°ä½æ‰€æœ‰è®­ç»ƒæ ·æœ¬çš„ç»†èŠ‚ï¼ˆåŒ…æ‹¬å™ªå£°ï¼‰
        - æ­£åˆ™åŒ– = åªè®°ä½ä¸»è¦è§„å¾‹ï¼Œå¿½ç•¥ç»†èŠ‚

        ====================================================================
        """
        n_samples, n_features = X.shape

        # åˆå§‹åŒ–å‚æ•°
        self.weights = np.random.randn(n_features) * 0.01
        self.bias = 0

        # Mini-batch æ¢¯åº¦ä¸‹é™
        for epoch in range(self.n_epochs):
            indices = np.random.permutation(n_samples)

            for start_idx in range(0, n_samples, self.batch_size):
                batch_indices = indices[start_idx:start_idx + self.batch_size]
                X_batch = X[batch_indices]
                y_batch = y[batch_indices]

                # å‰å‘ä¼ æ’­
                y_pred = np.dot(X_batch, self.weights) + self.bias

                # è®¡ç®—æ¢¯åº¦
                batch_size_actual = len(X_batch)
                error = y_pred - y_batch

                # MSE çš„æ¢¯åº¦
                dw = (1 / batch_size_actual) * np.dot(X_batch.T, error)
                db = (1 / batch_size_actual) * np.sum(error)

                # æ·»åŠ æ­£åˆ™åŒ–é¡¹çš„æ¢¯åº¦
                if self.regularization == 'l2':
                    # L2: âˆ‚(Î»||W||Â²)/âˆ‚w = 2Î»w
                    dw += 2 * self.lambda_ * self.weights

                elif self.regularization == 'l1':
                    # L1: âˆ‚(Î»||W||â‚)/âˆ‚w = Î» * sign(w)
                    dw += self.lambda_ * np.sign(self.weights)

                # æ›´æ–°å‚æ•°
                self.weights -= self.lr * dw
                self.bias -= self.lr * db

            # è®°å½•æŸå¤±
            if epoch % 10 == 0:
                y_pred_all = np.dot(X, self.weights) + self.bias
                mse = np.mean((y - y_pred_all) ** 2)

                # æ·»åŠ æ­£åˆ™åŒ–é¡¹åˆ°æŸå¤±
                if self.regularization == 'l2':
                    reg_term = self.lambda_ * np.sum(self.weights ** 2)
                elif self.regularization == 'l1':
                    reg_term = self.lambda_ * np.sum(np.abs(self.weights))
                else:
                    reg_term = 0

                total_loss = mse + reg_term
                self.loss_history.append(total_loss)

    def predict(self, X):
        """é¢„æµ‹"""
        return np.dot(X, self.weights) + self.bias

    def score(self, X, y):
        """è®¡ç®— RÂ² åˆ†æ•°"""
        y_pred = self.predict(X)
        ss_res = np.sum((y - y_pred) ** 2)
        ss_tot = np.sum((y - np.mean(y)) ** 2)
        return 1 - (ss_res / ss_tot)


# ==================== 3. æ•°æ®ç”Ÿæˆ ====================
def generate_overfitting_data(n_samples=100, noise=10, random_state=42):
    """
    ç”Ÿæˆå®¹æ˜“è¿‡æ‹Ÿåˆçš„æ•°æ®

    ç­–ç•¥ï¼šæ ·æœ¬å°‘ + å™ªå£°å¤§ + åé¢ä¼šåŠ é«˜æ¬¡å¤šé¡¹å¼ç‰¹å¾
    """
    np.random.seed(random_state)

    # çœŸå®å…³ç³»ï¼šy = 2x + 1 + å™ªå£°
    X = np.random.uniform(-3, 3, (n_samples, 1))
    y = 2 * X.ravel() + 1 + np.random.randn(n_samples) * noise

    return X, y


# ==================== 4. å¯è§†åŒ– ====================
def plot_overfitting_demo():
    """
    æ¼”ç¤ºè¿‡æ‹Ÿåˆç°è±¡
    """
    print("=" * 70)
    print("ğŸ“Š æ¼”ç¤ºï¼šä»€ä¹ˆæ˜¯è¿‡æ‹Ÿåˆï¼Ÿ")
    print("=" * 70)

    # ç”Ÿæˆæ•°æ®
    X_train, y_train = generate_overfitting_data(n_samples=20, noise=2)
    X_test, y_test = generate_overfitting_data(n_samples=100, noise=2, random_state=123)

    # ä¸‰ç§æ¨¡å‹ï¼šæ¬ æ‹Ÿåˆã€é€‚ä¸­ã€è¿‡æ‹Ÿåˆ
    degrees = [1, 3, 15]
    titles = ['Underfitting (åº¦=1)', 'Good Fit (åº¦=3)', 'Overfitting (åº¦=15)']

    fig, axes = plt.subplots(1, 3, figsize=(18, 5))

    for idx, (degree, title) in enumerate(zip(degrees, titles)):
        ax = axes[idx]

        # æ·»åŠ å¤šé¡¹å¼ç‰¹å¾
        X_train_poly = add_polynomial_features(X_train, degree)
        X_test_poly = add_polynomial_features(X_test, degree)

        # è®­ç»ƒæ¨¡å‹ï¼ˆæ— æ­£åˆ™åŒ–ï¼‰
        model = RegularizedLinearRegression(regularization='none',
                                            learning_rate=0.01, n_epochs=1000)
        model.fit(X_train_poly, y_train)

        # è¯„ä¼°
        train_score = model.score(X_train_poly, y_train)
        test_score = model.score(X_test_poly, y_test)

        # ç»˜åˆ¶
        X_plot = np.linspace(-3, 3, 300).reshape(-1, 1)
        X_plot_poly = add_polynomial_features(X_plot, degree)
        y_plot = model.predict(X_plot_poly)

        ax.scatter(X_train, y_train, s=50, alpha=0.7, label='Training data', color='blue')
        ax.plot(X_plot, y_plot, color='red', linewidth=2, label='Model')
        ax.set_xlabel('X', fontsize=12)
        ax.set_ylabel('y', fontsize=12)
        ax.set_title(f'{title}\nTrain RÂ²={train_score:.3f}, Test RÂ²={test_score:.3f}',
                     fontsize=12, fontweight='bold')
        ax.legend(fontsize=10)
        ax.grid(True, alpha=0.3)
        ax.set_ylim(-10, 10)

        # æ–‡å­—è¯´æ˜
        if idx == 0:
            ax.text(0, -8, 'æ¨¡å‹å¤ªç®€å•\næ— æ³•æ•æ‰è§„å¾‹', ha='center',
                   bbox=dict(boxstyle='round', facecolor='yellow', alpha=0.5))
        elif idx == 1:
            ax.text(0, -8, 'æ°åˆ°å¥½å¤„\næ³›åŒ–èƒ½åŠ›å¼º', ha='center',
                   bbox=dict(boxstyle='round', facecolor='lightgreen', alpha=0.5))
        else:
            ax.text(0, -8, 'è¿‡åº¦æ‹Ÿåˆè®­ç»ƒæ•°æ®\næ³›åŒ–èƒ½åŠ›å·®', ha='center',
                   bbox=dict(boxstyle='round', facecolor='red', alpha=0.3))

    plt.tight_layout()
    plt.savefig('overfitting_demo.png', dpi=100)
    print("\nğŸ“Š è¿‡æ‹Ÿåˆæ¼”ç¤ºå›¾å·²ä¿å­˜åˆ°: overfitting_demo.png")
    plt.show()

    print("\nğŸ’¡ è§‚å¯Ÿï¼š")
    print("  åº¦=1ï¼šæ¬ æ‹Ÿåˆ â†’ è®­ç»ƒå’Œæµ‹è¯• RÂ² éƒ½ä½ï¼ˆæ¨¡å‹å¤ªç®€å•ï¼‰")
    print("  åº¦=3ï¼šé€‚ä¸­ â†’ è®­ç»ƒå’Œæµ‹è¯• RÂ² éƒ½é«˜ï¼ˆæ°åˆ°å¥½å¤„ï¼‰")
    print("  åº¦=15ï¼šè¿‡æ‹Ÿåˆ â†’ è®­ç»ƒ RÂ² é«˜ï¼Œæµ‹è¯• RÂ² ä½ï¼ˆè®°ä½äº†å™ªå£°ï¼‰")


def compare_regularization(X_train, y_train, X_test, y_test, degree=10):
    """
    å¯¹æ¯”ä¸åŒæ­£åˆ™åŒ–æ–¹æ³•
    """
    print("\n" + "=" * 70)
    print("ğŸ”¬ å¯¹æ¯”ï¼šL1 vs L2 vs æ— æ­£åˆ™åŒ–")
    print("=" * 70)

    # æ·»åŠ å¤šé¡¹å¼ç‰¹å¾
    X_train_poly = add_polynomial_features(X_train, degree)
    X_test_poly = add_polynomial_features(X_test, degree)

    print(f"\nå¤šé¡¹å¼åº¦æ•°ï¼š{degree}")
    print(f"ç‰¹å¾æ•°é‡ï¼š{X_train_poly.shape[1]}")

    # ä¸‰ç§æ¨¡å‹
    configs = [
        ('none', 0, 'No Regularization'),
        ('l2', 0.1, 'L2 (Ridge) Î»=0.1'),
        ('l1', 0.1, 'L1 (Lasso) Î»=0.1')
    ]

    fig, axes = plt.subplots(2, 3, figsize=(18, 10))

    for idx, (reg_type, lambda_, title) in enumerate(configs):
        # è®­ç»ƒæ¨¡å‹
        model = RegularizedLinearRegression(
            regularization=reg_type,
            lambda_=lambda_,
            learning_rate=0.01,
            n_epochs=1000
        )
        model.fit(X_train_poly, y_train)

        # è¯„ä¼°
        train_score = model.score(X_train_poly, y_train)
        test_score = model.score(X_test_poly, y_test)

        # å­å›¾1ï¼šæ‹Ÿåˆæ›²çº¿
        ax1 = axes[0, idx]
        X_plot = np.linspace(-3, 3, 300).reshape(-1, 1)
        X_plot_poly = add_polynomial_features(X_plot, degree)
        y_plot = model.predict(X_plot_poly)

        ax1.scatter(X_train, y_train, s=50, alpha=0.7, label='Training', color='blue')
        ax1.scatter(X_test, y_test, s=20, alpha=0.3, label='Test', color='green')
        ax1.plot(X_plot, y_plot, color='red', linewidth=2, label='Model')
        ax1.set_xlabel('X', fontsize=11)
        ax1.set_ylabel('y', fontsize=11)
        ax1.set_title(f'{title}\nTrain RÂ²={train_score:.3f}, Test RÂ²={test_score:.3f}',
                     fontsize=11, fontweight='bold')
        ax1.legend(fontsize=9)
        ax1.grid(True, alpha=0.3)
        ax1.set_ylim(-15, 15)

        # å­å›¾2ï¼šæƒé‡åˆ†å¸ƒ
        ax2 = axes[1, idx]
        weights = model.weights
        ax2.bar(range(len(weights)), weights, alpha=0.7, color='purple')
        ax2.axhline(y=0, color='red', linestyle='--', linewidth=1)
        ax2.set_xlabel('Feature Index', fontsize=11)
        ax2.set_ylabel('Weight Value', fontsize=11)
        ax2.set_title(f'Weight Distribution\nNon-zero: {np.sum(np.abs(weights) > 0.01)}/{len(weights)}',
                     fontsize=11, fontweight='bold')
        ax2.grid(True, alpha=0.3)

        # æ‰“å°ç»“æœ
        print(f"\n{title}:")
        print(f"  è®­ç»ƒ RÂ²: {train_score:.4f}")
        print(f"  æµ‹è¯• RÂ²: {test_score:.4f}")
        print(f"  éé›¶æƒé‡æ•°: {np.sum(np.abs(weights) > 0.01)}/{len(weights)}")
        print(f"  æƒé‡èŒƒå›´: [{weights.min():.2f}, {weights.max():.2f}]")

    plt.tight_layout()
    plt.savefig('regularization_comparison.png', dpi=100)
    print("\nğŸ“Š æ­£åˆ™åŒ–å¯¹æ¯”å›¾å·²ä¿å­˜åˆ°: regularization_comparison.png")
    plt.show()


def lambda_sweep(X_train, y_train, X_test, y_test, degree=10):
    """
    æ¢ç´¢æ­£åˆ™åŒ–å¼ºåº¦ Î» çš„å½±å“
    """
    print("\n" + "=" * 70)
    print("ğŸ§ª å®éªŒï¼šæ­£åˆ™åŒ–å¼ºåº¦ Î» çš„å½±å“")
    print("=" * 70)

    X_train_poly = add_polynomial_features(X_train, degree)
    X_test_poly = add_polynomial_features(X_test, degree)

    # æµ‹è¯•ä¸åŒçš„ Î» å€¼
    lambdas = np.logspace(-4, 2, 20)  # 0.0001 åˆ° 100

    results_l1 = {'train': [], 'test': [], 'non_zero': []}
    results_l2 = {'train': [], 'test': [], 'non_zero': []}

    for lambda_ in lambdas:
        # L1
        model_l1 = RegularizedLinearRegression('l1', lambda_, learning_rate=0.01, n_epochs=1000)
        model_l1.fit(X_train_poly, y_train)
        results_l1['train'].append(model_l1.score(X_train_poly, y_train))
        results_l1['test'].append(model_l1.score(X_test_poly, y_test))
        results_l1['non_zero'].append(np.sum(np.abs(model_l1.weights) > 0.01))

        # L2
        model_l2 = RegularizedLinearRegression('l2', lambda_, learning_rate=0.01, n_epochs=1000)
        model_l2.fit(X_train_poly, y_train)
        results_l2['train'].append(model_l2.score(X_train_poly, y_train))
        results_l2['test'].append(model_l2.score(X_test_poly, y_test))
        results_l2['non_zero'].append(np.sum(np.abs(model_l2.weights) > 0.01))

    # å¯è§†åŒ–
    fig, axes = plt.subplots(1, 2, figsize=(14, 5))

    # RÂ² vs Î»
    ax1 = axes[0]
    ax1.semilogx(lambdas, results_l1['test'], 'o-', label='L1 Test', linewidth=2, color='blue')
    ax1.semilogx(lambdas, results_l2['test'], 's-', label='L2 Test', linewidth=2, color='red')
    ax1.semilogx(lambdas, results_l1['train'], 'o--', label='L1 Train', linewidth=1,
                 alpha=0.5, color='blue')
    ax1.semilogx(lambdas, results_l2['train'], 's--', label='L2 Train', linewidth=1,
                 alpha=0.5, color='red')
    ax1.set_xlabel('Î» (Regularization Strength)', fontsize=12)
    ax1.set_ylabel('RÂ² Score', fontsize=12)
    ax1.set_title('Model Performance vs Î»', fontsize=13, fontweight='bold')
    ax1.legend(fontsize=10)
    ax1.grid(True, alpha=0.3)

    # éé›¶æƒé‡æ•° vs Î»
    ax2 = axes[1]
    ax2.semilogx(lambdas, results_l1['non_zero'], 'o-', label='L1', linewidth=2, color='blue')
    ax2.semilogx(lambdas, results_l2['non_zero'], 's-', label='L2', linewidth=2, color='red')
    ax2.set_xlabel('Î» (Regularization Strength)', fontsize=12)
    ax2.set_ylabel('Number of Non-zero Weights', fontsize=12)
    ax2.set_title('Model Sparsity vs Î»', fontsize=13, fontweight='bold')
    ax2.legend(fontsize=10)
    ax2.grid(True, alpha=0.3)

    plt.tight_layout()
    plt.savefig('lambda_sweep.png', dpi=100)
    print("\nğŸ“Š Î» æ‰«æå›¾å·²ä¿å­˜åˆ°: lambda_sweep.png")
    plt.show()

    print("\nğŸ’¡ è§‚å¯Ÿï¼š")
    print("  Î» å¤ªå° â†’ å‡ ä¹æ— æ­£åˆ™åŒ–ï¼Œå¯èƒ½è¿‡æ‹Ÿåˆ")
    print("  Î» é€‚ä¸­ â†’ æµ‹è¯• RÂ² è¾¾åˆ°å³°å€¼ï¼ˆæœ€ä½³æ³›åŒ–ï¼‰")
    print("  Î» å¤ªå¤§ â†’ æƒé‡è¢«è¿‡åº¦å‹ç¼©ï¼Œæ¬ æ‹Ÿåˆ")
    print("\n  L1 ç‰¹æ€§ï¼šéšç€ Î» å¢å¤§ï¼Œéé›¶æƒé‡æ•°å‡å°‘ï¼ˆç¨€ç–è§£ï¼‰")
    print("  L2 ç‰¹æ€§ï¼šæƒé‡å˜å°ä½†ä¸ä¸º 0ï¼ˆå¯†é›†è§£ï¼‰")


# ==================== 5. ä¸»ç¨‹åº ====================
def main():
    print("=" * 70)
    print("æ­£åˆ™åŒ– (Regularization) - é˜²æ­¢è¿‡æ‹Ÿåˆ")
    print("=" * 70)

    # 1. æ¼”ç¤ºè¿‡æ‹Ÿåˆ
    plot_overfitting_demo()

    # 2. ç”Ÿæˆæ•°æ®
    print("\n" + "=" * 70)
    print("ğŸ“Š ç”Ÿæˆè®­ç»ƒå’Œæµ‹è¯•æ•°æ®")
    print("=" * 70)
    X_train, y_train = generate_overfitting_data(n_samples=30, noise=3)
    X_test, y_test = generate_overfitting_data(n_samples=100, noise=3, random_state=123)
    print(f"è®­ç»ƒé›†: {len(X_train)} ä¸ªæ ·æœ¬")
    print(f"æµ‹è¯•é›†: {len(X_test)} ä¸ªæ ·æœ¬")

    # 3. å¯¹æ¯”æ­£åˆ™åŒ–æ–¹æ³•
    compare_regularization(X_train, y_train, X_test, y_test, degree=10)

    # 4. Î» æ‰«æ
    lambda_sweep(X_train, y_train, X_test, y_test, degree=10)

    # 5. æ€»ç»“
    print("\n" + "=" * 70)
    print("âœ… æ ¸å¿ƒè¦ç‚¹æ€»ç»“")
    print("=" * 70)
    print("""
1. è¿‡æ‹Ÿåˆ vs æ¬ æ‹Ÿåˆ
   - è¿‡æ‹Ÿåˆï¼šè®­ç»ƒå¥½ï¼Œæµ‹è¯•å·®ï¼ˆæ¨¡å‹å¤ªå¤æ‚ï¼‰
   - æ¬ æ‹Ÿåˆï¼šè®­ç»ƒå·®ï¼Œæµ‹è¯•å·®ï¼ˆæ¨¡å‹å¤ªç®€å•ï¼‰
   - ç›®æ ‡ï¼šæ‰¾åˆ°æ°å½“çš„æ¨¡å‹å¤æ‚åº¦

2. æ­£åˆ™åŒ–çš„ä½œç”¨
   - åœ¨æŸå¤±å‡½æ•°ä¸­æ·»åŠ å¯¹æƒé‡çš„æƒ©ç½š
   - é˜²æ­¢æƒé‡è¿‡å¤§ â†’ æ¨¡å‹æ›´å¹³æ»‘
   - å¼ºåˆ¶æ¨¡å‹ç®€åŒ– â†’ æé«˜æ³›åŒ–èƒ½åŠ›

3. L1 æ­£åˆ™åŒ– (Lasso)
   å…¬å¼ï¼šLoss = MSE + Î» * Î£|w|
   ç‰¹ç‚¹ï¼š
   - äº§ç”Ÿç¨€ç–è§£ï¼ˆå¾ˆå¤šæƒé‡ = 0ï¼‰
   - å¯ç”¨äºç‰¹å¾é€‰æ‹©
   - æ¢¯åº¦ï¼šÎ» * sign(w)

4. L2 æ­£åˆ™åŒ– (Ridge)
   å…¬å¼ï¼šLoss = MSE + Î» * Î£wÂ²
   ç‰¹ç‚¹ï¼š
   - æƒé‡å˜å°ä½†ä¸ä¸º 0
   - æ‰€æœ‰ç‰¹å¾éƒ½ä¿ç•™
   - ä¹Ÿå«"æƒé‡è¡°å‡"
   - æ¢¯åº¦ï¼š2Î»w

5. é€‰æ‹© Î» (æ­£åˆ™åŒ–å¼ºåº¦)
   - Î» = 0ï¼šæ— æ­£åˆ™åŒ–
   - Î» å°ï¼šè½»å¾®æ­£åˆ™åŒ–
   - Î» é€‚ä¸­ï¼šæœ€ä½³æ³›åŒ–ï¼ˆé€šè¿‡äº¤å‰éªŒè¯æ‰¾åˆ°ï¼‰
   - Î» å¤§ï¼šæ¬ æ‹Ÿåˆ

6. L1 vs L2 å¦‚ä½•é€‰æ‹©ï¼Ÿ
   - ç‰¹å¾å¾ˆå¤šï¼Œæ€€ç–‘å¾ˆå¤šä¸é‡è¦ â†’ L1ï¼ˆè‡ªåŠ¨ç‰¹å¾é€‰æ‹©ï¼‰
   - æ‰€æœ‰ç‰¹å¾éƒ½å¯èƒ½æœ‰ç”¨ â†’ L2ï¼ˆæ›´ç¨³å®šï¼‰
   - ä¸ç¡®å®š â†’ éƒ½è¯•è¯•ï¼Œæˆ–ç”¨ Elastic Netï¼ˆL1+L2ï¼‰

7. åº”ç”¨åœºæ™¯
   âœ“ é«˜ç»´æ•°æ®ï¼ˆç‰¹å¾æ•° >> æ ·æœ¬æ•°ï¼‰
   âœ“ å¤šé¡¹å¼ç‰¹å¾ï¼ˆå®¹æ˜“è¿‡æ‹Ÿåˆï¼‰
   âœ“ æ·±åº¦å­¦ä¹ ï¼ˆæƒé‡è¡°å‡æ˜¯æ ‡é…ï¼‰
   âœ“ ç‰¹å¾é€‰æ‹©ï¼ˆL1ï¼‰

8. å®è·µå»ºè®®
   - æ€»æ˜¯æ ‡å‡†åŒ–ç‰¹å¾ï¼ˆæ­£åˆ™åŒ–å¯¹ç‰¹å¾å°ºåº¦æ•æ„Ÿï¼‰
   - ç”¨äº¤å‰éªŒè¯é€‰æ‹©æœ€ä½³ Î»
   - è§‚å¯Ÿè®­ç»ƒ/æµ‹è¯•æ›²çº¿åˆ¤æ–­è¿‡æ‹Ÿåˆ
   - ä¸è¦æ­£åˆ™åŒ–åç½®é¡¹ bias
    """)


# ==================== 6. ä¸ sklearn å¯¹æ¯” ====================
def sklearn_comparison():
    print("\n" + "=" * 70)
    print("ğŸ”¬ ä¸ sklearn å¯¹æ¯”")
    print("=" * 70)

    from sklearn.linear_model import Ridge, Lasso

    # ç”Ÿæˆæ•°æ®
    X_train, y_train = generate_overfitting_data(n_samples=30, noise=3)
    X_test, y_test = generate_overfitting_data(n_samples=100, noise=3, random_state=123)

    X_train_poly = add_polynomial_features(X_train, 10)
    X_test_poly = add_polynomial_features(X_test, 10)

    # sklearn Ridge
    ridge = Ridge(alpha=0.1)
    ridge.fit(X_train_poly, y_train)
    print(f"\nsklearn Ridge:")
    print(f"  è®­ç»ƒ RÂ²: {ridge.score(X_train_poly, y_train):.4f}")
    print(f"  æµ‹è¯• RÂ²: {ridge.score(X_test_poly, y_test):.4f}")

    # sklearn Lasso
    lasso = Lasso(alpha=0.1, max_iter=5000)
    lasso.fit(X_train_poly, y_train)
    print(f"\nsklearn Lasso:")
    print(f"  è®­ç»ƒ RÂ²: {lasso.score(X_train_poly, y_train):.4f}")
    print(f"  æµ‹è¯• RÂ²: {lasso.score(X_test_poly, y_test):.4f}")
    print(f"  éé›¶æƒé‡: {np.sum(np.abs(lasso.coef_) > 0.01)}/{len(lasso.coef_)}")

    print(f"\nâœ… sklearn å®ç°æ›´ä¼˜åŒ–ï¼Œä½†åŸç†ç›¸åŒï¼")


if __name__ == "__main__":
    main()
    sklearn_comparison()

    print("\nğŸ’¡ ç»ƒä¹ å»ºè®®ï¼š")
    print("  1. ä¿®æ”¹å¤šé¡¹å¼åº¦æ•°ï¼ˆ5, 10, 15, 20ï¼‰ï¼Œè§‚å¯Ÿè¿‡æ‹Ÿåˆç¨‹åº¦")
    print("  2. å°è¯•ä¸åŒçš„ Î» å€¼ï¼Œæ‰¾åˆ°æœ€ä½³å€¼")
    print("  3. æ¯”è¾ƒ L1 å’Œ L2 åœ¨é«˜ç»´æ•°æ®ä¸Šçš„è¡¨ç°")
    print("  4. æ€è€ƒï¼šä¸ºä»€ä¹ˆ L1 èƒ½äº§ç”Ÿç¨€ç–è§£ï¼ŒL2 ä¸èƒ½ï¼Ÿ")
