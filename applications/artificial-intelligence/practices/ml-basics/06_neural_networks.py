"""
ç¥ç»ç½‘ç»œåŸºç¡€ (Neural Networks - Multi-Layer Perceptron)

é—®é¢˜ï¼šé€»è¾‘å›å½’åªèƒ½å­¦ä¹ çº¿æ€§å†³ç­–è¾¹ç•Œï¼Œå¦‚ä½•å¤„ç†éçº¿æ€§é—®é¢˜ï¼Ÿ
ç›®æ ‡ï¼šé€šè¿‡å¤šå±‚ç¥ç»ç½‘ç»œå­¦ä¹ éçº¿æ€§æ˜ å°„

æ ¸å¿ƒæ¦‚å¿µï¼š
1. å¤šå±‚ç»“æ„ï¼šè¾“å…¥å±‚ â†’ éšè—å±‚ â†’ è¾“å‡ºå±‚
2. éçº¿æ€§æ¿€æ´»ï¼šReLUã€Sigmoidã€Tanh
3. å‰å‘ä¼ æ’­ï¼šä»è¾“å…¥åˆ°è¾“å‡ºçš„è®¡ç®—è¿‡ç¨‹
4. åå‘ä¼ æ’­ï¼šä»è¾“å‡ºåˆ°è¾“å…¥çš„æ¢¯åº¦è®¡ç®—
5. é€šç”¨è¿‘ä¼¼å®šç†ï¼šå•éšè—å±‚ç¥ç»ç½‘ç»œå¯ä»¥é€¼è¿‘ä»»æ„è¿ç»­å‡½æ•°

ä»é€»è¾‘å›å½’åˆ°ç¥ç»ç½‘ç»œï¼š
- é€»è¾‘å›å½’ = å•å±‚ç¥ç»ç½‘ç»œï¼ˆçº¿æ€§ + Sigmoidï¼‰
- ç¥ç»ç½‘ç»œ = å¤šå±‚ + éçº¿æ€§æ¿€æ´»ï¼ˆå¯å­¦ä¹ éçº¿æ€§å…³ç³»ï¼‰
"""

import numpy as np
import matplotlib.pyplot as plt
from sklearn.datasets import make_moons, make_circles, make_classification
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler


# ==================== 1. æ¿€æ´»å‡½æ•° ====================
class ActivationFunctions:
    """
    æ¿€æ´»å‡½æ•°é›†åˆ

    ====================================================================
    ğŸ”‘ ä¸ºä»€ä¹ˆéœ€è¦æ¿€æ´»å‡½æ•°ï¼Ÿ
    ====================================================================

    å¦‚æœæ²¡æœ‰æ¿€æ´»å‡½æ•°ï¼ˆæˆ–åªç”¨çº¿æ€§æ¿€æ´»ï¼‰ï¼š
    å¤šå±‚ç½‘ç»œ = å•å±‚ç½‘ç»œ

    æ¨å¯¼ï¼š
    y = W2(W1x + b1) + b2
      = W2W1x + W2b1 + b2
      = W'x + b'  â† è¿˜æ˜¯çº¿æ€§çš„ï¼

    ç»“è®ºï¼šæ— è®ºå¤šå°‘å±‚ï¼Œæ²¡æœ‰éçº¿æ€§æ¿€æ´»ï¼Œç½‘ç»œåªèƒ½å­¦ä¹ çº¿æ€§å…³ç³»

    æ¿€æ´»å‡½æ•°çš„ä½œç”¨ï¼š
    1. å¼•å…¥éçº¿æ€§ â†’ ç½‘ç»œå¯ä»¥é€¼è¿‘ä»»æ„å‡½æ•°
    2. æ‰“ç ´å¯¹ç§°æ€§ â†’ ä¸åŒç¥ç»å…ƒå­¦ä¹ ä¸åŒç‰¹å¾
    3. æ§åˆ¶è¾“å‡ºèŒƒå›´ â†’ æ•°å€¼ç¨³å®š

    ====================================================================
    """

    @staticmethod
    def relu(z):
        """
        ReLU (Rectified Linear Unit)
        f(z) = max(0, z)

        ä¼˜ç‚¹ï¼š
        - è®¡ç®—ç®€å•
        - ç¼“è§£æ¢¯åº¦æ¶ˆå¤±ï¼ˆæ­£åŒºé—´æ¢¯åº¦=1ï¼‰
        - ç¨€ç–æ¿€æ´»ï¼ˆè´Ÿå€¼ä¸º0ï¼‰
        - æ·±åº¦å­¦ä¹ æœ€å¸¸ç”¨

        ç¼ºç‚¹ï¼š
        - è´ŸåŒºé—´æ¢¯åº¦=0ï¼ˆDead ReLUï¼‰
        - è¾“å‡ºæ— ä¸Šç•Œ
        """
        return np.maximum(0, z)

    @staticmethod
    def relu_derivative(z):
        """ReLU å¯¼æ•°ï¼š1 if z>0, else 0"""
        return (z > 0).astype(float)

    @staticmethod
    def sigmoid(z):
        """
        Sigmoid
        f(z) = 1 / (1 + e^(-z))

        ä¼˜ç‚¹ï¼š
        - è¾“å‡º (0, 1)ï¼Œå¯è§£é‡Šä¸ºæ¦‚ç‡
        - å¹³æ»‘è¿ç»­

        ç¼ºç‚¹ï¼š
        - æ¢¯åº¦æ¶ˆå¤±ï¼ˆä¸¤ç«¯æ¢¯åº¦æ¥è¿‘0ï¼‰
        - è¾“å‡ºéé›¶ä¸­å¿ƒï¼ˆå½±å“ä¼˜åŒ–ï¼‰
        - è®¡ç®— exp è¾ƒæ…¢
        """
        return 1 / (1 + np.exp(-np.clip(z, -500, 500)))

    @staticmethod
    def sigmoid_derivative(z):
        """Sigmoid å¯¼æ•°ï¼šÏƒ(z) * (1 - Ïƒ(z))"""
        s = ActivationFunctions.sigmoid(z)
        return s * (1 - s)

    @staticmethod
    def tanh(z):
        """
        Tanh (Hyperbolic Tangent)
        f(z) = (e^z - e^(-z)) / (e^z + e^(-z))

        ä¼˜ç‚¹ï¼š
        - è¾“å‡º (-1, 1)ï¼Œé›¶ä¸­å¿ƒ
        - æ¯” Sigmoid å¥½

        ç¼ºç‚¹ï¼š
        - ä»æœ‰æ¢¯åº¦æ¶ˆå¤±
        - è®¡ç®— exp è¾ƒæ…¢
        """
        return np.tanh(z)

    @staticmethod
    def tanh_derivative(z):
        """Tanh å¯¼æ•°ï¼š1 - tanhÂ²(z)"""
        t = np.tanh(z)
        return 1 - t ** 2


# ==================== 2. ç¥ç»ç½‘ç»œç±» ====================
class NeuralNetwork:
    """
    ä¸¤å±‚ç¥ç»ç½‘ç»œï¼ˆ1ä¸ªéšè—å±‚ï¼‰

    ç»“æ„ï¼š
    è¾“å…¥å±‚ (n_features) â†’ éšè—å±‚ (n_hidden) â†’ è¾“å‡ºå±‚ (n_outputs)
    """

    def __init__(self, n_features, n_hidden, n_outputs,
                 activation='relu', learning_rate=0.01, n_epochs=1000, batch_size=32):
        """
        å‚æ•°ï¼š
            n_features: è¾“å…¥ç‰¹å¾æ•°
            n_hidden: éšè—å±‚ç¥ç»å…ƒæ•°
            n_outputs: è¾“å‡ºæ•°ï¼ˆåˆ†ç±»ç±»åˆ«æ•°ï¼‰
            activation: éšè—å±‚æ¿€æ´»å‡½æ•° ('relu', 'sigmoid', 'tanh')
            learning_rate: å­¦ä¹ ç‡
            n_epochs: è®­ç»ƒè½®æ•°
            batch_size: æ‰¹é‡å¤§å°
        """
        self.n_features = n_features
        self.n_hidden = n_hidden
        self.n_outputs = n_outputs
        self.lr = learning_rate
        self.n_epochs = n_epochs
        self.batch_size = batch_size

        # é€‰æ‹©æ¿€æ´»å‡½æ•°
        self.activation_name = activation
        if activation == 'relu':
            self.activation = ActivationFunctions.relu
            self.activation_derivative = ActivationFunctions.relu_derivative
        elif activation == 'sigmoid':
            self.activation = ActivationFunctions.sigmoid
            self.activation_derivative = ActivationFunctions.sigmoid_derivative
        elif activation == 'tanh':
            self.activation = ActivationFunctions.tanh
            self.activation_derivative = ActivationFunctions.tanh_derivative
        else:
            raise ValueError(f"Unknown activation: {activation}")

        # åˆå§‹åŒ–å‚æ•°
        self._initialize_weights()

        self.loss_history = []

    def _initialize_weights(self):
        """
        å‚æ•°åˆå§‹åŒ–

        ====================================================================
        ğŸ”‘ ä¸ºä»€ä¹ˆéœ€è¦éšæœºåˆå§‹åŒ–ï¼Ÿ
        ====================================================================

        å¦‚æœæ‰€æœ‰æƒé‡åˆå§‹åŒ–ä¸º 0ï¼š
        - æ‰€æœ‰ç¥ç»å…ƒè®¡ç®—ç›¸åŒçš„ç»“æœ
        - æ‰€æœ‰æ¢¯åº¦ç›¸åŒ
        - æ— æ³•æ‰“ç ´å¯¹ç§°æ€§ â†’ ç½‘ç»œé€€åŒ–

        å¸¸ç”¨åˆå§‹åŒ–æ–¹æ³•ï¼š
        1. Xavier åˆå§‹åŒ–ï¼ˆtanh/sigmoidï¼‰
           W ~ N(0, 1/sqrt(n_in))

        2. He åˆå§‹åŒ–ï¼ˆReLUï¼‰
           W ~ N(0, 2/sqrt(n_in))

        3. å°éšæœºæ•°
           W ~ N(0, 0.01)

        ====================================================================
        """
        # He åˆå§‹åŒ–ï¼ˆé€‚åˆ ReLUï¼‰
        self.W1 = np.random.randn(self.n_features, self.n_hidden) * np.sqrt(2.0 / self.n_features)
        self.b1 = np.zeros((1, self.n_hidden))

        self.W2 = np.random.randn(self.n_hidden, self.n_outputs) * np.sqrt(2.0 / self.n_hidden)
        self.b2 = np.zeros((1, self.n_outputs))

    def forward(self, X):
        """
        å‰å‘ä¼ æ’­

        ====================================================================
        ğŸ”‘ å‰å‘ä¼ æ’­æµç¨‹
        ====================================================================

        å±‚1ï¼ˆè¾“å…¥ â†’ éšè—ï¼‰ï¼š
        Z1 = X @ W1 + b1           â† çº¿æ€§å˜æ¢
        A1 = activation(Z1)        â† éçº¿æ€§æ¿€æ´»

        å±‚2ï¼ˆéšè— â†’ è¾“å‡ºï¼‰ï¼š
        Z2 = A1 @ W2 + b2          â† çº¿æ€§å˜æ¢
        A2 = softmax(Z2)           â† Softmaxï¼ˆå¤šåˆ†ç±»ï¼‰

        ç¬¦å·è¯´æ˜ï¼š
        - Z: çº¿æ€§è¾“å‡ºï¼ˆæœªæ¿€æ´»ï¼‰
        - A: æ¿€æ´»åçš„è¾“å‡º
        - @: çŸ©é˜µä¹˜æ³•

        ====================================================================
        """
        # å±‚1ï¼šè¾“å…¥ â†’ éšè—
        self.Z1 = np.dot(X, self.W1) + self.b1  # (batch_size, n_hidden)
        self.A1 = self.activation(self.Z1)      # (batch_size, n_hidden)

        # å±‚2ï¼šéšè— â†’ è¾“å‡º
        self.Z2 = np.dot(self.A1, self.W2) + self.b2  # (batch_size, n_outputs)
        self.A2 = self._softmax(self.Z2)              # (batch_size, n_outputs)

        return self.A2

    def backward(self, X, y_true, y_pred):
        """
        åå‘ä¼ æ’­

        ====================================================================
        ğŸ”‘ åå‘ä¼ æ’­æ¨å¯¼ï¼ˆé“¾å¼æ³•åˆ™ï¼‰
        ====================================================================

        æŸå¤±å‡½æ•°ï¼šL = CrossEntropy(y_true, y_pred)

        ç›®æ ‡ï¼šè®¡ç®— âˆ‚L/âˆ‚W1, âˆ‚L/âˆ‚b1, âˆ‚L/âˆ‚W2, âˆ‚L/âˆ‚b2

        ã€å±‚2çš„æ¢¯åº¦ï¼ˆè¾“å‡ºå±‚ï¼‰ã€‘

        1. è¾“å‡ºå±‚è¯¯å·®
           dZ2 = âˆ‚L/âˆ‚Z2 = y_pred - y_true

           è¿™æ˜¯äº¤å‰ç†µ + Softmax çš„ç¥å¥‡ç®€åŒ–ï¼
           ï¼ˆæ¨å¯¼è¿‡ç¨‹ç±»ä¼¼é€»è¾‘å›å½’ï¼‰

        2. W2 çš„æ¢¯åº¦
           âˆ‚L/âˆ‚W2 = âˆ‚L/âˆ‚Z2 Â· âˆ‚Z2/âˆ‚W2
                  = A1^T @ dZ2

        3. b2 çš„æ¢¯åº¦
           âˆ‚L/âˆ‚b2 = sum(dZ2, axis=0)

        ã€å±‚1çš„æ¢¯åº¦ï¼ˆéšè—å±‚ï¼‰ã€‘

        4. éšè—å±‚è¯¯å·®ï¼ˆé€šè¿‡å±‚2åä¼ ï¼‰
           dA1 = dZ2 @ W2^T
           dZ1 = dA1 * activation'(Z1)

        5. W1 çš„æ¢¯åº¦
           âˆ‚L/âˆ‚W1 = X^T @ dZ1

        6. b1 çš„æ¢¯åº¦
           âˆ‚L/âˆ‚b1 = sum(dZ1, axis=0)

        æ ¸å¿ƒæ€æƒ³ï¼š
        - ä»è¾“å‡ºå±‚å¼€å§‹ï¼Œé€å±‚åå‘è®¡ç®—æ¢¯åº¦
        - ä½¿ç”¨é“¾å¼æ³•åˆ™è¿æ¥å„å±‚
        - è¯¯å·®é€šè¿‡æƒé‡çŸ©é˜µåå‘ä¼ æ’­

        ====================================================================
        """
        m = X.shape[0]

        # ========== å±‚2æ¢¯åº¦ï¼ˆè¾“å‡ºå±‚ï¼‰ ==========
        dZ2 = y_pred - y_true  # (batch_size, n_outputs)

        dW2 = (1 / m) * np.dot(self.A1.T, dZ2)  # (n_hidden, n_outputs)
        db2 = (1 / m) * np.sum(dZ2, axis=0, keepdims=True)  # (1, n_outputs)

        # ========== å±‚1æ¢¯åº¦ï¼ˆéšè—å±‚ï¼‰ ==========
        # è¯¯å·®åå‘ä¼ æ’­åˆ°éšè—å±‚
        dA1 = np.dot(dZ2, self.W2.T)  # (batch_size, n_hidden)

        # ä¹˜ä»¥æ¿€æ´»å‡½æ•°çš„å¯¼æ•°
        dZ1 = dA1 * self.activation_derivative(self.Z1)  # (batch_size, n_hidden)

        dW1 = (1 / m) * np.dot(X.T, dZ1)  # (n_features, n_hidden)
        db1 = (1 / m) * np.sum(dZ1, axis=0, keepdims=True)  # (1, n_hidden)

        return dW1, db1, dW2, db2

    def fit(self, X, y):
        """
        è®­ç»ƒæ¨¡å‹

        æµç¨‹ï¼š
        1. å‰å‘ä¼ æ’­ï¼šè®¡ç®—é¢„æµ‹å€¼
        2. è®¡ç®—æŸå¤±
        3. åå‘ä¼ æ’­ï¼šè®¡ç®—æ¢¯åº¦
        4. æ›´æ–°å‚æ•°
        """
        n_samples = X.shape[0]

        # è½¬æ¢ä¸º one-hot ç¼–ç 
        y_one_hot = self._to_one_hot(y)

        # Mini-batch æ¢¯åº¦ä¸‹é™
        for epoch in range(self.n_epochs):
            # æ‰“ä¹±æ•°æ®
            indices = np.random.permutation(n_samples)

            # åˆ†æ‰¹è®­ç»ƒ
            for start_idx in range(0, n_samples, self.batch_size):
                batch_indices = indices[start_idx:start_idx + self.batch_size]
                X_batch = X[batch_indices]
                y_batch = y_one_hot[batch_indices]

                # å‰å‘ä¼ æ’­
                y_pred = self.forward(X_batch)

                # åå‘ä¼ æ’­
                dW1, db1, dW2, db2 = self.backward(X_batch, y_batch, y_pred)

                # æ›´æ–°å‚æ•°
                self.W1 -= self.lr * dW1
                self.b1 -= self.lr * db1
                self.W2 -= self.lr * dW2
                self.b2 -= self.lr * db2

            # è®°å½•æŸå¤±
            if epoch % 10 == 0:
                y_pred_all = self.forward(X)
                loss = self._cross_entropy_loss(y_one_hot, y_pred_all)
                self.loss_history.append(loss)

    def predict_proba(self, X):
        """é¢„æµ‹æ¦‚ç‡"""
        return self.forward(X)

    def predict(self, X):
        """é¢„æµ‹ç±»åˆ«"""
        probs = self.predict_proba(X)
        return np.argmax(probs, axis=1)

    def score(self, X, y):
        """è®¡ç®—å‡†ç¡®ç‡"""
        y_pred = self.predict(X)
        return np.mean(y_pred == y)

    @staticmethod
    def _softmax(z):
        """Softmax æ¿€æ´»å‡½æ•°"""
        z_shifted = z - np.max(z, axis=-1, keepdims=True)
        exp_z = np.exp(z_shifted)
        return exp_z / np.sum(exp_z, axis=-1, keepdims=True)

    @staticmethod
    def _cross_entropy_loss(y_true, y_pred, epsilon=1e-15):
        """äº¤å‰ç†µæŸå¤±"""
        y_pred = np.clip(y_pred, epsilon, 1 - epsilon)
        return -np.mean(np.sum(y_true * np.log(y_pred), axis=1))

    def _to_one_hot(self, y):
        """è½¬æ¢ä¸º one-hot ç¼–ç """
        one_hot = np.zeros((y.shape[0], self.n_outputs))
        one_hot[np.arange(y.shape[0]), y] = 1
        return one_hot


# ==================== 3. æ•°æ®ç”Ÿæˆ ====================
def generate_nonlinear_data(dataset='moons', n_samples=300, noise=0.2, random_state=42):
    """
    ç”Ÿæˆéçº¿æ€§æ•°æ®

    dataset: 'moons', 'circles', 'linear'
    """
    if dataset == 'moons':
        X, y = make_moons(n_samples=n_samples, noise=noise, random_state=random_state)
    elif dataset == 'circles':
        X, y = make_circles(n_samples=n_samples, noise=noise, factor=0.5, random_state=random_state)
    elif dataset == 'linear':
        X, y = make_classification(n_samples=n_samples, n_features=2, n_informative=2,
                                   n_redundant=0, n_clusters_per_class=1,
                                   random_state=random_state)
    else:
        raise ValueError(f"Unknown dataset: {dataset}")

    return X, y


# ==================== 4. å¯è§†åŒ– ====================
def plot_decision_boundary(model, X, y, title="Decision Boundary"):
    """ç»˜åˆ¶å†³ç­–è¾¹ç•Œ"""
    x_min, x_max = X[:, 0].min() - 1, X[:, 0].max() + 1
    y_min, y_max = X[:, 1].min() - 1, X[:, 1].max() + 1
    xx, yy = np.meshgrid(np.linspace(x_min, x_max, 200),
                         np.linspace(y_min, y_max, 200))

    Z = model.predict(np.c_[xx.ravel(), yy.ravel()])
    Z = Z.reshape(xx.shape)

    plt.contourf(xx, yy, Z, alpha=0.3, cmap='RdBu', levels=1)
    plt.scatter(X[y == 0][:, 0], X[y == 0][:, 1], c='blue', marker='o',
                s=50, edgecolors='k', label='Class 0', alpha=0.7)
    plt.scatter(X[y == 1][:, 0], X[y == 1][:, 1], c='red', marker='s',
                s=50, edgecolors='k', label='Class 1', alpha=0.7)
    plt.xlabel('Feature 1', fontsize=12)
    plt.ylabel('Feature 2', fontsize=12)
    plt.title(title, fontsize=13, fontweight='bold')
    plt.legend(fontsize=10)
    plt.grid(True, alpha=0.3)


def compare_with_logistic_regression():
    """
    å¯¹æ¯”ç¥ç»ç½‘ç»œå’Œé€»è¾‘å›å½’

    åœ¨éçº¿æ€§æ•°æ®ä¸Šï¼Œç¥ç»ç½‘ç»œæ˜æ˜¾ä¼˜äºé€»è¾‘å›å½’
    """
    print("=" * 70)
    print("ğŸ”¬ å¯¹æ¯”ï¼šç¥ç»ç½‘ç»œ vs é€»è¾‘å›å½’ï¼ˆéçº¿æ€§æ•°æ®ï¼‰")
    print("=" * 70)

    # ç”Ÿæˆéçº¿æ€§æ•°æ®
    X, y = generate_nonlinear_data('moons', n_samples=300, noise=0.2)
    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)

    # æ ‡å‡†åŒ–
    scaler = StandardScaler()
    X_train = scaler.fit_transform(X_train)
    X_test = scaler.transform(X_test)

    # 1. é€»è¾‘å›å½’ï¼ˆçº¿æ€§æ¨¡å‹ï¼‰
    from sklearn.linear_model import LogisticRegression
    lr = LogisticRegression()
    lr.fit(X_train, y_train)
    lr_train_acc = lr.score(X_train, y_train)
    lr_test_acc = lr.score(X_test, y_test)

    # 2. ç¥ç»ç½‘ç»œ
    nn = NeuralNetwork(n_features=2, n_hidden=10, n_outputs=2,
                      activation='relu', learning_rate=0.1, n_epochs=500)
    nn.fit(X_train, y_train)
    nn_train_acc = nn.score(X_train, y_train)
    nn_test_acc = nn.score(X_test, y_test)

    print(f"\né€»è¾‘å›å½’ï¼ˆçº¿æ€§æ¨¡å‹ï¼‰ï¼š")
    print(f"  è®­ç»ƒå‡†ç¡®ç‡: {lr_train_acc:.4f} ({lr_train_acc*100:.2f}%)")
    print(f"  æµ‹è¯•å‡†ç¡®ç‡: {lr_test_acc:.4f} ({lr_test_acc*100:.2f}%)")

    print(f"\nç¥ç»ç½‘ç»œï¼ˆéçº¿æ€§æ¨¡å‹ï¼‰ï¼š")
    print(f"  è®­ç»ƒå‡†ç¡®ç‡: {nn_train_acc:.4f} ({nn_train_acc*100:.2f}%)")
    print(f"  æµ‹è¯•å‡†ç¡®ç‡: {nn_test_acc:.4f} ({nn_test_acc*100:.2f}%)")

    # å¯è§†åŒ–
    fig, axes = plt.subplots(1, 2, figsize=(14, 5))

    plt.subplot(1, 2, 1)
    plot_decision_boundary(lr, X_test, y_test,
                          title=f'Logistic Regression (Linear)\nTest Acc: {lr_test_acc:.2%}')

    plt.subplot(1, 2, 2)
    plot_decision_boundary(nn, X_test, y_test,
                          title=f'Neural Network (Non-linear)\nTest Acc: {nn_test_acc:.2%}')

    plt.tight_layout()
    plt.savefig('nn_vs_lr.png', dpi=100)
    print("\nğŸ“Š å¯¹æ¯”å›¾å·²ä¿å­˜åˆ°: nn_vs_lr.png")
    plt.show()

    print("\nğŸ’¡ è§‚å¯Ÿï¼š")
    print("  - é€»è¾‘å›å½’åªèƒ½å­¦ä¹ çº¿æ€§è¾¹ç•Œ â†’ åœ¨éçº¿æ€§æ•°æ®ä¸Šè¡¨ç°å·®")
    print("  - ç¥ç»ç½‘ç»œå¯ä»¥å­¦ä¹ å¤æ‚çš„éçº¿æ€§è¾¹ç•Œ â†’ å‡†ç¡®ç‡æ˜¾è‘—æå‡")


def compare_activations():
    """å¯¹æ¯”ä¸åŒæ¿€æ´»å‡½æ•°"""
    print("\n" + "=" * 70)
    print("ğŸ§ª å®éªŒï¼šä¸åŒæ¿€æ´»å‡½æ•°çš„å½±å“")
    print("=" * 70)

    X, y = generate_nonlinear_data('moons', n_samples=300, noise=0.2)
    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)

    scaler = StandardScaler()
    X_train = scaler.fit_transform(X_train)
    X_test = scaler.transform(X_test)

    activations = ['relu', 'sigmoid', 'tanh']
    results = {}

    fig, axes = plt.subplots(1, 3, figsize=(18, 5))

    for idx, activation in enumerate(activations):
        print(f"\nè®­ç»ƒ {activation.upper()} æ¿€æ´»...")

        model = NeuralNetwork(n_features=2, n_hidden=10, n_outputs=2,
                             activation=activation, learning_rate=0.1, n_epochs=500)
        model.fit(X_train, y_train)

        train_acc = model.score(X_train, y_train)
        test_acc = model.score(X_test, y_test)

        results[activation] = {'train': train_acc, 'test': test_acc}

        print(f"  æµ‹è¯•å‡†ç¡®ç‡: {test_acc:.4f}")

        # å¯è§†åŒ–
        plt.subplot(1, 3, idx + 1)
        plot_decision_boundary(model, X_test, y_test,
                              title=f'{activation.upper()} Activation\nTest Acc: {test_acc:.2%}')

    plt.tight_layout()
    plt.savefig('activation_comparison.png', dpi=100)
    print("\nğŸ“Š æ¿€æ´»å‡½æ•°å¯¹æ¯”å›¾å·²ä¿å­˜åˆ°: activation_comparison.png")
    plt.show()

    print("\nğŸ’¡ è§‚å¯Ÿï¼š")
    print("  - ReLU: æœ€å¸¸ç”¨ï¼Œè®­ç»ƒå¿«ï¼Œæ€§èƒ½å¥½")
    print("  - Sigmoid: å®¹æ˜“æ¢¯åº¦æ¶ˆå¤±ï¼Œæ€§èƒ½ç¨å·®")
    print("  - Tanh: æ¯” Sigmoid å¥½ï¼Œä½†ä¸å¦‚ ReLU")


def visualize_hidden_neurons():
    """å¯è§†åŒ–éšè—å±‚ç¥ç»å…ƒæ•°é‡çš„å½±å“"""
    print("\n" + "=" * 70)
    print("ğŸ§ª å®éªŒï¼šéšè—å±‚ç¥ç»å…ƒæ•°é‡çš„å½±å“")
    print("=" * 70)

    X, y = generate_nonlinear_data('circles', n_samples=300, noise=0.15)
    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)

    scaler = StandardScaler()
    X_train = scaler.fit_transform(X_train)
    X_test = scaler.transform(X_test)

    n_hidden_list = [2, 5, 10, 20]
    fig, axes = plt.subplots(2, 2, figsize=(12, 10))
    axes = axes.ravel()

    for idx, n_hidden in enumerate(n_hidden_list):
        print(f"\néšè—å±‚ç¥ç»å…ƒæ•°: {n_hidden}")

        model = NeuralNetwork(n_features=2, n_hidden=n_hidden, n_outputs=2,
                             activation='relu', learning_rate=0.1, n_epochs=500)
        model.fit(X_train, y_train)

        test_acc = model.score(X_test, y_test)
        print(f"  æµ‹è¯•å‡†ç¡®ç‡: {test_acc:.4f}")

        plt.subplot(2, 2, idx + 1)
        plot_decision_boundary(model, X_test, y_test,
                              title=f'Hidden Units: {n_hidden}\nTest Acc: {test_acc:.2%}')

    plt.tight_layout()
    plt.savefig('hidden_neurons.png', dpi=100)
    print("\nğŸ“Š éšè—ç¥ç»å…ƒå¯¹æ¯”å›¾å·²ä¿å­˜åˆ°: hidden_neurons.png")
    plt.show()

    print("\nğŸ’¡ è§‚å¯Ÿï¼š")
    print("  - ç¥ç»å…ƒå¤ªå°‘ â†’ å®¹é‡ä¸è¶³ï¼Œæ— æ³•å­¦ä¹ å¤æ‚æ¨¡å¼")
    print("  - ç¥ç»å…ƒé€‚ä¸­ â†’ æ°åˆ°å¥½å¤„")
    print("  - ç¥ç»å…ƒå¤ªå¤š â†’ å¯èƒ½è¿‡æ‹Ÿåˆï¼ˆéœ€è¦æ­£åˆ™åŒ–ï¼‰")


# ==================== 5. ä¸»ç¨‹åº ====================
def main():
    print("=" * 70)
    print("ç¥ç»ç½‘ç»œåŸºç¡€ (Neural Networks / MLP)")
    print("=" * 70)

    # 1. ç¥ç»ç½‘ç»œ vs é€»è¾‘å›å½’
    compare_with_logistic_regression()

    # 2. å¯¹æ¯”æ¿€æ´»å‡½æ•°
    compare_activations()

    # 3. éšè—å±‚å¤§å°çš„å½±å“
    visualize_hidden_neurons()

    # 4. æ€»ç»“
    print("\n" + "=" * 70)
    print("âœ… æ ¸å¿ƒè¦ç‚¹æ€»ç»“")
    print("=" * 70)
    print("""
1. ç¥ç»ç½‘ç»œ = å¤šå±‚ + éçº¿æ€§æ¿€æ´»
   - å•å±‚ï¼ˆé€»è¾‘å›å½’ï¼‰â†’ åªèƒ½å­¦ä¹ çº¿æ€§
   - å¤šå±‚ + æ¿€æ´» â†’ å¯ä»¥å­¦ä¹ éçº¿æ€§

2. ç½‘ç»œç»“æ„
   è¾“å…¥å±‚ â†’ [éšè—å±‚1 â†’ ... â†’ éšè—å±‚N] â†’ è¾“å‡ºå±‚
   - éšè—å±‚ï¼šæå–ç‰¹å¾ï¼Œå­¦ä¹ è¡¨ç¤º
   - è¾“å‡ºå±‚ï¼šæœ€ç»ˆå†³ç­–

3. å‰å‘ä¼ æ’­
   é€å±‚è®¡ç®—ï¼šçº¿æ€§å˜æ¢ + éçº¿æ€§æ¿€æ´»
   Z = WÂ·A_prev + b
   A = activation(Z)

4. åå‘ä¼ æ’­ï¼ˆæ ¸å¿ƒç®—æ³•ï¼‰
   ä»è¾“å‡ºå±‚å¼€å§‹ï¼Œé€å±‚åå‘è®¡ç®—æ¢¯åº¦
   - ä½¿ç”¨é“¾å¼æ³•åˆ™
   - è¯¯å·®é€šè¿‡æƒé‡çŸ©é˜µåä¼ 
   - dZ_l = dA_l * activation'(Z_l)

5. æ¿€æ´»å‡½æ•°é€‰æ‹©
   - ReLU: é»˜è®¤é€‰æ‹©ï¼ˆæ·±åº¦å­¦ä¹ æ ‡é…ï¼‰
   - Sigmoid: è¾“å‡ºå±‚ï¼ˆäºŒåˆ†ç±»ï¼‰
   - Tanh: æ¯” Sigmoid å¥½ï¼Œä½†ä¸å¦‚ ReLU
   - Softmax: è¾“å‡ºå±‚ï¼ˆå¤šåˆ†ç±»ï¼‰

6. å‚æ•°åˆå§‹åŒ–
   - ä¸èƒ½å…¨ 0ï¼ˆæ‰“ç ´å¯¹ç§°æ€§ï¼‰
   - He åˆå§‹åŒ–ï¼ˆReLUï¼‰
   - Xavier åˆå§‹åŒ–ï¼ˆTanh/Sigmoidï¼‰

7. éšè—å±‚å¤§å°
   - å¤ªå°ï¼šå®¹é‡ä¸è¶³
   - é€‚ä¸­ï¼šæ°åˆ°å¥½å¤„
   - å¤ªå¤§ï¼šå¯èƒ½è¿‡æ‹Ÿåˆ

8. é€šç”¨è¿‘ä¼¼å®šç†
   å•éšè—å±‚ç¥ç»ç½‘ç»œå¯ä»¥é€¼è¿‘ä»»æ„è¿ç»­å‡½æ•°
   ï¼ˆå‰æï¼šéšè—å±‚è¶³å¤Ÿå¤§ï¼‰

9. ä¸æ·±åº¦å­¦ä¹ çš„å…³ç³»
   - MLP æ˜¯æ·±åº¦å­¦ä¹ çš„åŸºç¡€
   - CNNã€RNNã€Transformer éƒ½åŸºäºç›¸åŒåŸç†
   - åªæ˜¯ç½‘ç»œç»“æ„æ›´å¤æ‚

10. å®è·µè¦ç‚¹
    âœ“ æ ‡å‡†åŒ–è¾“å…¥æ•°æ®
    âœ“ ä½¿ç”¨ ReLU æ¿€æ´»
    âœ“ He åˆå§‹åŒ–
    âœ“ Mini-batch æ¢¯åº¦ä¸‹é™
    âœ“ ç›‘æ§è®­ç»ƒ/æµ‹è¯•æ›²çº¿ï¼ˆé˜²æ­¢è¿‡æ‹Ÿåˆï¼‰
    """)


if __name__ == "__main__":
    main()

    print("\nğŸ’¡ ç»ƒä¹ å»ºè®®ï¼š")
    print("  1. å°è¯• 3 å±‚ç½‘ç»œï¼ˆ2ä¸ªéšè—å±‚ï¼‰ï¼Œè§‚å¯Ÿæ€§èƒ½")
    print("  2. åœ¨ä¸åŒæ•°æ®é›†ä¸Šæµ‹è¯•ï¼ˆmoonsã€circlesï¼‰")
    print("  3. æ‰‹åŠ¨æ¨å¯¼ä¸€éåå‘ä¼ æ’­å…¬å¼ï¼ˆéå¸¸é‡è¦ï¼ï¼‰")
    print("  4. æ€è€ƒï¼šä¸ºä»€ä¹ˆæ·±åº¦ç½‘ç»œæ¯”æµ…å±‚ç½‘ç»œæ›´å¼ºå¤§ï¼Ÿ")
