# tf.data - é«˜æ•ˆæ•°æ®ç®¡é“

> æ„å»ºé«˜æ€§èƒ½ã€å¯æ‰©å±•çš„æ•°æ®è¾“å…¥æµæ°´çº¿

## ğŸ¯ ä¸ºä»€ä¹ˆéœ€è¦ tf.dataï¼Ÿ

### ä¼ ç»Ÿæ–¹æ³•çš„é—®é¢˜

```python
# âŒ ä½æ•ˆçš„æ•°æ®åŠ è½½æ–¹å¼
import numpy as np

# ä¸€æ¬¡æ€§åŠ è½½æ‰€æœ‰æ•°æ®åˆ°å†…å­˜
x_train = np.load('train_images.npy')  # å¯èƒ½å‡ ä¸ªGBï¼
y_train = np.load('train_labels.npy')

model.fit(x_train, y_train, epochs=10)
```

**é—®é¢˜**ï¼š
1. å†…å­˜ä¸è¶³ï¼šå¤§æ•°æ®é›†æ— æ³•å…¨éƒ¨åŠ è½½
2. GPU ç©ºé—²ï¼šæ•°æ®åŠ è½½æ—¶ GPU åœ¨ç­‰å¾…
3. æ— æ³•æ‰©å±•ï¼šæ— æ³•å¤„ç†åˆ†å¸ƒå¼æ•°æ®

### tf.data çš„ä¼˜åŠ¿

```python
# âœ… é«˜æ•ˆçš„æ•°æ®ç®¡é“
import tensorflow as tf

dataset = tf.data.Dataset.from_tensor_slices((x_train, y_train))
dataset = dataset.batch(32).prefetch(tf.data.AUTOTUNE)

model.fit(dataset, epochs=10)
```

**ä¼˜åŠ¿**ï¼š
- âš¡ **æµå¼åŠ è½½**ï¼šæŒ‰éœ€åŠ è½½ï¼Œä¸å ç”¨å¤§é‡å†…å­˜
- âš¡ **é¢„å–ï¼ˆPrefetchï¼‰**ï¼šCPU å’Œ GPU å¹¶è¡Œå·¥ä½œ
- âš¡ **å¹¶è¡Œå¤„ç†**ï¼šå¤šçº¿ç¨‹åŠ é€Ÿæ•°æ®é¢„å¤„ç†
- ğŸ“ˆ **å¯æ‰©å±•**ï¼šæ”¯æŒåˆ†å¸ƒå¼è®­ç»ƒ

## 1. åˆ›å»º Dataset

### æ–¹å¼ 1ï¼šä»å†…å­˜æ•°æ®åˆ›å»º

```python
import tensorflow as tf
import numpy as np

# ä» NumPy æ•°ç»„åˆ›å»º
x = np.array([1, 2, 3, 4, 5])
y = np.array([10, 20, 30, 40, 50])

dataset = tf.data.Dataset.from_tensor_slices((x, y))

# éå†æ•°æ®
for x_item, y_item in dataset:
    print(f"x: {x_item.numpy()}, y: {y_item.numpy()}")

# è¾“å‡ºï¼š
# x: 1, y: 10
# x: 2, y: 20
# ...
```

### æ–¹å¼ 2ï¼šä»æ–‡ä»¶è·¯å¾„åˆ›å»º

```python
# å›¾åƒæ–‡ä»¶åˆ—è¡¨
image_paths = ['img1.jpg', 'img2.jpg', 'img3.jpg']
labels = [0, 1, 0]

dataset = tf.data.Dataset.from_tensor_slices((image_paths, labels))

def load_image(path, label):
    image = tf.io.read_file(path)
    image = tf.image.decode_jpeg(image, channels=3)
    image = tf.image.resize(image, [224, 224])
    image = image / 255.0  # å½’ä¸€åŒ–
    return image, label

dataset = dataset.map(load_image)
```

### æ–¹å¼ 3ï¼šä»ç”Ÿæˆå™¨åˆ›å»º

```python
def data_generator():
    for i in range(100):
        # æ¨¡æ‹Ÿä»æ•°æ®åº“/API è·å–æ•°æ®
        x = np.random.rand(28, 28, 1)
        y = np.random.randint(0, 10)
        yield x, y

dataset = tf.data.Dataset.from_generator(
    data_generator,
    output_signature=(
        tf.TensorSpec(shape=(28, 28, 1), dtype=tf.float32),
        tf.TensorSpec(shape=(), dtype=tf.int32)
    )
)
```

### æ–¹å¼ 4ï¼šä» TFRecord æ–‡ä»¶åˆ›å»º

```python
# TFRecord æ˜¯ TensorFlow çš„é«˜æ•ˆäºŒè¿›åˆ¶æ ¼å¼
filenames = ['data_part1.tfrecord', 'data_part2.tfrecord']
dataset = tf.data.TFRecordDataset(filenames)

def parse_tfrecord(example):
    feature_description = {
        'image': tf.io.FixedLenFeature([], tf.string),
        'label': tf.io.FixedLenFeature([], tf.int64),
    }
    parsed = tf.io.parse_single_example(example, feature_description)
    image = tf.io.decode_raw(parsed['image'], tf.uint8)
    image = tf.reshape(image, [28, 28, 1])
    return image, parsed['label']

dataset = dataset.map(parse_tfrecord)
```

## 2. æ•°æ®è½¬æ¢æ“ä½œ

### map() - æ•°æ®é¢„å¤„ç†

```python
dataset = tf.data.Dataset.from_tensor_slices([1, 2, 3, 4, 5])

# å¯¹æ¯ä¸ªå…ƒç´ åº”ç”¨å‡½æ•°
dataset = dataset.map(lambda x: x * 2)
# ç»“æœï¼š[2, 4, 6, 8, 10]

# å›¾åƒé¢„å¤„ç†ç¤ºä¾‹
def preprocess_image(image, label):
    # æ•°æ®å¢å¼º
    image = tf.image.random_flip_left_right(image)
    image = tf.image.random_brightness(image, 0.2)

    # å½’ä¸€åŒ–
    image = (image - 127.5) / 127.5  # [-1, 1]

    return image, label

dataset = dataset.map(preprocess_image)
```

### batch() - æ‰¹æ¬¡å¤„ç†

```python
dataset = tf.data.Dataset.from_tensor_slices([1, 2, 3, 4, 5, 6, 7])

# åˆ›å»ºæ‰¹æ¬¡
dataset = dataset.batch(3)

for batch in dataset:
    print(batch.numpy())

# è¾“å‡ºï¼š
# [1 2 3]
# [4 5 6]
# [7]  â† æœ€åä¸€ä¸ªæ‰¹æ¬¡å¯èƒ½ä¸å®Œæ•´

# ä¸¢å¼ƒä¸å®Œæ•´æ‰¹æ¬¡
dataset = dataset.batch(3, drop_remainder=True)
```

### shuffle() - æ‰“ä¹±æ•°æ®

```python
dataset = tf.data.Dataset.from_tensor_slices([1, 2, 3, 4, 5])

# buffer_sizeï¼šéšæœºç¼“å†²åŒºå¤§å°
dataset = dataset.shuffle(buffer_size=5, seed=42)

# buffer_size çš„å«ä¹‰ï¼š
# - å¤ªå°ï¼šæ‰“ä¹±ä¸å¤Ÿéšæœº
# - å¤ªå¤§ï¼šå ç”¨å†…å­˜å¤š
# - æ¨èï¼šæ•°æ®é›†å¤§å°ï¼ˆå°æ•°æ®é›†ï¼‰æˆ– 10000+ï¼ˆå¤§æ•°æ®é›†ï¼‰
```

### repeat() - é‡å¤æ•°æ®é›†

```python
dataset = tf.data.Dataset.from_tensor_slices([1, 2, 3])

# é‡å¤ 3 æ¬¡
dataset = dataset.repeat(3)  # [1,2,3,1,2,3,1,2,3]

# æ— é™é‡å¤ï¼ˆå¸¸ç”¨äºè®­ç»ƒï¼‰
dataset = dataset.repeat()
```

### filter() - è¿‡æ»¤æ•°æ®

```python
dataset = tf.data.Dataset.from_tensor_slices([1, 2, 3, 4, 5, 6])

# åªä¿ç•™å¶æ•°
dataset = dataset.filter(lambda x: x % 2 == 0)  # [2, 4, 6]
```

### take() / skip() - æˆªå–æ•°æ®

```python
dataset = tf.data.Dataset.from_tensor_slices(range(10))

train_dataset = dataset.skip(2).take(6)  # [2, 3, 4, 5, 6, 7]
test_dataset = dataset.take(2)            # [0, 1]
```

## 3. æ€§èƒ½ä¼˜åŒ–

### prefetch() - é¢„å–æ•°æ®ï¼ˆæœ€é‡è¦ï¼ï¼‰

```python
# âŒ æ²¡æœ‰é¢„å–ï¼šGPU ç­‰å¾… CPU åŠ è½½æ•°æ®
dataset = dataset.batch(32)

# âœ… ä½¿ç”¨é¢„å–ï¼šCPU æå‰å‡†å¤‡ä¸‹ä¸€æ‰¹æ•°æ®
dataset = dataset.batch(32).prefetch(tf.data.AUTOTUNE)
#                                     â†‘ è‡ªåŠ¨è°ƒä¼˜ç¼“å†²åŒºå¤§å°
```

**å·¥ä½œåŸç†**ï¼š

```
æ²¡æœ‰ prefetch:
CPU: [åŠ è½½batch1] ç©ºé—²      [åŠ è½½batch2] ç©ºé—²      ...
GPU: ç©ºé—²        [è®­ç»ƒbatch1] ç©ºé—²        [è®­ç»ƒbatch2] ...

ä½¿ç”¨ prefetch:
CPU: [åŠ è½½batch1] [åŠ è½½batch2] [åŠ è½½batch3] ...
GPU: ç©ºé—²        [è®­ç»ƒbatch1] [è®­ç»ƒbatch2] ...
     â†‘ GPUè®­ç»ƒæ—¶ï¼ŒCPUåŒæ—¶å‡†å¤‡ä¸‹ä¸€æ‰¹æ•°æ®
```

### cache() - ç¼“å­˜æ•°æ®

```python
# ç¬¬ä¸€æ¬¡è¿­ä»£åï¼Œæ•°æ®ç¼“å­˜åœ¨å†…å­˜ä¸­
dataset = dataset.cache()

# ç¼“å­˜åˆ°ç£ç›˜ï¼ˆæ•°æ®é‡å¤§æ—¶ï¼‰
dataset = dataset.cache('/tmp/my_cache')

# å…¸å‹ç”¨æ³•ï¼šç¼“å­˜ â†’ æ‰“ä¹± â†’ æ‰¹æ¬¡ â†’ é¢„å–
dataset = (dataset
    .cache()               # 1. ç¼“å­˜åŸå§‹æ•°æ®
    .shuffle(10000)        # 2. æ‰“ä¹±
    .batch(32)             # 3. æ‰¹æ¬¡
    .prefetch(tf.data.AUTOTUNE)  # 4. é¢„å–
)
```

### map() å¹¶è¡ŒåŒ–

```python
# ä½¿ç”¨å¤šçº¿ç¨‹å¹¶è¡Œå¤„ç†
dataset = dataset.map(
    preprocess_function,
    num_parallel_calls=tf.data.AUTOTUNE  # è‡ªåŠ¨è°ƒä¼˜çº¿ç¨‹æ•°
)
```

### interleave() - å¹¶è¡Œè¯»å–å¤šä¸ªæ–‡ä»¶

```python
# ä»å¤šä¸ªæ–‡ä»¶å¹¶è¡Œè¯»å–
files = tf.data.Dataset.list_files('data/*.tfrecord')

dataset = files.interleave(
    lambda x: tf.data.TFRecordDataset(x),
    cycle_length=4,        # åŒæ—¶è¯»å– 4 ä¸ªæ–‡ä»¶
    num_parallel_calls=tf.data.AUTOTUNE
)
```

## 4. å®Œæ•´çš„ä¼˜åŒ–æµç¨‹

### æ ‡å‡†æ¨¡æ¿ï¼ˆæ¨èï¼‰

```python
def create_dataset(file_pattern, batch_size, is_training=True):
    """åˆ›å»ºé«˜æ€§èƒ½æ•°æ®ç®¡é“"""

    # 1. åŠ è½½æ–‡ä»¶åˆ—è¡¨
    files = tf.data.Dataset.list_files(file_pattern, shuffle=is_training)

    # 2. å¹¶è¡Œè¯»å–æ–‡ä»¶
    dataset = files.interleave(
        tf.data.TFRecordDataset,
        cycle_length=tf.data.AUTOTUNE,
        num_parallel_calls=tf.data.AUTOTUNE
    )

    # 3. è§£ææ•°æ®
    dataset = dataset.map(
        parse_example,
        num_parallel_calls=tf.data.AUTOTUNE
    )

    # 4. ç¼“å­˜ï¼ˆå¦‚æœæ•°æ®é›†ä¸å¤§ï¼‰
    if is_training:
        dataset = dataset.cache()

    # 5. æ‰“ä¹±ï¼ˆä»…è®­ç»ƒæ—¶ï¼‰
    if is_training:
        dataset = dataset.shuffle(buffer_size=10000)

    # 6. æ‰¹æ¬¡å¤„ç†
    dataset = dataset.batch(batch_size)

    # 7. æ•°æ®å¢å¼ºï¼ˆä»…è®­ç»ƒæ—¶ï¼‰
    if is_training:
        dataset = dataset.map(
            augment_data,
            num_parallel_calls=tf.data.AUTOTUNE
        )

    # 8. é‡å¤ï¼ˆè®­ç»ƒæ—¶æ— é™å¾ªç¯ï¼‰
    if is_training:
        dataset = dataset.repeat()

    # 9. é¢„å–ï¼ˆæœ€é‡è¦ï¼ï¼‰
    dataset = dataset.prefetch(tf.data.AUTOTUNE)

    return dataset

# ä½¿ç”¨
train_dataset = create_dataset('train/*.tfrecord', batch_size=32, is_training=True)
val_dataset = create_dataset('val/*.tfrecord', batch_size=64, is_training=False)

model.fit(train_dataset, validation_data=val_dataset, epochs=10)
```

### æ“ä½œé¡ºåºå¾ˆé‡è¦ï¼

```python
# âœ… æ­£ç¡®é¡ºåºï¼šå…ˆç¼“å­˜å†æ‰“ä¹±
dataset = (dataset
    .map(parse_function)    # è§£æ
    .cache()                # ç¼“å­˜è§£æåçš„æ•°æ®
    .shuffle(10000)         # æ‰“ä¹±ï¼ˆæ¯ä¸ªepoché‡æ–°æ‰“ä¹±ï¼‰
    .batch(32)
    .prefetch(tf.data.AUTOTUNE)
)

# âŒ é”™è¯¯é¡ºåºï¼šå…ˆæ‰“ä¹±å†ç¼“å­˜
dataset = (dataset
    .map(parse_function)
    .shuffle(10000)         # æ‰“ä¹±
    .cache()                # ç¼“å­˜æ‰“ä¹±åçš„æ•°æ®ï¼ˆæ¯ä¸ªepoché¡ºåºç›¸åŒï¼ï¼‰
    .batch(32)
    .prefetch(tf.data.AUTOTUNE)
)
```

## 5. å®æˆ˜ç¤ºä¾‹

### ç¤ºä¾‹ 1ï¼šMNIST æ•°æ®ç®¡é“

```python
import tensorflow as tf

# åŠ è½½ MNIST
(x_train, y_train), (x_test, y_test) = tf.keras.datasets.mnist.load_data()

# åˆ›å»ºè®­ç»ƒé›†
train_dataset = tf.data.Dataset.from_tensor_slices((x_train, y_train))

train_dataset = (train_dataset
    .map(lambda x, y: (tf.cast(x, tf.float32) / 255.0, y))  # å½’ä¸€åŒ–
    .cache()                    # ç¼“å­˜
    .shuffle(10000)             # æ‰“ä¹±
    .batch(128)                 # æ‰¹æ¬¡
    .prefetch(tf.data.AUTOTUNE) # é¢„å–
)

# åˆ›å»ºæµ‹è¯•é›†
test_dataset = tf.data.Dataset.from_tensor_slices((x_test, y_test))

test_dataset = (test_dataset
    .map(lambda x, y: (tf.cast(x, tf.float32) / 255.0, y))
    .batch(128)
    .prefetch(tf.data.AUTOTUNE)
)

# è®­ç»ƒ
model.fit(train_dataset, validation_data=test_dataset, epochs=10)
```

### ç¤ºä¾‹ 2ï¼šå›¾åƒåˆ†ç±»æ•°æ®ç®¡é“

```python
import tensorflow as tf
import pathlib

# å›¾åƒç›®å½•ç»“æ„ï¼š
# data/
#   â”œâ”€â”€ train/
#   â”‚   â”œâ”€â”€ cat/
#   â”‚   â”‚   â”œâ”€â”€ img1.jpg
#   â”‚   â”‚   â””â”€â”€ img2.jpg
#   â”‚   â””â”€â”€ dog/
#   â”‚       â”œâ”€â”€ img1.jpg
#   â”‚       â””â”€â”€ img2.jpg

data_dir = pathlib.Path('data/train')

# ä½¿ç”¨ Keras å·¥å…·åˆ›å»ºæ•°æ®é›†
train_dataset = tf.keras.utils.image_dataset_from_directory(
    data_dir,
    validation_split=0.2,
    subset='training',
    seed=42,
    image_size=(224, 224),
    batch_size=32
)

# è‡ªå®šä¹‰é¢„å¤„ç†
def preprocess(image, label):
    image = tf.cast(image, tf.float32) / 255.0
    # æ•°æ®å¢å¼º
    image = tf.image.random_flip_left_right(image)
    image = tf.image.random_brightness(image, 0.2)
    return image, label

train_dataset = (train_dataset
    .map(preprocess, num_parallel_calls=tf.data.AUTOTUNE)
    .cache()
    .prefetch(tf.data.AUTOTUNE)
)
```

### ç¤ºä¾‹ 3ï¼šæ–‡æœ¬æ•°æ®ç®¡é“

```python
import tensorflow as tf

# æ–‡æœ¬æ•°æ®
texts = ['I love TensorFlow', 'Deep learning is awesome', ...]
labels = [1, 1, 0, ...]

# åˆ›å»º TextVectorization å±‚
vectorize_layer = tf.keras.layers.TextVectorization(
    max_tokens=10000,
    output_mode='int',
    output_sequence_length=50
)

# é€‚é…è¯æ±‡è¡¨
vectorize_layer.adapt(texts)

# åˆ›å»ºæ•°æ®é›†
dataset = tf.data.Dataset.from_tensor_slices((texts, labels))

dataset = (dataset
    .map(lambda text, label: (vectorize_layer(text), label))
    .cache()
    .shuffle(1000)
    .batch(32)
    .prefetch(tf.data.AUTOTUNE)
)
```

### ç¤ºä¾‹ 4ï¼šå¤„ç†å¤§å‹æ•°æ®é›†

```python
# å‡è®¾æœ‰ 1TB å›¾åƒæ•°æ®ï¼Œæ— æ³•å…¨éƒ¨åŠ è½½åˆ°å†…å­˜

def process_large_dataset(file_pattern):
    # 1. åˆ›å»ºæ–‡ä»¶åˆ—è¡¨
    files = tf.io.gfile.glob(file_pattern)
    dataset = tf.data.Dataset.from_tensor_slices(files)

    # 2. å¹¶è¡Œè¯»å–æ–‡ä»¶
    def load_and_preprocess(path):
        image = tf.io.read_file(path)
        image = tf.image.decode_jpeg(image, channels=3)
        image = tf.image.resize(image, [224, 224])
        image = image / 255.0

        # ä»æ–‡ä»¶åæå–æ ‡ç­¾
        label = tf.strings.split(path, '/')[-2]
        label = tf.cast(label == 'cat', tf.int32)

        return image, label

    dataset = dataset.map(
        load_and_preprocess,
        num_parallel_calls=tf.data.AUTOTUNE
    )

    # 3. ä¼˜åŒ–æµæ°´çº¿
    dataset = (dataset
        .shuffle(10000)             # å¤§ç¼“å†²åŒº
        .batch(32)
        .prefetch(tf.data.AUTOTUNE)
    )

    return dataset

# ä½¿ç”¨
train_dataset = process_large_dataset('data/train/*/*.jpg')
model.fit(train_dataset, epochs=10, steps_per_epoch=1000)
```

## 6. è°ƒè¯•ä¸æ£€æŸ¥

### æŸ¥çœ‹æ•°æ®é›†å†…å®¹

```python
# å–å‡ºå‰å‡ ä¸ªæ ·æœ¬æŸ¥çœ‹
for image, label in train_dataset.take(3):
    print(f"Image shape: {image.shape}, Label: {label.numpy()}")

# å¯è§†åŒ–æ‰¹æ¬¡
import matplotlib.pyplot as plt

for images, labels in train_dataset.take(1):
    plt.figure(figsize=(10, 10))
    for i in range(9):
        plt.subplot(3, 3, i+1)
        plt.imshow(images[i])
        plt.title(f"Label: {labels[i]}")
        plt.axis('off')
    plt.show()
```

### æ€§èƒ½åˆ†æ

```python
import time

# æµ‹è¯•æ•°æ®ç®¡é“æ€§èƒ½
dataset = create_dataset(...)

# é¢„çƒ­
for _ in dataset.take(10):
    pass

# æµ‹è¯•
start = time.time()
for i, batch in enumerate(dataset.take(100)):
    if i % 10 == 0:
        print(f"Batch {i}: {time.time() - start:.2f}s")

# ä½¿ç”¨ TensorFlow Profiler
tf.profiler.experimental.start('logs')
model.fit(dataset, epochs=1)
tf.profiler.experimental.stop()
```

### å¸¸è§é—®é¢˜æ’æŸ¥

```python
# é—®é¢˜1ï¼šå†…å­˜ä¸è¶³
# è§£å†³ï¼šå‡å° batch_size æˆ– buffer_size
dataset = dataset.batch(16)  # å‡å°æ‰¹æ¬¡
dataset = dataset.shuffle(1000)  # å‡å°ç¼“å†²åŒº

# é—®é¢˜2ï¼šæ•°æ®åŠ è½½å¤ªæ…¢
# è§£å†³ï¼šå¢åŠ å¹¶è¡Œåº¦
dataset = dataset.map(fn, num_parallel_calls=tf.data.AUTOTUNE)
dataset = dataset.prefetch(tf.data.AUTOTUNE)

# é—®é¢˜3ï¼šè®­ç»ƒé€Ÿåº¦æ…¢
# è§£å†³ï¼šä½¿ç”¨ cache() ç¼“å­˜æ•°æ®
dataset = dataset.cache()

# é—®é¢˜4ï¼šæ•°æ®ä¸éšæœº
# è§£å†³ï¼šå¢å¤§ shuffle buffer_size
dataset = dataset.shuffle(10000)  # è‡³å°‘æ˜¯ batch_size çš„å‡ å€
```

## 7. é«˜çº§æŠ€å·§

### è‡ªå®šä¹‰æ•°æ®å¢å¼º

```python
@tf.function  # ç¼–è¯‘ä¸ºé™æ€å›¾åŠ é€Ÿ
def augment(image, label):
    # éšæœºè£å‰ª
    image = tf.image.random_crop(image, size=[224, 224, 3])

    # éšæœºç¿»è½¬
    image = tf.image.random_flip_left_right(image)
    image = tf.image.random_flip_up_down(image)

    # éšæœºäº®åº¦/å¯¹æ¯”åº¦
    image = tf.image.random_brightness(image, 0.2)
    image = tf.image.random_contrast(image, 0.8, 1.2)

    # éšæœºè‰²ç›¸/é¥±å’Œåº¦
    image = tf.image.random_hue(image, 0.1)
    image = tf.image.random_saturation(image, 0.8, 1.2)

    # å½’ä¸€åŒ–
    image = tf.clip_by_value(image, 0.0, 1.0)

    return image, label

dataset = dataset.map(augment, num_parallel_calls=tf.data.AUTOTUNE)
```

### æ··åˆç²¾åº¦è®­ç»ƒ

```python
# å¯ç”¨æ··åˆç²¾åº¦
from tensorflow.keras import mixed_precision
mixed_precision.set_global_policy('mixed_float16')

# æ•°æ®ç±»å‹è½¬æ¢
def cast_to_fp16(image, label):
    image = tf.cast(image, tf.float16)
    return image, label

dataset = dataset.map(cast_to_fp16)
```

### åˆ†å¸ƒå¼è®­ç»ƒ

```python
# å¤š GPU è®­ç»ƒ
strategy = tf.distribute.MirroredStrategy()

with strategy.scope():
    # åˆ›å»ºæ•°æ®é›†ï¼ˆè‡ªåŠ¨åˆ†ç‰‡ï¼‰
    train_dataset = strategy.experimental_distribute_dataset(train_dataset)

    # åˆ›å»ºæ¨¡å‹
    model = create_model()
    model.compile(...)

# è®­ç»ƒ
model.fit(train_dataset, epochs=10)
```

## ğŸ”— ä¸‹ä¸€æ­¥

- [04 - æ¨¡å‹æ„å»ºä¸è®­ç»ƒ](./04-model-building.md) - è‡ªå®šä¹‰è®­ç»ƒå¾ªç¯ã€æ··åˆç²¾åº¦è®­ç»ƒ
- [å®è·µé¡¹ç›®](./practices/) - åŠ¨æ‰‹å®ç°å®Œæ•´é¡¹ç›®

## ğŸ“š å‚è€ƒèµ„æº

- [tf.data å®˜æ–¹æŒ‡å—](https://www.tensorflow.org/guide/data)
- [tf.data æ€§èƒ½ä¼˜åŒ–](https://www.tensorflow.org/guide/data_performance)
- [æ•°æ®å¢å¼ºæœ€ä½³å®è·µ](https://www.tensorflow.org/tutorials/images/data_augmentation)
