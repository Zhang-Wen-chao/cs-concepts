# Keras API - TensorFlow çš„é«˜çº§æ¥å£

> Keras è®©æ·±åº¦å­¦ä¹ æ¨¡å‹æ„å»ºåƒæ­ç§¯æœ¨ä¸€æ ·ç®€å•

## ğŸ¯ Keras æ˜¯ä»€ä¹ˆï¼Ÿ

Keras æ˜¯ TensorFlow çš„**é«˜çº§ API**ï¼Œæä¾›äº†ç®€æ´ã€ç”¨æˆ·å‹å¥½çš„æ¥å£æ¥æ„å»ºå’Œè®­ç»ƒæ·±åº¦å­¦ä¹ æ¨¡å‹ã€‚

**æ ¸å¿ƒç†å¿µ**ï¼š
- ä¸ºäººç±»è®¾è®¡ï¼Œä¸æ˜¯ä¸ºæœºå™¨
- æ¨¡å—åŒ–ï¼šå±‚ã€æ¨¡å‹ã€ä¼˜åŒ–å™¨ã€æŸå¤±å‡½æ•°éƒ½æ˜¯ç‹¬ç«‹æ¨¡å—
- æ˜“äºæ‰©å±•ï¼šå¯ä»¥è‡ªå®šä¹‰ä»»ä½•ç»„ä»¶

## 1. ä¸‰ç§æ„å»ºæ¨¡å‹çš„æ–¹å¼

### æ–¹å¼ 1ï¼šSequential APIï¼ˆé¡ºåºæ¨¡å‹ï¼‰

**é€‚ç”¨åœºæ™¯**ï¼šçº¿æ€§å †å çš„ç®€å•æ¨¡å‹ï¼ˆæœ€å¸¸ç”¨ï¼‰

```python
import tensorflow as tf
from tensorflow import keras

# æ„å»ºæ¨¡å‹
model = keras.Sequential([
    keras.layers.Dense(128, activation='relu', input_shape=(784,)),
    keras.layers.Dropout(0.2),
    keras.layers.Dense(64, activation='relu'),
    keras.layers.Dense(10, activation='softmax')
])

# æˆ–è€…é€å±‚æ·»åŠ 
model = keras.Sequential()
model.add(keras.layers.Dense(128, activation='relu', input_shape=(784,)))
model.add(keras.layers.Dropout(0.2))
model.add(keras.layers.Dense(10, activation='softmax'))
```

**ä¼˜ç‚¹**ï¼š
- âœ… ä»£ç ç®€æ´ï¼Œæ˜“è¯»
- âœ… é€‚åˆ 90% çš„åœºæ™¯

**å±€é™**ï¼š
- âŒ åªèƒ½å¤„ç†å•è¾“å…¥ã€å•è¾“å‡º
- âŒ ä¸æ”¯æŒåˆ†æ”¯æˆ–è·³è·ƒè¿æ¥ï¼ˆå¦‚ ResNetï¼‰

### æ–¹å¼ 2ï¼šFunctional APIï¼ˆå‡½æ•°å¼æ¨¡å‹ï¼‰

**é€‚ç”¨åœºæ™¯**ï¼šå¤æ‚æ¶æ„ï¼ˆå¤šè¾“å…¥ã€å¤šè¾“å‡ºã€åˆ†æ”¯ç»“æ„ï¼‰

```python
from tensorflow import keras

# å®šä¹‰è¾“å…¥
inputs = keras.Input(shape=(784,))

# å®šä¹‰å±‚ä¹‹é—´çš„è¿æ¥
x = keras.layers.Dense(128, activation='relu')(inputs)
x = keras.layers.Dropout(0.2)(x)
x = keras.layers.Dense(64, activation='relu')(x)
outputs = keras.layers.Dense(10, activation='softmax')(x)

# åˆ›å»ºæ¨¡å‹
model = keras.Model(inputs=inputs, outputs=outputs)
```

**å¤šè¾“å…¥å¤šè¾“å‡ºç¤ºä¾‹**ï¼š

```python
# ä¾‹å­ï¼šæ–‡æœ¬åˆ†ç±» + æƒ…æ„Ÿåˆ†æï¼ˆå…±äº«ç‰¹å¾æå–ï¼‰
text_input = keras.Input(shape=(100,), name='text')

# å…±äº«çš„ç‰¹å¾æå–å±‚
x = keras.layers.Embedding(10000, 128)(text_input)
x = keras.layers.LSTM(64)(x)

# åˆ†æ”¯1ï¼šæ–‡æœ¬åˆ†ç±»ï¼ˆ5ä¸ªç±»åˆ«ï¼‰
classification_output = keras.layers.Dense(5, activation='softmax', name='classification')(x)

# åˆ†æ”¯2ï¼šæƒ…æ„Ÿåˆ†æï¼ˆæ­£é¢/è´Ÿé¢ï¼‰
sentiment_output = keras.layers.Dense(1, activation='sigmoid', name='sentiment')(x)

# åˆ›å»ºå¤šè¾“å‡ºæ¨¡å‹
model = keras.Model(
    inputs=text_input,
    outputs=[classification_output, sentiment_output]
)
```

**æ®‹å·®è¿æ¥ï¼ˆResNeté£æ ¼ï¼‰**ï¼š

```python
inputs = keras.Input(shape=(32, 32, 3))

# ä¸»è·¯å¾„
x = keras.layers.Conv2D(64, 3, padding='same')(inputs)
x = keras.layers.BatchNormalization()(x)
x = keras.layers.Activation('relu')(x)

x = keras.layers.Conv2D(64, 3, padding='same')(x)
x = keras.layers.BatchNormalization()(x)

# è·³è·ƒè¿æ¥ï¼ˆshortcutï¼‰
shortcut = keras.layers.Conv2D(64, 1)(inputs)  # è°ƒæ•´é€šé“æ•°
x = keras.layers.Add()([x, shortcut])  # æ®‹å·®ç›¸åŠ 
x = keras.layers.Activation('relu')(x)

# è¾“å‡º
outputs = keras.layers.GlobalAveragePooling2D()(x)
outputs = keras.layers.Dense(10, activation='softmax')(outputs)

model = keras.Model(inputs, outputs)
```

### æ–¹å¼ 3ï¼šModel Subclassingï¼ˆå­ç±»åŒ–æ¨¡å‹ï¼‰

**é€‚ç”¨åœºæ™¯**ï¼šéœ€è¦å®Œå…¨è‡ªå®šä¹‰çš„å¤æ‚é€»è¾‘ï¼ˆç ”ç©¶ã€ç‰¹æ®Šéœ€æ±‚ï¼‰

```python
class MyModel(keras.Model):
    def __init__(self):
        super().__init__()
        self.dense1 = keras.layers.Dense(128, activation='relu')
        self.dropout = keras.layers.Dropout(0.2)
        self.dense2 = keras.layers.Dense(10, activation='softmax')

    def call(self, inputs, training=False):
        x = self.dense1(inputs)
        if training:  # åªåœ¨è®­ç»ƒæ—¶åº”ç”¨ Dropout
            x = self.dropout(x)
        return self.dense2(x)

# åˆ›å»ºæ¨¡å‹å®ä¾‹
model = MyModel()
```

**é«˜çº§ç¤ºä¾‹ï¼šè‡ªå®šä¹‰å‰å‘ä¼ æ’­**ï¼š

```python
class AttentionModel(keras.Model):
    def __init__(self, vocab_size, embed_dim, num_heads):
        super().__init__()
        self.embedding = keras.layers.Embedding(vocab_size, embed_dim)
        self.attention = keras.layers.MultiHeadAttention(num_heads, embed_dim)
        self.dense = keras.layers.Dense(1, activation='sigmoid')

    def call(self, inputs):
        # è‡ªå®šä¹‰å‰å‘ä¼ æ’­é€»è¾‘
        x = self.embedding(inputs)

        # Self-attention
        attention_output = self.attention(query=x, key=x, value=x)

        # å…¨å±€å¹³å‡æ± åŒ–
        x = tf.reduce_mean(attention_output, axis=1)

        return self.dense(x)

model = AttentionModel(vocab_size=10000, embed_dim=128, num_heads=4)
```

### ä¸‰ç§æ–¹å¼å¯¹æ¯”

| ç‰¹æ€§ | Sequential | Functional | Subclassing |
|------|-----------|-----------|-------------|
| æ˜“ç”¨æ€§ | â­â­â­â­â­ | â­â­â­â­ | â­â­â­ |
| çµæ´»æ€§ | â­â­ | â­â­â­â­ | â­â­â­â­â­ |
| è°ƒè¯•éš¾åº¦ | ç®€å• | ä¸­ç­‰ | å›°éš¾ |
| æ¨¡å‹å¯è§†åŒ– | âœ… | âœ… | âŒ |
| æ¨¡å‹ä¿å­˜ | âœ… | âœ… | éƒ¨åˆ†æ”¯æŒ |
| é€‚ç”¨åœºæ™¯ | ç®€å•çº¿æ€§æ¨¡å‹ | å¤æ‚æ¶æ„ | ç ”ç©¶/ç‰¹æ®Šéœ€æ±‚ |

**æ¨èé€‰æ‹©**ï¼š
- åˆå­¦è€…ï¼šSequential
- å·¥ç¨‹å¸ˆï¼šFunctionalï¼ˆ90%åœºæ™¯å¤Ÿç”¨ï¼‰
- ç ”ç©¶è€…ï¼šSubclassing

## 2. å¸¸ç”¨å±‚ï¼ˆLayersï¼‰

### æ ¸å¿ƒå±‚

```python
from tensorflow.keras import layers

# 1. å…¨è¿æ¥å±‚ï¼ˆDenseï¼‰
layers.Dense(units=128, activation='relu')

# 2. å·ç§¯å±‚ï¼ˆç”¨äºå›¾åƒï¼‰
layers.Conv2D(filters=64, kernel_size=3, strides=1, padding='same')

# 3. å¾ªç¯å±‚ï¼ˆç”¨äºåºåˆ—ï¼‰
layers.LSTM(units=64, return_sequences=True)  # è¿”å›å®Œæ•´åºåˆ—
layers.GRU(units=64)                          # è½»é‡çº§ RNN

# 4. åµŒå…¥å±‚ï¼ˆç”¨äºæ–‡æœ¬/ç±»åˆ«ç‰¹å¾ï¼‰
layers.Embedding(input_dim=10000, output_dim=128)

# 5. å½’ä¸€åŒ–å±‚
layers.BatchNormalization()    # æ‰¹å½’ä¸€åŒ–
layers.LayerNormalization()    # å±‚å½’ä¸€åŒ–ï¼ˆTransformerå¸¸ç”¨ï¼‰

# 6. Dropoutï¼ˆé˜²æ­¢è¿‡æ‹Ÿåˆï¼‰
layers.Dropout(rate=0.5)

# 7. æ± åŒ–å±‚
layers.MaxPooling2D(pool_size=2)
layers.GlobalAveragePooling2D()  # å…¨å±€å¹³å‡æ± åŒ–
```

### æ¿€æ´»å‡½æ•°

```python
# ä½œä¸ºå±‚
layers.ReLU()
layers.LeakyReLU(alpha=0.2)
layers.Softmax()

# ä½œä¸ºå‚æ•°
layers.Dense(64, activation='relu')
layers.Dense(10, activation='softmax')

# å¸¸ç”¨æ¿€æ´»å‡½æ•°å¯¹æ¯”
# - ReLU: æœ€å¸¸ç”¨ï¼Œé€Ÿåº¦å¿«ï¼Œä½†å¯èƒ½"æ­»äº¡"
# - LeakyReLU: è§£å†³ ReLU æ­»äº¡é—®é¢˜
# - Sigmoid: è¾“å‡º [0, 1]ï¼Œç”¨äºäºŒåˆ†ç±»
# - Tanh: è¾“å‡º [-1, 1]ï¼Œæ¯” Sigmoid æ•ˆæœå¥½
# - Softmax: å¤šåˆ†ç±»è¾“å‡ºå±‚
```

### æ­£åˆ™åŒ–å±‚

```python
# L1/L2 æ­£åˆ™åŒ–
layers.Dense(64, kernel_regularizer=keras.regularizers.l2(0.01))

# Dropoutï¼ˆéšæœºä¸¢å¼ƒç¥ç»å…ƒï¼‰
layers.Dropout(0.5)

# Spatial Dropoutï¼ˆä¸¢å¼ƒæ•´ä¸ªç‰¹å¾å›¾é€šé“ï¼‰
layers.SpatialDropout2D(0.2)

# Batch Normalizationï¼ˆæ‰¹å½’ä¸€åŒ–ï¼‰
layers.BatchNormalization()
```

## 3. ç¼–è¯‘æ¨¡å‹ï¼ˆCompileï¼‰

æ¨¡å‹å®šä¹‰åï¼Œéœ€è¦é…ç½®è®­ç»ƒå‚æ•°ï¼š

```python
model.compile(
    optimizer='adam',                      # ä¼˜åŒ–å™¨
    loss='sparse_categorical_crossentropy', # æŸå¤±å‡½æ•°
    metrics=['accuracy']                    # è¯„ä¼°æŒ‡æ ‡
)
```

### ä¼˜åŒ–å™¨ï¼ˆOptimizerï¼‰

```python
# 1. SGDï¼ˆéšæœºæ¢¯åº¦ä¸‹é™ï¼‰
optimizer = keras.optimizers.SGD(learning_rate=0.01, momentum=0.9)

# 2. Adamï¼ˆè‡ªé€‚åº”å­¦ä¹ ç‡ï¼Œæœ€å¸¸ç”¨ï¼‰
optimizer = keras.optimizers.Adam(learning_rate=0.001)

# 3. RMSpropï¼ˆé€‚åˆ RNNï¼‰
optimizer = keras.optimizers.RMSprop(learning_rate=0.001)

# 4. AdamWï¼ˆAdam + æƒé‡è¡°å‡ï¼Œç°ä»£é¦–é€‰ï¼‰
optimizer = keras.optimizers.AdamW(learning_rate=0.001, weight_decay=0.01)
```

### æŸå¤±å‡½æ•°ï¼ˆLossï¼‰

```python
# å›å½’ä»»åŠ¡
loss = 'mean_squared_error'        # MSE
loss = 'mean_absolute_error'       # MAE

# äºŒåˆ†ç±»ä»»åŠ¡
loss = 'binary_crossentropy'       # æ ‡ç­¾: [0, 1]

# å¤šåˆ†ç±»ä»»åŠ¡
loss = 'categorical_crossentropy'  # æ ‡ç­¾: one-hot [[1,0,0], [0,1,0]]
loss = 'sparse_categorical_crossentropy'  # æ ‡ç­¾: æ•´æ•° [0, 1, 2]

# è‡ªå®šä¹‰æŸå¤±
def custom_loss(y_true, y_pred):
    return tf.reduce_mean(tf.square(y_true - y_pred))

model.compile(optimizer='adam', loss=custom_loss)
```

### è¯„ä¼°æŒ‡æ ‡ï¼ˆMetricsï¼‰

```python
# åˆ†ç±»ä»»åŠ¡
metrics = ['accuracy']                          # å‡†ç¡®ç‡
metrics = [keras.metrics.Precision()]           # ç²¾ç¡®ç‡
metrics = [keras.metrics.Recall()]              # å¬å›ç‡
metrics = [keras.metrics.AUC()]                 # AUC

# å›å½’ä»»åŠ¡
metrics = [keras.metrics.MeanSquaredError()]    # MSE
metrics = [keras.metrics.MeanAbsoluteError()]   # MAE

# å¤šä¸ªæŒ‡æ ‡
model.compile(
    optimizer='adam',
    loss='sparse_categorical_crossentropy',
    metrics=['accuracy', keras.metrics.Precision(), keras.metrics.Recall()]
)
```

## 4. è®­ç»ƒæ¨¡å‹ï¼ˆFitï¼‰

```python
# åŸºæœ¬è®­ç»ƒ
history = model.fit(
    x_train, y_train,          # è®­ç»ƒæ•°æ®
    batch_size=32,             # æ‰¹æ¬¡å¤§å°
    epochs=10,                 # è®­ç»ƒè½®æ•°
    validation_data=(x_val, y_val),  # éªŒè¯æ•°æ®
    verbose=1                  # æ˜¾ç¤ºè¿›åº¦æ¡
)

# ä½¿ç”¨ç”Ÿæˆå™¨ï¼ˆå¤§æ•°æ®é›†ï¼‰
history = model.fit(
    train_dataset,             # tf.data.Dataset å¯¹è±¡
    epochs=10,
    validation_data=val_dataset
)
```

### å›è°ƒå‡½æ•°ï¼ˆCallbacksï¼‰

å›è°ƒå‡½æ•°åœ¨è®­ç»ƒè¿‡ç¨‹ä¸­æ‰§è¡Œç‰¹å®šæ“ä½œï¼š

```python
from tensorflow.keras.callbacks import *

callbacks = [
    # 1. æ—©åœï¼ˆé˜²æ­¢è¿‡æ‹Ÿåˆï¼‰
    EarlyStopping(
        monitor='val_loss',        # ç›‘æ§éªŒè¯æŸå¤±
        patience=5,                # 5è½®ä¸æ”¹å–„å°±åœæ­¢
        restore_best_weights=True  # æ¢å¤æœ€ä½³æƒé‡
    ),

    # 2. ä¿å­˜æœ€ä½³æ¨¡å‹
    ModelCheckpoint(
        filepath='best_model.h5',
        monitor='val_accuracy',
        save_best_only=True
    ),

    # 3. å­¦ä¹ ç‡è¡°å‡
    ReduceLROnPlateau(
        monitor='val_loss',
        factor=0.5,               # å­¦ä¹ ç‡å‡åŠ
        patience=3,
        min_lr=1e-7
    ),

    # 4. TensorBoard å¯è§†åŒ–
    TensorBoard(log_dir='./logs'),

    # 5. è‡ªå®šä¹‰å›è°ƒ
    LambdaCallback(
        on_epoch_end=lambda epoch, logs: print(f"Epoch {epoch}: loss={logs['loss']:.4f}")
    )
]

history = model.fit(
    x_train, y_train,
    epochs=50,
    validation_data=(x_val, y_val),
    callbacks=callbacks
)
```

### è®­ç»ƒå†å²ï¼ˆHistoryï¼‰

```python
import matplotlib.pyplot as plt

# è®­ç»ƒå®Œæˆåï¼Œhistory å¯¹è±¡åŒ…å«è®­ç»ƒæŒ‡æ ‡
print(history.history.keys())  # ['loss', 'accuracy', 'val_loss', 'val_accuracy']

# å¯è§†åŒ–è®­ç»ƒè¿‡ç¨‹
plt.plot(history.history['loss'], label='Training Loss')
plt.plot(history.history['val_loss'], label='Validation Loss')
plt.xlabel('Epoch')
plt.ylabel('Loss')
plt.legend()
plt.show()
```

## 5. è¯„ä¼°ä¸é¢„æµ‹

### è¯„ä¼°æ¨¡å‹

```python
# åœ¨æµ‹è¯•é›†ä¸Šè¯„ä¼°
test_loss, test_accuracy = model.evaluate(x_test, y_test, verbose=0)
print(f"Test Accuracy: {test_accuracy:.4f}")

# è‡ªå®šä¹‰è¯„ä¼°
results = model.evaluate(x_test, y_test, return_dict=True)
print(results)  # {'loss': 0.123, 'accuracy': 0.95}
```

### é¢„æµ‹

```python
# æ‰¹é‡é¢„æµ‹
predictions = model.predict(x_test)
print(predictions.shape)  # (10000, 10) - 10000ä¸ªæ ·æœ¬ï¼Œ10ä¸ªç±»åˆ«çš„æ¦‚ç‡

# å•æ ·æœ¬é¢„æµ‹
single_sample = x_test[0:1]  # ä¿æŒç»´åº¦ (1, 784)
prediction = model.predict(single_sample, verbose=0)
predicted_class = np.argmax(prediction)

# è·å–ç±»åˆ«ï¼ˆä¸æ˜¯æ¦‚ç‡ï¼‰
predicted_classes = np.argmax(predictions, axis=1)
```

## 6. æ¨¡å‹ä¿å­˜ä¸åŠ è½½

### æ–¹å¼ 1ï¼šSavedModel æ ¼å¼ï¼ˆæ¨èï¼‰

```python
# ä¿å­˜æ•´ä¸ªæ¨¡å‹ï¼ˆæ¶æ„ + æƒé‡ + ä¼˜åŒ–å™¨çŠ¶æ€ï¼‰
model.save('my_model')  # åˆ›å»ºç›®å½•

# åŠ è½½æ¨¡å‹
loaded_model = keras.models.load_model('my_model')

# å¯ä»¥ç›´æ¥ä½¿ç”¨
predictions = loaded_model.predict(x_test)
```

### æ–¹å¼ 2ï¼šHDF5 æ ¼å¼

```python
# ä¿å­˜ä¸º .h5 æ–‡ä»¶
model.save('my_model.h5')

# åŠ è½½
loaded_model = keras.models.load_model('my_model.h5')
```

### æ–¹å¼ 3ï¼šåªä¿å­˜æƒé‡

```python
# ä¿å­˜æƒé‡
model.save_weights('my_weights.h5')

# åŠ è½½æƒé‡ï¼ˆéœ€è¦å…ˆå®šä¹‰ç›¸åŒæ¶æ„ï¼‰
model = create_model()  # ä½ çš„æ¨¡å‹å®šä¹‰å‡½æ•°
model.load_weights('my_weights.h5')
```

### æ–¹å¼ 4ï¼šä¿å­˜ä¸º TFLiteï¼ˆç§»åŠ¨ç«¯éƒ¨ç½²ï¼‰

```python
# è½¬æ¢ä¸º TFLite
converter = tf.lite.TFLiteConverter.from_keras_model(model)
tflite_model = converter.convert()

# ä¿å­˜
with open('model.tflite', 'wb') as f:
    f.write(tflite_model)
```

## 7. å®Œæ•´ç¤ºä¾‹ï¼šMNIST æ‰‹å†™æ•°å­—è¯†åˆ«

```python
import tensorflow as tf
from tensorflow import keras
import numpy as np

# 1. åŠ è½½æ•°æ®
(x_train, y_train), (x_test, y_test) = keras.datasets.mnist.load_data()

# 2. æ•°æ®é¢„å¤„ç†
x_train = x_train.reshape(-1, 784).astype('float32') / 255.0
x_test = x_test.reshape(-1, 784).astype('float32') / 255.0

# 3. æ„å»ºæ¨¡å‹
model = keras.Sequential([
    keras.layers.Dense(128, activation='relu', input_shape=(784,)),
    keras.layers.Dropout(0.2),
    keras.layers.Dense(64, activation='relu'),
    keras.layers.Dropout(0.2),
    keras.layers.Dense(10, activation='softmax')
])

# 4. ç¼–è¯‘æ¨¡å‹
model.compile(
    optimizer='adam',
    loss='sparse_categorical_crossentropy',
    metrics=['accuracy']
)

# 5. è®­ç»ƒæ¨¡å‹
history = model.fit(
    x_train, y_train,
    batch_size=128,
    epochs=10,
    validation_split=0.2,
    callbacks=[
        keras.callbacks.EarlyStopping(patience=3, restore_best_weights=True)
    ]
)

# 6. è¯„ä¼°æ¨¡å‹
test_loss, test_acc = model.evaluate(x_test, y_test, verbose=0)
print(f"Test Accuracy: {test_acc:.4f}")

# 7. é¢„æµ‹
predictions = model.predict(x_test[:5])
predicted_classes = np.argmax(predictions, axis=1)
print(f"Predicted: {predicted_classes}")
print(f"Actual: {y_test[:5]}")

# 8. ä¿å­˜æ¨¡å‹
model.save('mnist_model.h5')
```

## 8. å®ç”¨æŠ€å·§

### æŸ¥çœ‹æ¨¡å‹ç»“æ„

```python
model.summary()

# è¾“å‡ºç¤ºä¾‹ï¼š
# Model: "sequential"
# _________________________________________________________________
# Layer (type)                 Output Shape              Param #
# =================================================================
# dense (Dense)                (None, 128)               100480
# dropout (Dropout)            (None, 128)               0
# dense_1 (Dense)              (None, 10)                1290
# =================================================================
# Total params: 101,770
# Trainable params: 101,770
# Non-trainable params: 0
```

### å¯è§†åŒ–æ¨¡å‹

```python
keras.utils.plot_model(model, to_file='model.png', show_shapes=True)
```

### å†»ç»“å±‚ï¼ˆè¿ç§»å­¦ä¹ ï¼‰

```python
# åŠ è½½é¢„è®­ç»ƒæ¨¡å‹
base_model = keras.applications.ResNet50(weights='imagenet', include_top=False)

# å†»ç»“åŸºç¡€æ¨¡å‹çš„æƒé‡
base_model.trainable = False

# æ·»åŠ è‡ªå®šä¹‰é¡¶å±‚
model = keras.Sequential([
    base_model,
    keras.layers.GlobalAveragePooling2D(),
    keras.layers.Dense(10, activation='softmax')
])

# åªè®­ç»ƒé¡¶å±‚
model.compile(optimizer='adam', loss='sparse_categorical_crossentropy')
```

### è·å–ä¸­é—´å±‚è¾“å‡º

```python
# æ–¹æ³•1ï¼šåˆ›å»ºæ–°æ¨¡å‹
layer_name = 'dense_1'
intermediate_model = keras.Model(
    inputs=model.input,
    outputs=model.get_layer(layer_name).output
)
intermediate_output = intermediate_model.predict(x_test)

# æ–¹æ³•2ï¼šä½¿ç”¨ Functional API
from tensorflow.keras import backend as K
get_layer_output = K.function([model.input], [model.layers[2].output])
layer_output = get_layer_output([x_test])[0]
```

## ğŸ”— ä¸‹ä¸€æ­¥

- [03 - æ•°æ®ç®¡é“ tf.data](./03-data-pipeline.md) - é«˜æ•ˆåŠ è½½å’Œé¢„å¤„ç†æ•°æ®
- [04 - æ¨¡å‹è®­ç»ƒè¿›é˜¶](./04-model-building.md) - è‡ªå®šä¹‰è®­ç»ƒå¾ªç¯ã€æ··åˆç²¾åº¦è®­ç»ƒ
- [å®è·µé¡¹ç›®](./practices/) - åŠ¨æ‰‹å®ç°ç»å…¸æ¨¡å‹

## ğŸ“š å‚è€ƒèµ„æº

- [Keras å®˜æ–¹æ–‡æ¡£](https://keras.io/)
- [Sequential API æŒ‡å—](https://keras.io/guides/sequential_model/)
- [Functional API æŒ‡å—](https://keras.io/guides/functional_api/)
- [Keras ç¤ºä¾‹åº“](https://keras.io/examples/)
