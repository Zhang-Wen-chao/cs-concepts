# æ¨¡å‹æ„å»ºä¸è®­ç»ƒè¿›é˜¶

> è‡ªå®šä¹‰è®­ç»ƒå¾ªç¯ã€æ··åˆç²¾åº¦è®­ç»ƒã€æ¨¡å‹ä¼˜åŒ–æŠ€å·§

## ğŸ¯ å­¦ä¹ ç›®æ ‡

å‰é¢æˆ‘ä»¬å­¦ä¹ äº†ç”¨ `model.fit()` è¿›è¡Œè®­ç»ƒï¼Œè¿™å¯¹å¤§å¤šæ•°åœºæ™¯å¤Ÿç”¨ã€‚ä½†æœ‰æ—¶ä½ éœ€è¦æ›´ç»†ç²’åº¦çš„æ§åˆ¶ï¼š
- è‡ªå®šä¹‰æŸå¤±è®¡ç®—é€»è¾‘
- å®ç°å¤æ‚çš„è®­ç»ƒç­–ç•¥ï¼ˆå¦‚ GANã€å¼ºåŒ–å­¦ä¹ ï¼‰
- è°ƒè¯•è®­ç»ƒè¿‡ç¨‹
- ä¼˜åŒ–è®­ç»ƒæ€§èƒ½

## 1. è‡ªå®šä¹‰è®­ç»ƒå¾ªç¯

### ä¸ºä»€ä¹ˆéœ€è¦è‡ªå®šä¹‰è®­ç»ƒå¾ªç¯ï¼Ÿ

`model.fit()` çš„å±€é™ï¼š
- âŒ æ— æ³•å®ç°å¤æ‚çš„æŸå¤±å‡½æ•°ï¼ˆå¦‚å¤šä»»åŠ¡å­¦ä¹ ï¼‰
- âŒ æ— æ³•ç²¾ç»†æ§åˆ¶æ¢¯åº¦æ›´æ–°ï¼ˆå¦‚æ¢¯åº¦è£å‰ªã€æ¢¯åº¦ç´¯ç§¯ï¼‰
- âŒ æ— æ³•å®ç°å¯¹æŠ—è®­ç»ƒï¼ˆGANï¼‰
- âŒ éš¾ä»¥è°ƒè¯•ä¸­é—´è¿‡ç¨‹

### åŸºç¡€ç‰ˆï¼šæ‰‹åŠ¨å®ç°è®­ç»ƒå¾ªç¯

```python
import tensorflow as tf

# 1. å‡†å¤‡æ•°æ®
(x_train, y_train), (x_test, y_test) = tf.keras.datasets.mnist.load_data()
x_train = x_train.reshape(-1, 784).astype('float32') / 255.0
x_test = x_test.reshape(-1, 784).astype('float32') / 255.0

# 2. åˆ›å»ºæ•°æ®é›†
train_dataset = tf.data.Dataset.from_tensor_slices((x_train, y_train))
train_dataset = train_dataset.shuffle(10000).batch(128)

# 3. åˆ›å»ºæ¨¡å‹
model = tf.keras.Sequential([
    tf.keras.layers.Dense(128, activation='relu'),
    tf.keras.layers.Dense(10)
])

# 4. å®šä¹‰æŸå¤±å‡½æ•°å’Œä¼˜åŒ–å™¨
loss_fn = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)
optimizer = tf.keras.optimizers.Adam(learning_rate=0.001)

# 5. è®­ç»ƒå¾ªç¯
epochs = 10
for epoch in range(epochs):
    print(f"Epoch {epoch+1}/{epochs}")

    # éå†æ¯ä¸ªæ‰¹æ¬¡
    for step, (x_batch, y_batch) in enumerate(train_dataset):

        # å‰å‘ä¼ æ’­ + åå‘ä¼ æ’­
        with tf.GradientTape() as tape:
            # é¢„æµ‹
            logits = model(x_batch, training=True)

            # è®¡ç®—æŸå¤±
            loss = loss_fn(y_batch, logits)

        # è®¡ç®—æ¢¯åº¦
        gradients = tape.gradient(loss, model.trainable_variables)

        # æ›´æ–°æƒé‡
        optimizer.apply_gradients(zip(gradients, model.trainable_variables))

        # æ‰“å°è¿›åº¦
        if step % 100 == 0:
            print(f"Step {step}, Loss: {loss.numpy():.4f}")
```

### å®Œæ•´ç‰ˆï¼šå¸¦è¯„ä¼°æŒ‡æ ‡çš„è®­ç»ƒå¾ªç¯

```python
import tensorflow as tf

# åˆ›å»ºè¯„ä¼°æŒ‡æ ‡
train_loss_metric = tf.keras.metrics.Mean(name='train_loss')
train_acc_metric = tf.keras.metrics.SparseCategoricalAccuracy(name='train_accuracy')

val_loss_metric = tf.keras.metrics.Mean(name='val_loss')
val_acc_metric = tf.keras.metrics.SparseCategoricalAccuracy(name='val_accuracy')

# è®­ç»ƒä¸€ä¸ª epoch
def train_step(x, y):
    with tf.GradientTape() as tape:
        logits = model(x, training=True)
        loss = loss_fn(y, logits)

    gradients = tape.gradient(loss, model.trainable_variables)
    optimizer.apply_gradients(zip(gradients, model.trainable_variables))

    # æ›´æ–°æŒ‡æ ‡
    train_loss_metric.update_state(loss)
    train_acc_metric.update_state(y, logits)

    return loss

# éªŒè¯ä¸€ä¸ª epoch
def val_step(x, y):
    logits = model(x, training=False)
    loss = loss_fn(y, logits)

    val_loss_metric.update_state(loss)
    val_acc_metric.update_state(y, logits)

# å®Œæ•´è®­ç»ƒå¾ªç¯
for epoch in range(epochs):
    print(f"\nEpoch {epoch+1}/{epochs}")

    # é‡ç½®æŒ‡æ ‡
    train_loss_metric.reset_states()
    train_acc_metric.reset_states()
    val_loss_metric.reset_states()
    val_acc_metric.reset_states()

    # è®­ç»ƒ
    for x_batch, y_batch in train_dataset:
        train_step(x_batch, y_batch)

    # éªŒè¯
    for x_batch, y_batch in val_dataset:
        val_step(x_batch, y_batch)

    # æ‰“å°ç»“æœ
    print(f"Loss: {train_loss_metric.result():.4f}, "
          f"Accuracy: {train_acc_metric.result():.4f}")
    print(f"Val Loss: {val_loss_metric.result():.4f}, "
          f"Val Accuracy: {val_acc_metric.result():.4f}")
```

### ä½¿ç”¨ @tf.function åŠ é€Ÿ

```python
# å°†è®­ç»ƒæ­¥éª¤ç¼–è¯‘ä¸ºé™æ€å›¾
@tf.function
def train_step(x, y):
    with tf.GradientTape() as tape:
        logits = model(x, training=True)
        loss = loss_fn(y, logits)

    gradients = tape.gradient(loss, model.trainable_variables)
    optimizer.apply_gradients(zip(gradients, model.trainable_variables))

    train_loss_metric.update_state(loss)
    train_acc_metric.update_state(y, logits)

# åŒæ ·ç¼–è¯‘éªŒè¯æ­¥éª¤
@tf.function
def val_step(x, y):
    logits = model(x, training=False)
    loss = loss_fn(y, logits)

    val_loss_metric.update_state(loss)
    val_acc_metric.update_state(y, logits)

# è®­ç»ƒé€Ÿåº¦æå‡ 2-3 å€ï¼
```

## 2. è‡ªå®šä¹‰æŸå¤±å‡½æ•°

### ç®€å•è‡ªå®šä¹‰æŸå¤±

```python
def custom_mse_loss(y_true, y_pred):
    """è‡ªå®šä¹‰å‡æ–¹è¯¯å·®"""
    squared_diff = tf.square(y_true - y_pred)
    return tf.reduce_mean(squared_diff)

# ä½¿ç”¨
model.compile(optimizer='adam', loss=custom_mse_loss)
```

### å¸¦æƒé‡çš„æŸå¤±

```python
def weighted_binary_crossentropy(y_true, y_pred, pos_weight=2.0):
    """å¤„ç†ç±»åˆ«ä¸å¹³è¡¡ï¼šæ­£æ ·æœ¬æƒé‡æ›´é«˜"""
    bce = tf.keras.losses.binary_crossentropy(y_true, y_pred)

    # æ­£æ ·æœ¬æƒé‡ Ã— pos_weightï¼Œè´Ÿæ ·æœ¬æƒé‡ Ã— 1
    weights = y_true * (pos_weight - 1) + 1
    return tf.reduce_mean(bce * weights)

# ä½¿ç”¨
loss_fn = lambda y_true, y_pred: weighted_binary_crossentropy(y_true, y_pred, pos_weight=3.0)
model.compile(optimizer='adam', loss=loss_fn)
```

### å¤šä»»åŠ¡æŸå¤±

```python
def multi_task_loss(y_true, y_pred):
    """
    y_true = [classification_labels, regression_targets]
    y_pred = [classification_logits, regression_predictions]
    """
    cls_labels, reg_targets = y_true
    cls_logits, reg_preds = y_pred

    # åˆ†ç±»æŸå¤±
    cls_loss = tf.keras.losses.sparse_categorical_crossentropy(
        cls_labels, cls_logits, from_logits=True
    )

    # å›å½’æŸå¤±
    reg_loss = tf.keras.losses.mean_squared_error(reg_targets, reg_preds)

    # åŠ æƒç»„åˆ
    total_loss = cls_loss + 0.5 * reg_loss
    return total_loss
```

### Focal Lossï¼ˆå¤„ç†å›°éš¾æ ·æœ¬ï¼‰

```python
def focal_loss(y_true, y_pred, alpha=0.25, gamma=2.0):
    """
    Focal Lossï¼šè®©æ¨¡å‹æ›´å…³æ³¨å›°éš¾æ ·æœ¬
    è®ºæ–‡ï¼šhttps://arxiv.org/abs/1708.02002
    """
    # äºŒåˆ†ç±»äº¤å‰ç†µ
    bce = tf.keras.losses.binary_crossentropy(y_true, y_pred)

    # é¢„æµ‹æ¦‚ç‡
    p_t = y_true * y_pred + (1 - y_true) * (1 - y_pred)

    # Focal æƒé‡ï¼š(1 - p_t)^gamma
    focal_weight = tf.pow(1 - p_t, gamma)

    # ç±»åˆ«æƒé‡
    alpha_weight = y_true * alpha + (1 - y_true) * (1 - alpha)

    return tf.reduce_mean(alpha_weight * focal_weight * bce)
```

## 3. è‡ªå®šä¹‰æŒ‡æ ‡

```python
class F1Score(tf.keras.metrics.Metric):
    """è‡ªå®šä¹‰ F1 åˆ†æ•°æŒ‡æ ‡"""

    def __init__(self, name='f1_score', **kwargs):
        super().__init__(name=name, **kwargs)
        self.precision = tf.keras.metrics.Precision()
        self.recall = tf.keras.metrics.Recall()

    def update_state(self, y_true, y_pred, sample_weight=None):
        self.precision.update_state(y_true, y_pred, sample_weight)
        self.recall.update_state(y_true, y_pred, sample_weight)

    def result(self):
        p = self.precision.result()
        r = self.recall.result()
        # F1 = 2 * (precision * recall) / (precision + recall)
        return 2 * p * r / (p + r + tf.keras.backend.epsilon())

    def reset_states(self):
        self.precision.reset_states()
        self.recall.reset_states()

# ä½¿ç”¨
model.compile(
    optimizer='adam',
    loss='binary_crossentropy',
    metrics=[F1Score()]
)
```

## 4. æ¢¯åº¦è£å‰ªä¸ç´¯ç§¯

### æ¢¯åº¦è£å‰ªï¼ˆé˜²æ­¢æ¢¯åº¦çˆ†ç‚¸ï¼‰

```python
@tf.function
def train_step_with_clipping(x, y, clip_value=1.0):
    with tf.GradientTape() as tape:
        logits = model(x, training=True)
        loss = loss_fn(y, logits)

    gradients = tape.gradient(loss, model.trainable_variables)

    # æ–¹æ³•1ï¼šè£å‰ªæ¢¯åº¦å€¼
    clipped_gradients = [
        tf.clip_by_value(grad, -clip_value, clip_value)
        for grad in gradients
    ]

    # æ–¹æ³•2ï¼šè£å‰ªæ¢¯åº¦èŒƒæ•°ï¼ˆæ›´å¸¸ç”¨ï¼‰
    clipped_gradients, _ = tf.clip_by_global_norm(gradients, clip_norm=1.0)

    optimizer.apply_gradients(zip(clipped_gradients, model.trainable_variables))
```

### æ¢¯åº¦ç´¯ç§¯ï¼ˆæ¨¡æ‹Ÿå¤§ batch sizeï¼‰

```python
# å½“æ˜¾å­˜ä¸è¶³æ—¶ï¼Œç”¨å° batch + æ¢¯åº¦ç´¯ç§¯æ¨¡æ‹Ÿå¤§ batch
accumulation_steps = 4  # ç´¯ç§¯ 4 ä¸ª batch å†æ›´æ–°

@tf.function
def train_step_with_accumulation(x, y, accumulation_steps):
    # ç´¯ç§¯æ¢¯åº¦çš„å˜é‡
    accumulated_gradients = [
        tf.Variable(tf.zeros_like(var), trainable=False)
        for var in model.trainable_variables
    ]

    for step in range(accumulation_steps):
        # å–ä¸€å°æ‰¹æ•°æ®
        x_batch = x[step * batch_size:(step + 1) * batch_size]
        y_batch = y[step * batch_size:(step + 1) * batch_size]

        with tf.GradientTape() as tape:
            logits = model(x_batch, training=True)
            loss = loss_fn(y_batch, logits) / accumulation_steps  # é™¤ä»¥æ­¥æ•°

        # è®¡ç®—æ¢¯åº¦
        gradients = tape.gradient(loss, model.trainable_variables)

        # ç´¯ç§¯æ¢¯åº¦
        for i, grad in enumerate(gradients):
            accumulated_gradients[i].assign_add(grad)

    # åº”ç”¨ç´¯ç§¯çš„æ¢¯åº¦
    optimizer.apply_gradients(zip(accumulated_gradients, model.trainable_variables))

    # é‡ç½®ç´¯ç§¯æ¢¯åº¦
    for grad_var in accumulated_gradients:
        grad_var.assign(tf.zeros_like(grad_var))
```

## 5. æ··åˆç²¾åº¦è®­ç»ƒ

### ä»€ä¹ˆæ˜¯æ··åˆç²¾åº¦ï¼Ÿ

- **float32**ï¼ˆå•ç²¾åº¦ï¼‰ï¼šé»˜è®¤ï¼Œç²¾åº¦é«˜ï¼Œé€Ÿåº¦æ…¢ï¼Œæ˜¾å­˜å ç”¨å¤§
- **float16**ï¼ˆåŠç²¾åº¦ï¼‰ï¼šç²¾åº¦ä½ï¼Œé€Ÿåº¦å¿«ï¼ˆTensor Core åŠ é€Ÿï¼‰ï¼Œæ˜¾å­˜å ç”¨å°
- **æ··åˆç²¾åº¦**ï¼šè®¡ç®—ç”¨ float16ï¼Œå­˜å‚¨ç”¨ float32ï¼Œå…¼é¡¾é€Ÿåº¦å’Œç²¾åº¦

### å¯ç”¨æ··åˆç²¾åº¦

```python
from tensorflow.keras import mixed_precision

# å…¨å±€å¯ç”¨æ··åˆç²¾åº¦
mixed_precision.set_global_policy('mixed_float16')

# åˆ›å»ºæ¨¡å‹ï¼ˆè‡ªåŠ¨ä½¿ç”¨ float16ï¼‰
model = tf.keras.Sequential([
    tf.keras.layers.Dense(128, activation='relu'),
    tf.keras.layers.Dense(10)
])

# æœ€åä¸€å±‚éœ€è¦ float32 è¾“å‡ºï¼ˆæ•°å€¼ç¨³å®šæ€§ï¼‰
model = tf.keras.Sequential([
    tf.keras.layers.Dense(128, activation='relu', dtype='mixed_float16'),
    tf.keras.layers.Dense(10, dtype='float32')  # â† è¾“å‡ºå±‚ç”¨ float32
])

# ç¼–è¯‘æ¨¡å‹
model.compile(
    optimizer='adam',
    loss='sparse_categorical_crossentropy',
    metrics=['accuracy']
)

# æ­£å¸¸è®­ç»ƒï¼ˆé€Ÿåº¦æå‡ 2-3 å€ï¼‰
model.fit(train_dataset, epochs=10)
```

### è‡ªå®šä¹‰è®­ç»ƒå¾ªç¯ä¸­çš„æ··åˆç²¾åº¦

```python
# ä½¿ç”¨ Loss Scale é˜²æ­¢æ¢¯åº¦ä¸‹æº¢
optimizer = tf.keras.optimizers.Adam()
optimizer = mixed_precision.LossScaleOptimizer(optimizer)

@tf.function
def train_step(x, y):
    with tf.GradientTape() as tape:
        logits = model(x, training=True)
        loss = loss_fn(y, logits)

        # Loss Scaling
        scaled_loss = optimizer.get_scaled_loss(loss)

    # è®¡ç®—ç¼©æ”¾åçš„æ¢¯åº¦
    scaled_gradients = tape.gradient(scaled_loss, model.trainable_variables)

    # åç¼©æ”¾æ¢¯åº¦
    gradients = optimizer.get_unscaled_gradients(scaled_gradients)

    # åº”ç”¨æ¢¯åº¦
    optimizer.apply_gradients(zip(gradients, model.trainable_variables))

    return loss
```

### æ··åˆç²¾åº¦çš„æ€§èƒ½å¯¹æ¯”

```python
import time

# float32ï¼ˆé»˜è®¤ï¼‰
mixed_precision.set_global_policy('float32')
model_fp32 = create_model()
start = time.time()
model_fp32.fit(train_dataset, epochs=1)
time_fp32 = time.time() - start

# mixed_float16
mixed_precision.set_global_policy('mixed_float16')
model_fp16 = create_model()
start = time.time()
model_fp16.fit(train_dataset, epochs=1)
time_fp16 = time.time() - start

print(f"FP32: {time_fp32:.2f}s")
print(f"FP16: {time_fp16:.2f}s")
print(f"åŠ é€Ÿ: {time_fp32 / time_fp16:.2f}x")

# å…¸å‹ç»“æœï¼š
# FP32: 120.5s
# FP16: 45.3s
# åŠ é€Ÿ: 2.66x
```

## 6. å­¦ä¹ ç‡è°ƒåº¦

### å­¦ä¹ ç‡è¡°å‡ç­–ç•¥

```python
# 1. æŒ‡æ•°è¡°å‡
lr_schedule = tf.keras.optimizers.schedules.ExponentialDecay(
    initial_learning_rate=0.001,
    decay_steps=1000,
    decay_rate=0.96
)
optimizer = tf.keras.optimizers.Adam(learning_rate=lr_schedule)

# 2. ä½™å¼¦é€€ç«
lr_schedule = tf.keras.optimizers.schedules.CosineDecay(
    initial_learning_rate=0.001,
    decay_steps=10000
)

# 3. åˆ†æ®µå¸¸æ•°è¡°å‡
lr_schedule = tf.keras.optimizers.schedules.PiecewiseConstantDecay(
    boundaries=[1000, 2000],
    values=[0.001, 0.0005, 0.0001]
)

# 4. å¤šé¡¹å¼è¡°å‡
lr_schedule = tf.keras.optimizers.schedules.PolynomialDecay(
    initial_learning_rate=0.001,
    decay_steps=10000,
    end_learning_rate=0.0001,
    power=2.0
)
```

### Warm-up + ä½™å¼¦è¡°å‡

```python
class WarmUpCosineDecay(tf.keras.optimizers.schedules.LearningRateSchedule):
    def __init__(self, warmup_steps, total_steps, initial_lr, target_lr):
        super().__init__()
        self.warmup_steps = warmup_steps
        self.total_steps = total_steps
        self.initial_lr = initial_lr
        self.target_lr = target_lr

    def __call__(self, step):
        # Warm-up é˜¶æ®µï¼šçº¿æ€§å¢é•¿
        warmup_lr = self.initial_lr * step / self.warmup_steps

        # Cosine è¡°å‡é˜¶æ®µ
        decay_steps = self.total_steps - self.warmup_steps
        cosine_decay = 0.5 * (1 + tf.cos(
            3.14159 * (step - self.warmup_steps) / decay_steps
        ))
        decayed_lr = (self.initial_lr - self.target_lr) * cosine_decay + self.target_lr

        # ç»„åˆ
        return tf.where(step < self.warmup_steps, warmup_lr, decayed_lr)

# ä½¿ç”¨
lr_schedule = WarmUpCosineDecay(
    warmup_steps=1000,
    total_steps=10000,
    initial_lr=0.001,
    target_lr=0.00001
)
optimizer = tf.keras.optimizers.Adam(learning_rate=lr_schedule)
```

### ä½¿ç”¨å›è°ƒå‡½æ•°è°ƒæ•´å­¦ä¹ ç‡

```python
# ReduceLROnPlateauï¼šéªŒè¯æŸå¤±ä¸ä¸‹é™æ—¶é™ä½å­¦ä¹ ç‡
reduce_lr = tf.keras.callbacks.ReduceLROnPlateau(
    monitor='val_loss',
    factor=0.5,         # å­¦ä¹ ç‡å‡åŠ
    patience=3,         # 3 ä¸ª epoch ä¸æ”¹å–„
    min_lr=1e-7
)

model.fit(train_dataset, validation_data=val_dataset, callbacks=[reduce_lr])
```

## 7. æ—©åœä¸æ¨¡å‹æ£€æŸ¥ç‚¹

```python
# æ—©åœ
early_stopping = tf.keras.callbacks.EarlyStopping(
    monitor='val_loss',
    patience=10,
    restore_best_weights=True,
    verbose=1
)

# ä¿å­˜æœ€ä½³æ¨¡å‹
checkpoint = tf.keras.callbacks.ModelCheckpoint(
    filepath='best_model.h5',
    monitor='val_accuracy',
    save_best_only=True,
    mode='max',
    verbose=1
)

# è®­ç»ƒ
model.fit(
    train_dataset,
    validation_data=val_dataset,
    epochs=100,
    callbacks=[early_stopping, checkpoint]
)
```

## 8. TensorBoard å¯è§†åŒ–

```python
# åˆ›å»º TensorBoard å›è°ƒ
tensorboard_callback = tf.keras.callbacks.TensorBoard(
    log_dir='./logs',
    histogram_freq=1,       # è®°å½•æƒé‡åˆ†å¸ƒ
    write_graph=True,       # è®°å½•è®¡ç®—å›¾
    update_freq='epoch'     # æ¯ä¸ª epoch æ›´æ–°ä¸€æ¬¡
)

# è®­ç»ƒ
model.fit(train_dataset, callbacks=[tensorboard_callback], epochs=10)

# å¯åŠ¨ TensorBoard
# åœ¨ç»ˆç«¯è¿è¡Œï¼štensorboard --logdir=./logs
# æµè§ˆå™¨æ‰“å¼€ï¼šhttp://localhost:6006
```

### è‡ªå®šä¹‰ TensorBoard æ—¥å¿—

```python
# åˆ›å»ºæ–‡ä»¶å†™å…¥å™¨
train_writer = tf.summary.create_file_writer('logs/train')
val_writer = tf.summary.create_file_writer('logs/val')

for epoch in range(epochs):
    for step, (x, y) in enumerate(train_dataset):
        loss = train_step(x, y)

        # è®°å½•è®­ç»ƒæŸå¤±
        with train_writer.as_default():
            tf.summary.scalar('loss', loss, step=epoch * steps_per_epoch + step)

    # è®°å½•éªŒè¯æŒ‡æ ‡
    with val_writer.as_default():
        tf.summary.scalar('accuracy', val_acc, step=epoch)

        # è®°å½•å›¾åƒ
        tf.summary.image('predictions', images, step=epoch, max_outputs=4)

        # è®°å½•ç›´æ–¹å›¾
        for var in model.trainable_variables:
            tf.summary.histogram(var.name, var, step=epoch)
```

## ğŸ”— ä¸‹ä¸€æ­¥

- [å®è·µé¡¹ç›®](./practices/) - åŠ¨æ‰‹å®ç°å®Œæ•´çš„æ·±åº¦å­¦ä¹ é¡¹ç›®

## ğŸ“š å‚è€ƒèµ„æº

- [è‡ªå®šä¹‰è®­ç»ƒå¾ªç¯](https://www.tensorflow.org/guide/keras/writing_a_training_loop_from_scratch)
- [æ··åˆç²¾åº¦è®­ç»ƒ](https://www.tensorflow.org/guide/mixed_precision)
- [TensorBoard æŒ‡å—](https://www.tensorflow.org/tensorboard)
