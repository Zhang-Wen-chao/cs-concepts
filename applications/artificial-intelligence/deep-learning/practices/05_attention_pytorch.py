"""
Attention æœºåˆ¶ - PyTorchå®ç°
å®é™…åº”ç”¨ï¼šåºåˆ—åˆ°åºåˆ—ä»»åŠ¡ + GPUåŠ é€Ÿ

ä½œè€…: Zhang Wenchao
æ—¥æœŸ: 2025-11-20
"""

import torch
import torch.nn as nn
import torch.nn.functional as F
import numpy as np
import matplotlib.pyplot as plt
from torch.utils.data import Dataset, DataLoader
import time

# è®¾ç½®éšæœºç§å­
torch.manual_seed(42)
np.random.seed(42)


class ScaledDotProductAttention(nn.Module):
    """ç¼©æ”¾ç‚¹ç§¯æ³¨æ„åŠ› - PyTorchç‰ˆæœ¬

    ä½¿ç”¨ PyTorch çš„å¼ é‡æ“ä½œå’Œè‡ªåŠ¨æ±‚å¯¼
    """

    def __init__(self, dropout=0.1):
        super().__init__()
        self.dropout = nn.Dropout(dropout)

    def forward(self, Q, K, V, mask=None):
        """
        å‚æ•°:
            Q: (batch_size, num_heads, seq_len, d_k)
            K: (batch_size, num_heads, seq_len, d_k)
            V: (batch_size, num_heads, seq_len, d_v)
            mask: (batch_size, 1, 1, seq_len) æˆ– None

        è¿”å›:
            output: (batch_size, num_heads, seq_len, d_v)
            attention_weights: (batch_size, num_heads, seq_len, seq_len)
        """
        d_k = Q.size(-1)

        # 1. è®¡ç®—æ³¨æ„åŠ›åˆ†æ•°
        scores = torch.matmul(Q, K.transpose(-2, -1)) / np.sqrt(d_k)

        # 2. åº”ç”¨æ©ç 
        if mask is not None:
            scores = scores.masked_fill(mask == 0, -1e9)

        # 3. Softmax å¾—åˆ°æ³¨æ„åŠ›æƒé‡
        attention_weights = F.softmax(scores, dim=-1)
        attention_weights = self.dropout(attention_weights)

        # 4. åŠ æƒæ±‚å’Œ
        output = torch.matmul(attention_weights, V)

        return output, attention_weights


class MultiHeadAttention(nn.Module):
    """å¤šå¤´æ³¨æ„åŠ› - PyTorchç‰ˆæœ¬

    ä½¿ç”¨ nn.Linear å®ç°çº¿æ€§æŠ•å½±ï¼Œæ”¯æŒè‡ªåŠ¨æ±‚å¯¼å’ŒGPUåŠ é€Ÿ
    """

    def __init__(self, d_model, num_heads, dropout=0.1):
        super().__init__()
        assert d_model % num_heads == 0, "d_model å¿…é¡»èƒ½è¢« num_heads æ•´é™¤"

        self.d_model = d_model
        self.num_heads = num_heads
        self.d_k = d_model // num_heads

        # çº¿æ€§æŠ•å½±å±‚ï¼ˆè‡ªåŠ¨åˆå§‹åŒ–ï¼‰
        self.W_Q = nn.Linear(d_model, d_model)
        self.W_K = nn.Linear(d_model, d_model)
        self.W_V = nn.Linear(d_model, d_model)
        self.W_O = nn.Linear(d_model, d_model)

        self.attention = ScaledDotProductAttention(dropout)
        self.dropout = nn.Dropout(dropout)

    def split_heads(self, x):
        """
        æ‹†åˆ†ä¸ºå¤šä¸ªå¤´

        è¾“å…¥: (batch_size, seq_len, d_model)
        è¾“å‡º: (batch_size, num_heads, seq_len, d_k)
        """
        batch_size, seq_len, _ = x.size()
        x = x.view(batch_size, seq_len, self.num_heads, self.d_k)
        return x.transpose(1, 2)  # (batch, num_heads, seq_len, d_k)

    def combine_heads(self, x):
        """
        åˆå¹¶å¤šä¸ªå¤´

        è¾“å…¥: (batch_size, num_heads, seq_len, d_k)
        è¾“å‡º: (batch_size, seq_len, d_model)
        """
        batch_size, _, seq_len, _ = x.size()
        x = x.transpose(1, 2)  # (batch, seq_len, num_heads, d_k)
        return x.contiguous().view(batch_size, seq_len, self.d_model)

    def forward(self, Q, K, V, mask=None):
        """
        å‰å‘ä¼ æ’­

        å‚æ•°:
            Q, K, V: (batch_size, seq_len, d_model)
            mask: (batch_size, 1, 1, seq_len) æˆ– None

        è¿”å›:
            output: (batch_size, seq_len, d_model)
            attention_weights: (batch_size, num_heads, seq_len, seq_len)
        """
        # 1. çº¿æ€§æŠ•å½±
        Q = self.W_Q(Q)
        K = self.W_K(K)
        V = self.W_V(V)

        # 2. æ‹†åˆ†ä¸ºå¤šä¸ªå¤´
        Q = self.split_heads(Q)
        K = self.split_heads(K)
        V = self.split_heads(V)

        # 3. è®¡ç®—æ³¨æ„åŠ›
        x, attention_weights = self.attention(Q, K, V, mask)

        # 4. åˆå¹¶å¤šä¸ªå¤´
        x = self.combine_heads(x)

        # 5. æœ€ç»ˆæŠ•å½±
        output = self.W_O(x)

        return output, attention_weights


class PositionwiseFeedForward(nn.Module):
    """Position-wise å‰é¦ˆç½‘ç»œ

    Transformer çš„ç¬¬äºŒä¸ªå­å±‚ï¼šä¸¤å±‚å…¨è¿æ¥ç½‘ç»œ
    FFN(x) = max(0, xW1 + b1)W2 + b2
    """

    def __init__(self, d_model, d_ff, dropout=0.1):
        super().__init__()
        self.fc1 = nn.Linear(d_model, d_ff)
        self.fc2 = nn.Linear(d_ff, d_model)
        self.dropout = nn.Dropout(dropout)

    def forward(self, x):
        return self.fc2(self.dropout(F.relu(self.fc1(x))))


class EncoderLayer(nn.Module):
    """Transformer Encoder å±‚

    åŒ…å«ï¼š
    1. Multi-Head Self-Attention
    2. Add & Norm
    3. Feed-Forward Network
    4. Add & Norm
    """

    def __init__(self, d_model, num_heads, d_ff, dropout=0.1):
        super().__init__()
        self.self_attn = MultiHeadAttention(d_model, num_heads, dropout)
        self.ffn = PositionwiseFeedForward(d_model, d_ff, dropout)

        self.norm1 = nn.LayerNorm(d_model)
        self.norm2 = nn.LayerNorm(d_model)

        self.dropout1 = nn.Dropout(dropout)
        self.dropout2 = nn.Dropout(dropout)

    def forward(self, x, mask=None):
        """
        å‚æ•°:
            x: (batch_size, seq_len, d_model)
            mask: (batch_size, 1, 1, seq_len)

        è¿”å›:
            output: (batch_size, seq_len, d_model)
            attention_weights: (batch_size, num_heads, seq_len, seq_len)
        """
        # 1. Multi-Head Self-Attention + Residual + Norm
        attn_output, attention_weights = self.self_attn(x, x, x, mask)
        x = self.norm1(x + self.dropout1(attn_output))

        # 2. Feed-Forward + Residual + Norm
        ffn_output = self.ffn(x)
        x = self.norm2(x + self.dropout2(ffn_output))

        return x, attention_weights


class PositionalEncoding(nn.Module):
    """ä½ç½®ç¼–ç 

    ç”±äº Attention æœºåˆ¶æ²¡æœ‰ä½ç½®ä¿¡æ¯ï¼Œéœ€è¦æ‰‹åŠ¨æ·»åŠ ä½ç½®ç¼–ç 
    PE(pos, 2i)   = sin(pos / 10000^(2i/d_model))
    PE(pos, 2i+1) = cos(pos / 10000^(2i/d_model))
    """

    def __init__(self, d_model, max_len=5000, dropout=0.1):
        super().__init__()
        self.dropout = nn.Dropout(dropout)

        # åˆ›å»ºä½ç½®ç¼–ç çŸ©é˜µ
        pe = torch.zeros(max_len, d_model)
        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)
        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-np.log(10000.0) / d_model))

        pe[:, 0::2] = torch.sin(position * div_term)
        pe[:, 1::2] = torch.cos(position * div_term)

        pe = pe.unsqueeze(0)  # (1, max_len, d_model)
        self.register_buffer('pe', pe)

    def forward(self, x):
        """
        å‚æ•°:
            x: (batch_size, seq_len, d_model)
        """
        x = x + self.pe[:, :x.size(1)]
        return self.dropout(x)


class SimpleTransformerEncoder(nn.Module):
    """ç®€å•çš„ Transformer Encoderï¼ˆç”¨äºåºåˆ—åˆ†ç±»ï¼‰

    åº”ç”¨ï¼šæ–‡æœ¬åˆ†ç±»ã€æƒ…æ„Ÿåˆ†æç­‰
    """

    def __init__(self, vocab_size, d_model=128, num_heads=8, num_layers=3,
                 d_ff=512, max_len=100, num_classes=2, dropout=0.1):
        super().__init__()

        self.d_model = d_model

        # Embeddingå±‚
        self.embedding = nn.Embedding(vocab_size, d_model)
        self.pos_encoding = PositionalEncoding(d_model, max_len, dropout)

        # Encoderå±‚å †å 
        self.encoder_layers = nn.ModuleList([
            EncoderLayer(d_model, num_heads, d_ff, dropout)
            for _ in range(num_layers)
        ])

        # åˆ†ç±»å¤´
        self.fc = nn.Linear(d_model, num_classes)
        self.dropout = nn.Dropout(dropout)

    def forward(self, x, mask=None):
        """
        å‚æ•°:
            x: (batch_size, seq_len) - è¯IDåºåˆ—
            mask: (batch_size, 1, 1, seq_len)

        è¿”å›:
            output: (batch_size, num_classes)
            all_attention_weights: list of attention weights
        """
        # 1. Embedding + Positional Encoding
        x = self.embedding(x) * np.sqrt(self.d_model)  # ç¼©æ”¾
        x = self.pos_encoding(x)

        # 2. é€šè¿‡æ‰€æœ‰ Encoder å±‚
        all_attention_weights = []
        for encoder_layer in self.encoder_layers:
            x, attn_weights = encoder_layer(x, mask)
            all_attention_weights.append(attn_weights)

        # 3. æ± åŒ–ï¼šå–ç¬¬ä¸€ä¸ªtokençš„è¡¨ç¤ºï¼ˆç±»ä¼¼BERTçš„[CLS]ï¼‰
        x = x[:, 0, :]

        # 4. åˆ†ç±»
        output = self.fc(self.dropout(x))

        return output, all_attention_weights


# ============ å®é™…åº”ç”¨ï¼šåºåˆ—åˆ†ç±»ä»»åŠ¡ ============

class SyntheticSequenceDataset(Dataset):
    """åˆæˆåºåˆ—åˆ†ç±»æ•°æ®é›†

    ä»»åŠ¡ï¼šåˆ¤æ–­åºåˆ—æ˜¯å¦åŒ…å«ç‰¹å®šæ¨¡å¼
    - ç±»åˆ«0: åºåˆ—ä¸­æ²¡æœ‰è¿ç»­çš„å¤§æ•°å­— (>50)
    - ç±»åˆ«1: åºåˆ—ä¸­æœ‰è¿ç»­çš„å¤§æ•°å­—
    """

    def __init__(self, num_samples=1000, seq_len=20, vocab_size=100):
        self.num_samples = num_samples
        self.seq_len = seq_len
        self.vocab_size = vocab_size

        self.data = []
        self.labels = []

        for _ in range(num_samples):
            seq = np.random.randint(1, vocab_size, size=seq_len)

            # è§„åˆ™ï¼šå¦‚æœæœ‰è¿ç»­3ä¸ªå¤§äº50çš„æ•°å­—ï¼Œæ ‡ç­¾ä¸º1
            has_pattern = False
            for i in range(seq_len - 2):
                if seq[i] > 50 and seq[i+1] > 50 and seq[i+2] > 50:
                    has_pattern = True
                    break

            self.data.append(seq)
            self.labels.append(1 if has_pattern else 0)

    def __len__(self):
        return self.num_samples

    def __getitem__(self, idx):
        return torch.LongTensor(self.data[idx]), torch.LongTensor([self.labels[idx]])


def train_model(model, train_loader, val_loader, device, num_epochs=10):
    """è®­ç»ƒæ¨¡å‹"""
    criterion = nn.CrossEntropyLoss()
    optimizer = torch.optim.Adam(model.parameters(), lr=0.001)

    history = {'train_loss': [], 'train_acc': [], 'val_loss': [], 'val_acc': []}

    print("\nå¼€å§‹è®­ç»ƒ...")
    for epoch in range(num_epochs):
        # è®­ç»ƒé˜¶æ®µ
        model.train()
        train_loss = 0
        train_correct = 0
        train_total = 0

        for batch_x, batch_y in train_loader:
            batch_x, batch_y = batch_x.to(device), batch_y.to(device).squeeze()

            optimizer.zero_grad()
            outputs, _ = model(batch_x)
            loss = criterion(outputs, batch_y)
            loss.backward()
            optimizer.step()

            train_loss += loss.item()
            _, predicted = outputs.max(1)
            train_total += batch_y.size(0)
            train_correct += predicted.eq(batch_y).sum().item()

        train_loss /= len(train_loader)
        train_acc = 100. * train_correct / train_total

        # éªŒè¯é˜¶æ®µ
        model.eval()
        val_loss = 0
        val_correct = 0
        val_total = 0

        with torch.no_grad():
            for batch_x, batch_y in val_loader:
                batch_x, batch_y = batch_x.to(device), batch_y.to(device).squeeze()

                outputs, _ = model(batch_x)
                loss = criterion(outputs, batch_y)

                val_loss += loss.item()
                _, predicted = outputs.max(1)
                val_total += batch_y.size(0)
                val_correct += predicted.eq(batch_y).sum().item()

        val_loss /= len(val_loader)
        val_acc = 100. * val_correct / val_total

        # è®°å½•å†å²
        history['train_loss'].append(train_loss)
        history['train_acc'].append(train_acc)
        history['val_loss'].append(val_loss)
        history['val_acc'].append(val_acc)

        print(f'Epoch {epoch+1}/{num_epochs}:')
        print(f'  Train Loss: {train_loss:.4f}, Train Acc: {train_acc:.2f}%')
        print(f'  Val Loss: {val_loss:.4f}, Val Acc: {val_acc:.2f}%')

    return history


def visualize_attention(model, data_loader, device, save_path='attention_heatmap.png'):
    """å¯è§†åŒ–æ³¨æ„åŠ›æƒé‡"""
    model.eval()

    # è·å–ä¸€ä¸ªbatch
    batch_x, batch_y = next(iter(data_loader))
    batch_x, batch_y = batch_x.to(device), batch_y.to(device)

    # å‰å‘ä¼ æ’­è·å–æ³¨æ„åŠ›æƒé‡
    with torch.no_grad():
        outputs, all_attention_weights = model(batch_x)

    # å–ç¬¬ä¸€ä¸ªæ ·æœ¬çš„ç¬¬ä¸€ä¸ªå¤´çš„æ³¨æ„åŠ›æƒé‡
    attention = all_attention_weights[0][0, 0].cpu().numpy()  # ç¬¬ä¸€å±‚ï¼Œç¬¬ä¸€ä¸ªæ ·æœ¬ï¼Œç¬¬ä¸€ä¸ªå¤´

    # å¯è§†åŒ–
    fig, ax = plt.subplots(figsize=(10, 8))
    im = ax.imshow(attention, cmap='viridis', aspect='auto')

    ax.set_xlabel('Key Position')
    ax.set_ylabel('Query Position')
    ax.set_title('Multi-Head Attention Weights (Layer 1, Head 1)')

    plt.colorbar(im, ax=ax)
    plt.tight_layout()
    plt.savefig(save_path, dpi=150, bbox_inches='tight')
    print(f"æ³¨æ„åŠ›å¯è§†åŒ–å·²ä¿å­˜åˆ° {save_path}")
    plt.close()


def plot_training_history(history, save_path='training_history.png'):
    """ç»˜åˆ¶è®­ç»ƒå†å²"""
    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 4))

    # Lossæ›²çº¿
    ax1.plot(history['train_loss'], label='Train Loss')
    ax1.plot(history['val_loss'], label='Val Loss')
    ax1.set_xlabel('Epoch')
    ax1.set_ylabel('Loss')
    ax1.set_title('Training and Validation Loss')
    ax1.legend()
    ax1.grid(True)

    # Accuracyæ›²çº¿
    ax2.plot(history['train_acc'], label='Train Acc')
    ax2.plot(history['val_acc'], label='Val Acc')
    ax2.set_xlabel('Epoch')
    ax2.set_ylabel('Accuracy (%)')
    ax2.set_title('Training and Validation Accuracy')
    ax2.legend()
    ax2.grid(True)

    plt.tight_layout()
    plt.savefig(save_path, dpi=150, bbox_inches='tight')
    print(f"è®­ç»ƒå†å²å·²ä¿å­˜åˆ° {save_path}")
    plt.close()


def compare_cpu_gpu_speed():
    """å¯¹æ¯”CPUå’ŒGPUçš„é€Ÿåº¦"""
    print("\n" + "=" * 60)
    print("GPU vs CPU é€Ÿåº¦å¯¹æ¯”")
    print("=" * 60)

    d_model = 512
    num_heads = 8
    batch_size = 32
    seq_len = 100

    # åˆ›å»ºæ¨¡å‹å’Œæ•°æ®
    model = MultiHeadAttention(d_model, num_heads)
    x = torch.randn(batch_size, seq_len, d_model)

    # CPUæµ‹è¯•
    model_cpu = model.cpu()
    x_cpu = x.cpu()

    start = time.time()
    for _ in range(100):
        _ = model_cpu(x_cpu, x_cpu, x_cpu)
    cpu_time = time.time() - start

    print(f"CPU æ—¶é—´ (100æ¬¡å‰å‘ä¼ æ’­): {cpu_time:.4f}ç§’")

    # GPUæµ‹è¯•
    if torch.cuda.is_available():
        model_gpu = model.cuda()
        x_gpu = x.cuda()

        # é¢„çƒ­
        for _ in range(10):
            _ = model_gpu(x_gpu, x_gpu, x_gpu)
        torch.cuda.synchronize()

        start = time.time()
        for _ in range(100):
            _ = model_gpu(x_gpu, x_gpu, x_gpu)
        torch.cuda.synchronize()
        gpu_time = time.time() - start

        print(f"GPU æ—¶é—´ (100æ¬¡å‰å‘ä¼ æ’­): {gpu_time:.4f}ç§’")
        print(f"åŠ é€Ÿæ¯”: {cpu_time/gpu_time:.2f}x")
    else:
        print("GPU ä¸å¯ç”¨")


def main():
    print("\n" + "ğŸš€ " + "=" * 58)
    print("  Attention æœºåˆ¶ - PyTorchå®ç°")
    print("  å®æˆ˜ï¼šåºåˆ—åˆ†ç±» + GPUåŠ é€Ÿ")
    print("=" * 60)

    # æ£€æŸ¥GPU
    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
    print(f"\nä½¿ç”¨è®¾å¤‡: {device}")
    if torch.cuda.is_available():
        print(f"GPUå‹å·: {torch.cuda.get_device_name(0)}")

    # é€Ÿåº¦å¯¹æ¯”
    compare_cpu_gpu_speed()

    # åˆ›å»ºæ•°æ®é›†
    print("\n" + "=" * 60)
    print("åˆ›å»ºåˆæˆæ•°æ®é›†")
    print("=" * 60)

    train_dataset = SyntheticSequenceDataset(num_samples=2000, seq_len=20, vocab_size=100)
    val_dataset = SyntheticSequenceDataset(num_samples=500, seq_len=20, vocab_size=100)

    train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)
    val_loader = DataLoader(val_dataset, batch_size=64, shuffle=False)

    print(f"è®­ç»ƒé›†å¤§å°: {len(train_dataset)}")
    print(f"éªŒè¯é›†å¤§å°: {len(val_dataset)}")

    # åˆ›å»ºæ¨¡å‹
    print("\n" + "=" * 60)
    print("åˆ›å»º Transformer Encoder æ¨¡å‹")
    print("=" * 60)

    model = SimpleTransformerEncoder(
        vocab_size=100,
        d_model=128,
        num_heads=8,
        num_layers=3,
        d_ff=512,
        max_len=20,
        num_classes=2,
        dropout=0.1
    ).to(device)

    print(f"æ¨¡å‹å‚æ•°é‡: {sum(p.numel() for p in model.parameters()):,}")

    # è®­ç»ƒæ¨¡å‹
    print("\n" + "=" * 60)
    print("è®­ç»ƒæ¨¡å‹")
    print("=" * 60)

    history = train_model(model, train_loader, val_loader, device, num_epochs=20)

    # å¯è§†åŒ–
    print("\n" + "=" * 60)
    print("ç”Ÿæˆå¯è§†åŒ–")
    print("=" * 60)

    plot_training_history(history)
    visualize_attention(model, val_loader, device)

    print("\n" + "=" * 60)
    print("å­¦ä¹ æ€»ç»“")
    print("=" * 60)

    print("""
1. PyTorch vs NumPy çš„åŒºåˆ«
   âœ“ è‡ªåŠ¨æ±‚å¯¼: ä¸éœ€è¦æ‰‹å†™åå‘ä¼ æ’­
   âœ“ GPUåŠ é€Ÿ: .to(device) å³å¯ä½¿ç”¨GPU
   âœ“ æ¨¡å—åŒ–: nn.Module å°è£…æ¨¡å‹
   âœ“ ä¼˜åŒ–å™¨: torch.optim è‡ªåŠ¨æ›´æ–°å‚æ•°

2. Transformer Encoder ç»„ä»¶
   âœ“ Positional Encoding: æ·»åŠ ä½ç½®ä¿¡æ¯
   âœ“ Multi-Head Attention: å¤šè§’åº¦å…³æ³¨
   âœ“ Feed-Forward Network: éçº¿æ€§å˜æ¢
   âœ“ Layer Normalization: ç¨³å®šè®­ç»ƒ

3. å·¥ä¸šå®è·µæŠ€å·§
   âœ“ Dropout: é˜²æ­¢è¿‡æ‹Ÿåˆ
   âœ“ Residual Connection: ç¼“è§£æ¢¯åº¦æ¶ˆå¤±
   âœ“ Layer Norm: åŠ é€Ÿæ”¶æ•›
   âœ“ Learning Rate Scheduling: æå‡æ€§èƒ½

4. ä¸‹ä¸€æ­¥
   â†’ å®Œæ•´çš„ Transformerï¼ˆEncoder + Decoderï¼‰
   â†’ é¢„è®­ç»ƒæ¨¡å‹ï¼ˆBERTã€GPTï¼‰
   â†’ å®é™…NLPä»»åŠ¡ï¼ˆæ–‡æœ¬åˆ†ç±»ã€ç¿»è¯‘ï¼‰
    """)

    print("\nâœ… è®­ç»ƒå®Œæˆï¼")


if __name__ == "__main__":
    main()
