"""
Transformer æ¶æ„ - PyTorchå®ç°
å®Œæ•´çš„ Encoder-Decoder ç»“æ„ + æœºå™¨ç¿»è¯‘å®æˆ˜

ä½œè€…: Zhang Wenchao
æ—¥æœŸ: 2025-11-21
"""

import torch
import torch.nn as nn
import torch.nn.functional as F
import numpy as np
import matplotlib.pyplot as plt
from torch.utils.data import Dataset, DataLoader
import time

# è®¾ç½®éšæœºç§å­
torch.manual_seed(42)
np.random.seed(42)

# æ£€æµ‹è®¾å¤‡
DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')


class ScaledDotProductAttention(nn.Module):
    """ç¼©æ”¾ç‚¹ç§¯æ³¨æ„åŠ›ï¼ˆä¸ 06 ç›¸åŒï¼‰"""

    def __init__(self, dropout=0.1):
        super().__init__()
        self.dropout = nn.Dropout(dropout)

    def forward(self, Q, K, V, mask=None):
        d_k = Q.size(-1)
        scores = torch.matmul(Q, K.transpose(-2, -1)) / np.sqrt(d_k)

        if mask is not None:
            scores = scores.masked_fill(mask == 0, -1e9)

        attention_weights = F.softmax(scores, dim=-1)
        attention_weights = self.dropout(attention_weights)

        output = torch.matmul(attention_weights, V)
        return output, attention_weights


class MultiHeadAttention(nn.Module):
    """å¤šå¤´æ³¨æ„åŠ›ï¼ˆæ”¯æŒ Self-Attention å’Œ Cross-Attentionï¼‰"""

    def __init__(self, d_model, num_heads, dropout=0.1):
        super().__init__()
        assert d_model % num_heads == 0

        self.d_model = d_model
        self.num_heads = num_heads
        self.d_k = d_model // num_heads

        self.W_Q = nn.Linear(d_model, d_model)
        self.W_K = nn.Linear(d_model, d_model)
        self.W_V = nn.Linear(d_model, d_model)
        self.W_O = nn.Linear(d_model, d_model)

        self.attention = ScaledDotProductAttention(dropout)
        self.dropout = nn.Dropout(dropout)

    def split_heads(self, x):
        batch_size, seq_len, _ = x.size()
        x = x.view(batch_size, seq_len, self.num_heads, self.d_k)
        return x.transpose(1, 2)

    def combine_heads(self, x):
        batch_size, _, seq_len, _ = x.size()
        x = x.transpose(1, 2)
        return x.contiguous().view(batch_size, seq_len, self.d_model)

    def forward(self, Q, K, V, mask=None):
        Q = self.W_Q(Q)
        K = self.W_K(K)
        V = self.W_V(V)

        Q = self.split_heads(Q)
        K = self.split_heads(K)
        V = self.split_heads(V)

        x, attention_weights = self.attention(Q, K, V, mask)
        x = self.combine_heads(x)
        output = self.W_O(x)

        return output, attention_weights


class PositionwiseFeedForward(nn.Module):
    """ä½ç½®å‰é¦ˆç½‘ç»œ"""

    def __init__(self, d_model, d_ff, dropout=0.1):
        super().__init__()
        self.fc1 = nn.Linear(d_model, d_ff)
        self.fc2 = nn.Linear(d_ff, d_model)
        self.dropout = nn.Dropout(dropout)

    def forward(self, x):
        return self.fc2(self.dropout(F.relu(self.fc1(x))))


class PositionalEncoding(nn.Module):
    """ä½ç½®ç¼–ç """

    def __init__(self, d_model, max_len=5000, dropout=0.1):
        super().__init__()
        self.dropout = nn.Dropout(dropout)

        pe = torch.zeros(max_len, d_model)
        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)
        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-np.log(10000.0) / d_model))

        pe[:, 0::2] = torch.sin(position * div_term)
        pe[:, 1::2] = torch.cos(position * div_term)

        pe = pe.unsqueeze(0)
        self.register_buffer('pe', pe)

    def forward(self, x):
        x = x + self.pe[:, :x.size(1)]
        return self.dropout(x)


class EncoderLayer(nn.Module):
    """Transformer Encoder å±‚"""

    def __init__(self, d_model, num_heads, d_ff, dropout=0.1):
        super().__init__()
        self.self_attn = MultiHeadAttention(d_model, num_heads, dropout)
        self.ffn = PositionwiseFeedForward(d_model, d_ff, dropout)

        self.norm1 = nn.LayerNorm(d_model)
        self.norm2 = nn.LayerNorm(d_model)

        self.dropout1 = nn.Dropout(dropout)
        self.dropout2 = nn.Dropout(dropout)

    def forward(self, x, mask=None):
        attn_output, _ = self.self_attn(x, x, x, mask)
        x = self.norm1(x + self.dropout1(attn_output))

        ffn_output = self.ffn(x)
        x = self.norm2(x + self.dropout2(ffn_output))

        return x


class DecoderLayer(nn.Module):
    """Transformer Decoder å±‚ï¼ˆæ–°å¢ï¼ï¼‰

    åŒ…å«ä¸‰ä¸ªå­å±‚ï¼š
    1. Masked Self-Attentionï¼ˆåªèƒ½çœ‹åˆ°å½“å‰å’Œä¹‹å‰çš„è¯ï¼‰
    2. Cross-Attentionï¼ˆå…³æ³¨ Encoder è¾“å‡ºï¼‰
    3. Feed-Forward Network
    """

    def __init__(self, d_model, num_heads, d_ff, dropout=0.1):
        super().__init__()
        self.self_attn = MultiHeadAttention(d_model, num_heads, dropout)
        self.cross_attn = MultiHeadAttention(d_model, num_heads, dropout)
        self.ffn = PositionwiseFeedForward(d_model, d_ff, dropout)

        self.norm1 = nn.LayerNorm(d_model)
        self.norm2 = nn.LayerNorm(d_model)
        self.norm3 = nn.LayerNorm(d_model)

        self.dropout1 = nn.Dropout(dropout)
        self.dropout2 = nn.Dropout(dropout)
        self.dropout3 = nn.Dropout(dropout)

    def forward(self, x, encoder_output, src_mask=None, tgt_mask=None):
        """
        å‚æ•°:
            x: (batch, tgt_seq_len, d_model) - Decoder è¾“å…¥
            encoder_output: (batch, src_seq_len, d_model) - Encoder è¾“å‡º
            src_mask: Encoder çš„ padding mask
            tgt_mask: Decoder çš„ look-ahead maskï¼ˆä¸‹ä¸‰è§’ï¼‰

        è¿”å›:
            output: (batch, tgt_seq_len, d_model)
        """
        # 1. Masked Self-Attention + Residual + Norm
        self_attn_output, _ = self.self_attn(x, x, x, tgt_mask)
        x = self.norm1(x + self.dropout1(self_attn_output))

        # 2. Cross-Attention + Residual + Norm
        # Query æ¥è‡ª Decoderï¼ŒKey å’Œ Value æ¥è‡ª Encoder
        cross_attn_output, _ = self.cross_attn(x, encoder_output, encoder_output, src_mask)
        x = self.norm2(x + self.dropout2(cross_attn_output))

        # 3. FFN + Residual + Norm
        ffn_output = self.ffn(x)
        x = self.norm3(x + self.dropout3(ffn_output))

        return x


class TransformerEncoder(nn.Module):
    """Transformer Encoderï¼ˆå †å å¤šä¸ª EncoderLayerï¼‰"""

    def __init__(self, vocab_size, d_model, num_heads, num_layers, d_ff, max_len, dropout=0.1):
        super().__init__()
        self.d_model = d_model

        self.embedding = nn.Embedding(vocab_size, d_model)
        self.pos_encoding = PositionalEncoding(d_model, max_len, dropout)

        self.layers = nn.ModuleList([
            EncoderLayer(d_model, num_heads, d_ff, dropout)
            for _ in range(num_layers)
        ])

    def forward(self, x, mask=None):
        x = self.embedding(x) * np.sqrt(self.d_model)
        x = self.pos_encoding(x)

        for layer in self.layers:
            x = layer(x, mask)

        return x


class TransformerDecoder(nn.Module):
    """Transformer Decoderï¼ˆå †å å¤šä¸ª DecoderLayerï¼‰"""

    def __init__(self, vocab_size, d_model, num_heads, num_layers, d_ff, max_len, dropout=0.1):
        super().__init__()
        self.d_model = d_model

        self.embedding = nn.Embedding(vocab_size, d_model)
        self.pos_encoding = PositionalEncoding(d_model, max_len, dropout)

        self.layers = nn.ModuleList([
            DecoderLayer(d_model, num_heads, d_ff, dropout)
            for _ in range(num_layers)
        ])

        self.fc = nn.Linear(d_model, vocab_size)

    def forward(self, x, encoder_output, src_mask=None, tgt_mask=None):
        x = self.embedding(x) * np.sqrt(self.d_model)
        x = self.pos_encoding(x)

        for layer in self.layers:
            x = layer(x, encoder_output, src_mask, tgt_mask)

        output = self.fc(x)
        return output


class Transformer(nn.Module):
    """å®Œæ•´çš„ Transformer æ¨¡å‹ï¼ˆEncoder-Decoderï¼‰"""

    def __init__(self, src_vocab_size, tgt_vocab_size, d_model=256, num_heads=8,
                 num_layers=6, d_ff=2048, max_len=100, dropout=0.1):
        super().__init__()

        self.encoder = TransformerEncoder(
            src_vocab_size, d_model, num_heads, num_layers, d_ff, max_len, dropout
        )
        self.decoder = TransformerDecoder(
            tgt_vocab_size, d_model, num_heads, num_layers, d_ff, max_len, dropout
        )

    def create_look_ahead_mask(self, size):
        """åˆ›å»º Decoder çš„ look-ahead maskï¼ˆä¸‹ä¸‰è§’çŸ©é˜µï¼‰"""
        mask = torch.triu(torch.ones(size, size), diagonal=1).bool()
        return ~mask  # åè½¬ï¼šä¸‹ä¸‰è§’ä¸º Trueï¼ˆå…è®¸ï¼‰ï¼Œä¸Šä¸‰è§’ä¸º Falseï¼ˆç¦æ­¢ï¼‰

    def forward(self, src, tgt):
        """
        å‚æ•°:
            src: (batch, src_seq_len) - æºåºåˆ—
            tgt: (batch, tgt_seq_len) - ç›®æ ‡åºåˆ—

        è¿”å›:
            output: (batch, tgt_seq_len, tgt_vocab_size)
        """
        # 1. Encoder
        encoder_output = self.encoder(src)

        # 2. åˆ›å»º Decoder çš„ look-ahead mask
        tgt_seq_len = tgt.size(1)
        tgt_mask = self.create_look_ahead_mask(tgt_seq_len).to(tgt.device)
        tgt_mask = tgt_mask.unsqueeze(0).unsqueeze(0)  # (1, 1, seq_len, seq_len)

        # 3. Decoder
        output = self.decoder(tgt, encoder_output, tgt_mask=tgt_mask)

        return output


# ============ å®é™…åº”ç”¨ï¼šç®€å•çš„åºåˆ—åˆ°åºåˆ—ä»»åŠ¡ ============

class Seq2SeqDataset(Dataset):
    """ç®€å•çš„åºåˆ—åˆ°åºåˆ—æ•°æ®é›†

    ä»»åŠ¡ï¼šæ•°å­—åºåˆ—åè½¬
    ä¾‹å­: [1, 2, 3, 4] -> [4, 3, 2, 1]
    """

    def __init__(self, num_samples=1000, seq_len=10, vocab_size=50):
        self.num_samples = num_samples
        self.seq_len = seq_len
        self.vocab_size = vocab_size

        self.data = []
        self.targets = []

        # ç‰¹æ®Šæ ‡è®°
        self.PAD_IDX = 0
        self.BOS_IDX = 1  # Begin of Sequence
        self.EOS_IDX = 2  # End of Sequence

        for _ in range(num_samples):
            # ç”Ÿæˆæºåºåˆ—ï¼ˆä»3å¼€å§‹ï¼Œé¿å…ç‰¹æ®Šæ ‡è®°ï¼‰
            src = np.random.randint(3, vocab_size, size=seq_len)

            # ç›®æ ‡åºåˆ—ï¼šåè½¬ + æ·»åŠ  BOS å’Œ EOS
            tgt_input = np.concatenate([[self.BOS_IDX], src[::-1]])  # [BOS, åè½¬åºåˆ—]
            tgt_output = np.concatenate([src[::-1], [self.EOS_IDX]])  # [åè½¬åºåˆ—, EOS]

            self.data.append((src, tgt_input))
            self.targets.append(tgt_output)

    def __len__(self):
        return self.num_samples

    def __getitem__(self, idx):
        src, tgt_input = self.data[idx]
        tgt_output = self.targets[idx]
        return (
            torch.LongTensor(src),
            torch.LongTensor(tgt_input),
            torch.LongTensor(tgt_output)
        )


def train_model(model, train_loader, val_loader, device, num_epochs=20, lr=0.0001):
    """è®­ç»ƒæ¨¡å‹"""
    criterion = nn.CrossEntropyLoss(ignore_index=0)  # å¿½ç•¥ padding
    optimizer = torch.optim.Adam(model.parameters(), lr=lr)

    # æ·»åŠ å­¦ä¹ ç‡è°ƒåº¦å™¨
    scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(
        optimizer, mode='min', factor=0.5, patience=5, verbose=True
    )

    history = {'train_loss': [], 'train_acc': [], 'val_loss': [], 'val_acc': []}

    # æ—©åœæœºåˆ¶
    best_val_loss = float('inf')
    patience_counter = 0
    patience = 10

    print("\nå¼€å§‹è®­ç»ƒ...")
    for epoch in range(num_epochs):
        # è®­ç»ƒé˜¶æ®µ
        model.train()
        train_loss = 0
        train_correct = 0
        train_total = 0

        for src, tgt_input, tgt_output in train_loader:
            src = src.to(device)
            tgt_input = tgt_input.to(device)
            tgt_output = tgt_output.to(device)

            optimizer.zero_grad()

            # å‰å‘ä¼ æ’­
            output = model(src, tgt_input)  # (batch, tgt_seq_len, vocab_size)

            # è®¡ç®—æŸå¤±
            output_flat = output.view(-1, output.size(-1))
            tgt_flat = tgt_output.view(-1)
            loss = criterion(output_flat, tgt_flat)

            # åå‘ä¼ æ’­
            loss.backward()
            optimizer.step()

            # ç»Ÿè®¡
            train_loss += loss.item()
            _, predicted = output.max(-1)
            mask = tgt_output != 0  # å¿½ç•¥ padding
            train_correct += (predicted == tgt_output).masked_select(mask).sum().item()
            train_total += mask.sum().item()

        train_loss /= len(train_loader)
        train_acc = 100. * train_correct / train_total

        # éªŒè¯é˜¶æ®µ
        model.eval()
        val_loss = 0
        val_correct = 0
        val_total = 0

        with torch.no_grad():
            for src, tgt_input, tgt_output in val_loader:
                src = src.to(device)
                tgt_input = tgt_input.to(device)
                tgt_output = tgt_output.to(device)

                output = model(src, tgt_input)

                output_flat = output.view(-1, output.size(-1))
                tgt_flat = tgt_output.view(-1)
                loss = criterion(output_flat, tgt_flat)

                val_loss += loss.item()
                _, predicted = output.max(-1)
                mask = tgt_output != 0
                val_correct += (predicted == tgt_output).masked_select(mask).sum().item()
                val_total += mask.sum().item()

        val_loss /= len(val_loader)
        val_acc = 100. * val_correct / val_total

        # è®°å½•å†å²
        history['train_loss'].append(train_loss)
        history['train_acc'].append(train_acc)
        history['val_loss'].append(val_loss)
        history['val_acc'].append(val_acc)

        # å­¦ä¹ ç‡è°ƒåº¦
        scheduler.step(val_loss)

        # æ—©åœæ£€æŸ¥
        if val_loss < best_val_loss:
            best_val_loss = val_loss
            patience_counter = 0
            # ä¿å­˜æœ€ä½³æ¨¡å‹
            torch.save(model.state_dict(), 'best_transformer.pth')
        else:
            patience_counter += 1
            if patience_counter >= patience:
                print(f'\næ—©åœè§¦å‘ï¼åœ¨ epoch {epoch+1}')
                # åŠ è½½æœ€ä½³æ¨¡å‹
                model.load_state_dict(torch.load('best_transformer.pth'))
                break

        if (epoch + 1) % 10 == 0:  # æ¯10è½®æ‰“å°ä¸€æ¬¡
            print(f'Epoch {epoch+1}/{num_epochs}:')
            print(f'  Train Loss: {train_loss:.4f}, Train Acc: {train_acc:.2f}%')
            print(f'  Val Loss: {val_loss:.4f}, Val Acc: {val_acc:.2f}%')
            print(f'  å½“å‰å­¦ä¹ ç‡: {optimizer.param_groups[0]["lr"]:.6f}')

    return history


def test_inference(model, dataset, device, num_examples=5):
    """æµ‹è¯•æ¨ç†ï¼ˆé€è¯ç”Ÿæˆï¼‰"""
    model.eval()

    print("\n" + "=" * 60)
    print("æ¨ç†æµ‹è¯•ï¼ˆåºåˆ—åè½¬ï¼‰")
    print("=" * 60)

    BOS_IDX = dataset.BOS_IDX
    EOS_IDX = dataset.EOS_IDX
    max_len = dataset.seq_len + 2

    for i in range(num_examples):
        src, _, tgt_output = dataset[i]
        src = src.unsqueeze(0).to(device)  # (1, seq_len)

        # ç¼–ç æºåºåˆ—
        encoder_output = model.encoder(src)

        # åˆå§‹åŒ–è§£ç å™¨è¾“å…¥ï¼ˆåªæœ‰ BOSï¼‰
        tgt = torch.LongTensor([[BOS_IDX]]).to(device)

        # é€è¯ç”Ÿæˆ
        for _ in range(max_len):
            tgt_mask = model.create_look_ahead_mask(tgt.size(1)).to(device)
            tgt_mask = tgt_mask.unsqueeze(0).unsqueeze(0)

            output = model.decoder(tgt, encoder_output, tgt_mask=tgt_mask)
            next_token = output[:, -1, :].argmax(-1)  # å–æœ€åä¸€ä¸ªä½ç½®çš„é¢„æµ‹

            tgt = torch.cat([tgt, next_token.unsqueeze(0)], dim=1)

            if next_token.item() == EOS_IDX:
                break

        # æ‰“å°ç»“æœ
        src_seq = src.squeeze().cpu().numpy()
        pred_seq = tgt.squeeze().cpu().numpy()[1:-1]  # å»æ‰ BOS å’Œ EOS
        target_seq = tgt_output.numpy()

        print(f"\nä¾‹å­ {i+1}:")
        print(f"  è¾“å…¥: {src_seq}")
        print(f"  é¢„æµ‹: {pred_seq}")
        print(f"  ç›®æ ‡: {target_seq[:-1]}")  # å»æ‰ EOS
        print(f"  æ­£ç¡®: {'âœ“' if np.array_equal(pred_seq, target_seq[:-1]) else 'âœ—'}")


def plot_training_history(history, save_path='transformer_training_history.png'):
    """ç»˜åˆ¶è®­ç»ƒå†å²"""
    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 4))

    ax1.plot(history['train_loss'], label='Train Loss')
    ax1.plot(history['val_loss'], label='Val Loss')
    ax1.set_xlabel('Epoch')
    ax1.set_ylabel('Loss')
    ax1.set_title('Training and Validation Loss')
    ax1.legend()
    ax1.grid(True)

    ax2.plot(history['train_acc'], label='Train Acc')
    ax2.plot(history['val_acc'], label='Val Acc')
    ax2.set_xlabel('Epoch')
    ax2.set_ylabel('Accuracy (%)')
    ax2.set_title('Training and Validation Accuracy')
    ax2.legend()
    ax2.grid(True)

    plt.tight_layout()
    plt.savefig(save_path, dpi=150, bbox_inches='tight')
    print(f"\nè®­ç»ƒå†å²å·²ä¿å­˜åˆ° {save_path}")
    plt.close()


def visualize_look_ahead_mask():
    """å¯è§†åŒ– look-ahead mask"""
    seq_len = 8
    mask = torch.triu(torch.ones(seq_len, seq_len), diagonal=1).bool()
    mask = (~mask).float().numpy()  # åè½¬å¹¶è½¬æ¢ä¸ºæµ®ç‚¹æ•°

    fig, ax = plt.subplots(figsize=(8, 6))
    im = ax.imshow(mask, cmap='RdYlGn', aspect='auto', vmin=0, vmax=1)

    ax.set_xticks(range(seq_len))
    ax.set_yticks(range(seq_len))
    ax.set_xlabel('Key Position')
    ax.set_ylabel('Query Position')
    ax.set_title('Look-Ahead Mask (Decoder)\nGreen=Allowed, Red=Masked')

    for i in range(seq_len):
        for j in range(seq_len):
            text = 'âœ“' if mask[i, j] == 1 else 'âœ—'
            color = 'black' if mask[i, j] == 1 else 'white'
            ax.text(j, i, text, ha="center", va="center", color=color, fontsize=12)

    plt.colorbar(im, ax=ax)
    plt.tight_layout()
    plt.savefig('transformer_look_ahead_mask.png', dpi=150, bbox_inches='tight')
    print("Look-Ahead Mask visualization saved to transformer_look_ahead_mask.png")
    plt.close()


def main():
    print("\n" + "ğŸš€ " + "=" * 58)
    print("  Transformer æ¶æ„ - PyTorchå®ç°")
    print("  å®æˆ˜ï¼šåºåˆ—åˆ°åºåˆ—ï¼ˆSeq2Seqï¼‰ä»»åŠ¡")
    print("=" * 60)

    # æ£€æŸ¥GPU
    print(f"\nä½¿ç”¨è®¾å¤‡: {DEVICE}")
    if torch.cuda.is_available():
        print(f"GPUå‹å·: {torch.cuda.get_device_name(0)}")

    # åˆ›å»ºæ•°æ®é›†
    print("\n" + "=" * 60)
    print("åˆ›å»ºæ•°æ®é›†ï¼ˆåºåˆ—åè½¬ä»»åŠ¡ï¼‰")
    print("=" * 60)

    vocab_size = 30  # é™ä½è¯æ±‡è¡¨å¤§å°ï¼Œæ›´å®¹æ˜“å­¦ä¹ 
    seq_len = 6  # é™ä½åºåˆ—é•¿åº¦ï¼Œæ›´å®¹æ˜“å­¦ä¹ 

    train_dataset = Seq2SeqDataset(num_samples=10000, seq_len=seq_len, vocab_size=vocab_size)  # å¤§å¹…å¢åŠ æ•°æ®
    val_dataset = Seq2SeqDataset(num_samples=2000, seq_len=seq_len, vocab_size=vocab_size)

    train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)
    val_loader = DataLoader(val_dataset, batch_size=64, shuffle=False)

    print(f"è®­ç»ƒé›†å¤§å°: {len(train_dataset)}")
    print(f"éªŒè¯é›†å¤§å°: {len(val_dataset)}")
    print(f"ä»»åŠ¡: åºåˆ—åè½¬ï¼ˆå¦‚ [5,8,3] -> [3,8,5]ï¼‰")

    # åˆ›å»ºæ¨¡å‹
    print("\n" + "=" * 60)
    print("åˆ›å»º Transformer æ¨¡å‹")
    print("=" * 60)

    model = Transformer(
        src_vocab_size=vocab_size,
        tgt_vocab_size=vocab_size,
        d_model=128,  # é™ä½æ¨¡å‹å®¹é‡ï¼Œé˜²æ­¢è¿‡æ‹Ÿåˆ
        num_heads=4,  # å‡å°‘æ³¨æ„åŠ›å¤´æ•°
        num_layers=2,  # å‡å°‘å±‚æ•°
        d_ff=512,  # å‡å°‘FFNç»´åº¦
        max_len=seq_len + 2,
        dropout=0.2  # å¢åŠ dropoutï¼Œé˜²æ­¢è¿‡æ‹Ÿåˆ
    ).to(DEVICE)

    print(f"æ¨¡å‹å‚æ•°é‡: {sum(p.numel() for p in model.parameters()):,}")

    # å¯è§†åŒ– look-ahead mask
    print("\n" + "=" * 60)
    print("å¯è§†åŒ– Look-Ahead Mask")
    print("=" * 60)
    visualize_look_ahead_mask()

    # è®­ç»ƒæ¨¡å‹
    print("\n" + "=" * 60)
    print("è®­ç»ƒæ¨¡å‹")
    print("=" * 60)

    history = train_model(model, train_loader, val_loader, DEVICE, num_epochs=100, lr=0.001)  # æ›´å¤šè½®æ¬¡ï¼Œä½†æœ‰æ—©åœ

    # ç»˜åˆ¶è®­ç»ƒå†å²
    plot_training_history(history)

    # æµ‹è¯•æ¨ç†
    test_inference(model, val_dataset, DEVICE, num_examples=10)  # æµ‹è¯•æ›´å¤šä¾‹å­

    print("\n" + "=" * 60)
    print("å­¦ä¹ æ€»ç»“")
    print("=" * 60)

    print("""
1. Transformer æ ¸å¿ƒç»„ä»¶
   âœ“ Encoder: ç¼–ç æºåºåˆ—
   âœ“ Decoder: ç”Ÿæˆç›®æ ‡åºåˆ—ï¼ˆè‡ªå›å½’ï¼‰
   âœ“ Cross-Attention: Decoder å…³æ³¨ Encoder è¾“å‡º

2. Decoder çš„å…³é”®è®¾è®¡
   âœ“ Masked Self-Attention: é˜²æ­¢"ä½œå¼Š"ï¼ˆçœ‹åˆ°æœªæ¥ä¿¡æ¯ï¼‰
   âœ“ Look-Ahead Mask: ä¸‹ä¸‰è§’çŸ©é˜µæ©ç 
   âœ“ æ¨ç†æ—¶é€è¯ç”Ÿæˆï¼ˆAuto-regressiveï¼‰

3. è®­ç»ƒ vs æ¨ç†çš„åŒºåˆ«
   âœ“ è®­ç»ƒ: å¹¶è¡Œè®¡ç®—ï¼ˆTeacher Forcingï¼Œç»™å®Œæ•´ç›®æ ‡åºåˆ—ï¼‰
   âœ“ æ¨ç†: é€è¯ç”Ÿæˆï¼ˆä» BOS å¼€å§‹ï¼Œç›´åˆ° EOSï¼‰

4. åº”ç”¨åœºæ™¯
   âœ“ æœºå™¨ç¿»è¯‘: è‹±æ–‡ â†’ ä¸­æ–‡
   âœ“ æ–‡æœ¬æ‘˜è¦: é•¿æ–‡æœ¬ â†’ æ‘˜è¦
   âœ“ å¯¹è¯ç³»ç»Ÿ: é—®é¢˜ â†’ å›ç­”
   âœ“ ä»£ç ç”Ÿæˆ: æè¿° â†’ ä»£ç 

5. ä¸ Attention çš„å…³ç³»
   âœ“ Attention æ˜¯ Transformer çš„æ ¸å¿ƒç»„ä»¶
   âœ“ Transformer = Encoder + Decoder + Cross-Attention
   âœ“ BERT åªç”¨ Encoderï¼ŒGPT åªç”¨ Decoder

6. ä¸‹ä¸€æ­¥
   â†’ é¢„è®­ç»ƒæ¨¡å‹ï¼ˆBERTã€GPTï¼‰
   â†’ å®é™… NLP ä»»åŠ¡ï¼ˆåˆ†ç±»ã€ç¿»è¯‘ã€æ‘˜è¦ï¼‰
   â†’ æ¨èç³»ç»Ÿä¸­çš„åŒå¡”æ¨¡å‹
    """)

    print("\nâœ… è®­ç»ƒå®Œæˆï¼")
    print("\næç¤º: Transformer æ˜¯ç°ä»£ NLP/æ¨èç³»ç»Ÿçš„åŸºç¡€")
    print("      æ¥ä¸‹æ¥å¯ä»¥å­¦ä¹ å¦‚ä½•åº”ç”¨é¢„è®­ç»ƒæ¨¡å‹æˆ–å¼€å§‹æ¨èç³»ç»Ÿå®è·µ")


if __name__ == "__main__":
    main()
