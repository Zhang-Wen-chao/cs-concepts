"""
Embedding æŠ€æœ¯ - PyTorch å®ç°

å¯¹æ¯” NumPy ç‰ˆæœ¬ï¼š
- NumPy: æ‰‹å†™çŸ©é˜µæŸ¥æ‰¾ï¼Œç†è§£embeddingåŸç†
- PyTorch: ä½¿ç”¨ nn.Embeddingï¼ŒGPUåŠ é€Ÿï¼Œå·¥ä¸šå®è·µ

æœ¬æ–‡ä»¶å†…å®¹ï¼š
1. PyTorch Embedding åŸºç¡€ç»„ä»¶
2. Word2Vec (Skip-gram) PyTorch å®ç°
3. æ¨èç³»ç»Ÿä¸­çš„ Item Embedding
4. GPU è®­ç»ƒåŠ é€Ÿ
5. å¯è§†åŒ–ä¸å¯¹æ¯”
"""

import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import Dataset, DataLoader
import numpy as np
import matplotlib.pyplot as plt
from sklearn.decomposition import PCA
import time


# ==================== 1. PyTorch Embedding åŸºç¡€ç»„ä»¶ ====================
def demo_pytorch_embedding():
    """
    æ¼”ç¤º PyTorch çš„ Embedding æ“ä½œ

    ====================================================================
    ğŸ”‘ PyTorch vs NumPy Embedding
    ====================================================================

    NumPy ç‰ˆæœ¬ï¼ˆæ‰‹åŠ¨æŸ¥æ‰¾ï¼‰ï¼š
    ```python
    def lookup(word_idx):
        return embedding_matrix[word_idx]  # æ‰‹åŠ¨ç´¢å¼•
    ```

    PyTorch ç‰ˆæœ¬ï¼ˆä¸€è¡Œï¼‰ï¼š
    ```python
    embedding = nn.Embedding(vocab_size, embedding_dim)
    output = embedding(word_idx)  # è‡ªåŠ¨æŸ¥æ‰¾ + æ¢¯åº¦
    ```

    PyTorch å¸®ä½ åšäº†ä»€ä¹ˆï¼Ÿ
    - è‡ªåŠ¨æ‰¹é‡æŸ¥æ‰¾
    - è‡ªåŠ¨GPUåŠ é€Ÿ
    - è‡ªåŠ¨è®¡ç®—æ¢¯åº¦ï¼ˆå¯å­¦ä¹ ï¼‰
    - é«˜æ•ˆå†…å­˜ç®¡ç†

    ====================================================================
    """
    print("=" * 70)
    print("1. PyTorch Embedding æ“ä½œæ¼”ç¤º")
    print("=" * 70)

    # åˆ›å»º Embedding å±‚
    vocab_size = 10  # è¯æ±‡è¡¨å¤§å°
    embedding_dim = 5  # åµŒå…¥ç»´åº¦

    embedding = nn.Embedding(num_embeddings=vocab_size, embedding_dim=embedding_dim)

    print(f"\nEmbeddingå±‚:")
    print(f"  è¯æ±‡è¡¨å¤§å°: {vocab_size}")
    print(f"  åµŒå…¥ç»´åº¦: {embedding_dim}")
    print(f"  å‚æ•°é‡: {vocab_size * embedding_dim}")

    # æŸ¥æ‰¾å•ä¸ªè¯çš„åµŒå…¥
    word_idx = torch.LongTensor([3])  # è¯ç´¢å¼•
    word_embedding = embedding(word_idx)

    print(f"\nå•è¯ç´¢å¼• {word_idx.item()} çš„åµŒå…¥:")
    print(f"  Shape: {word_embedding.shape}")  # (1, embedding_dim)
    print(f"  å€¼: {word_embedding}")

    # æ‰¹é‡æŸ¥æ‰¾
    batch_indices = torch.LongTensor([0, 1, 2, 3, 4])
    batch_embeddings = embedding(batch_indices)

    print(f"\næ‰¹é‡æŸ¥æ‰¾ (batch_size={len(batch_indices)}):")
    print(f"  è¾“å…¥ç´¢å¼•: {batch_indices}")
    print(f"  è¾“å‡º shape: {batch_embeddings.shape}")  # (batch_size, embedding_dim)

    # åºåˆ—æŸ¥æ‰¾ï¼ˆç”¨äº NLPï¼‰
    sequence = torch.LongTensor([[1, 2, 3, 4],   # å¥å­1
                                 [5, 6, 7, 8]])   # å¥å­2
    sequence_embeddings = embedding(sequence)

    print(f"\nåºåˆ—æŸ¥æ‰¾:")
    print(f"  è¾“å…¥ shape: {sequence.shape}")  # (batch_size, seq_len)
    print(f"  è¾“å‡º shape: {sequence_embeddings.shape}")  # (batch_size, seq_len, embedding_dim)

    print("\nğŸ’¡ PyTorch Embedding ä¼˜åŠ¿:")
    print("  - è‡ªåŠ¨æ‰¹é‡å¤„ç†")
    print("  - å¯å­¦ä¹ å‚æ•°ï¼ˆé€šè¿‡åå‘ä¼ æ’­ï¼‰")
    print("  - GPU åŠ é€Ÿ")
    print("  - ä¸å…¶ä»–å±‚æ— ç¼é›†æˆ")

    # é¢„è®­ç»ƒåµŒå…¥åŠ è½½
    print("\n" + "-" * 70)
    print("ä»é¢„è®­ç»ƒæƒé‡åŠ è½½ Embedding")
    print("-" * 70)

    # å‡è®¾æˆ‘ä»¬æœ‰é¢„è®­ç»ƒçš„embeddingçŸ©é˜µ
    pretrained_embeddings = torch.randn(vocab_size, embedding_dim)

    # åˆ›å»º Embedding å±‚å¹¶åŠ è½½é¢„è®­ç»ƒæƒé‡
    embedding_pretrained = nn.Embedding.from_pretrained(
        pretrained_embeddings,
        freeze=False  # freeze=True è¡¨ç¤ºä¸æ›´æ–°ï¼Œfreeze=False è¡¨ç¤ºå¾®è°ƒ
    )

    print(f"åŠ è½½é¢„è®­ç»ƒ Embedding:")
    print(f"  freeze=False: å¯ä»¥å¾®è°ƒ")
    print(f"  freeze=True:  æƒé‡å›ºå®š")


# ==================== 2. Word2Vec (Skip-gram) PyTorch å®ç° ====================
class Word2VecDataset(Dataset):
    """Word2Vecè®­ç»ƒæ•°æ®é›†"""

    def __init__(self, corpus, window_size=2):
        """
        å‚æ•°:
            corpus: å¥å­åˆ—è¡¨ï¼Œæ¯ä¸ªå¥å­æ˜¯ä¸€ä¸ªå­—ç¬¦ä¸²
            window_size: ä¸Šä¸‹æ–‡çª—å£å¤§å°
        """
        self.window_size = window_size

        # æ„å»ºè¯æ±‡è¡¨
        words = []
        for sent in corpus:
            words.extend(sent.split())

        vocab = sorted(set(words))
        self.word2idx = {word: idx for idx, word in enumerate(vocab)}
        self.idx2word = {idx: word for word, idx in self.word2idx.items()}
        self.vocab_size = len(vocab)

        # ç”Ÿæˆè®­ç»ƒå¯¹ (ä¸­å¿ƒè¯, ä¸Šä¸‹æ–‡è¯)
        self.pairs = []
        for sent in corpus:
            word_indices = [self.word2idx[w] for w in sent.split()]

            for i, center_idx in enumerate(word_indices):
                # è·å–ä¸Šä¸‹æ–‡è¯ç´¢å¼•
                context_start = max(0, i - window_size)
                context_end = min(len(word_indices), i + window_size + 1)

                for j in range(context_start, context_end):
                    if j != i:
                        self.pairs.append((center_idx, word_indices[j]))

    def __len__(self):
        return len(self.pairs)

    def __getitem__(self, idx):
        center, context = self.pairs[idx]
        return torch.LongTensor([center]), torch.LongTensor([context])


class Word2VecModel(nn.Module):
    """
    Word2Vec Skip-gram æ¨¡å‹

    ====================================================================
    ğŸ”‘ PyTorch Word2Vec ç»“æ„
    ====================================================================

    æ¶æ„ï¼š
        Center word (ID) â†’ Embedding Layer â†’ Hidden Vector
                                                â†“
        Context word (ID) â† Softmax â† Linear â† Hidden Vector

    å…·ä½“ï¼š
        center_idx (1,) â†’ Embedding(vocab_size, embed_dim) â†’ (embed_dim,)
                                                               â†“
        context_prob (vocab_size,) â† Softmax â† Linear(embed_dim, vocab_size)

    Training:
        - ç»™å®šä¸­å¿ƒè¯ï¼Œé¢„æµ‹ä¸Šä¸‹æ–‡è¯
        - æœ€å¤§åŒ– P(context | center)
        - Cross-entropy Loss
    """

    def __init__(self, vocab_size, embedding_dim):
        super(Word2VecModel, self).__init__()

        # è¾“å…¥åµŒå…¥ï¼ˆä¸­å¿ƒè¯ï¼‰
        self.embeddings = nn.Embedding(vocab_size, embedding_dim)

        # è¾“å‡ºå±‚ï¼ˆé¢„æµ‹ä¸Šä¸‹æ–‡è¯ï¼‰
        self.linear = nn.Linear(embedding_dim, vocab_size)

    def forward(self, center_words):
        """
        center_words: (batch_size,)
        è¿”å›: (batch_size, vocab_size) - æ¯ä¸ªä¸­å¿ƒè¯çš„ä¸Šä¸‹æ–‡è¯æ¦‚ç‡åˆ†å¸ƒ
        """
        # æŸ¥æ‰¾åµŒå…¥
        embeds = self.embeddings(center_words)  # (batch_size, embedding_dim)

        # é¢„æµ‹ä¸Šä¸‹æ–‡
        scores = self.linear(embeds)  # (batch_size, vocab_size)

        return scores

    def get_embedding(self, word_idx):
        """è·å–è¯çš„åµŒå…¥å‘é‡"""
        with torch.no_grad():
            return self.embeddings(torch.LongTensor([word_idx])).squeeze()


def train_word2vec():
    """
    è®­ç»ƒ Word2Vec æ¨¡å‹

    ====================================================================
    ğŸ”‘ PyTorch Word2Vec è®­ç»ƒæµç¨‹
    ====================================================================

    1. å‡†å¤‡æ•°æ®
       - æ„å»ºè¯æ±‡è¡¨
       - ç”Ÿæˆ (ä¸­å¿ƒè¯, ä¸Šä¸‹æ–‡è¯) å¯¹

    2. å®šä¹‰æ¨¡å‹
       - Embedding å±‚ï¼ˆå­¦ä¹ è¯å‘é‡ï¼‰
       - Linear å±‚ï¼ˆé¢„æµ‹ä¸Šä¸‹æ–‡ï¼‰

    3. è®­ç»ƒ
       - è¾“å…¥: ä¸­å¿ƒè¯
       - è¾“å‡º: ä¸Šä¸‹æ–‡è¯æ¦‚ç‡åˆ†å¸ƒ
       - æŸå¤±: CrossEntropyLoss

    4. æå–åµŒå…¥
       - è®­ç»ƒåçš„ Embedding å±‚å°±æ˜¯è¯å‘é‡

    ====================================================================
    """
    print("\n" + "=" * 70)
    print("2. è®­ç»ƒ Word2Vec (Skip-gram) æ¨¡å‹")
    print("=" * 70)

    # ========== 1. å‡†å¤‡æ•°æ® ==========
    corpus = [
        "cat likes fish",
        "dog likes bone",
        "cat likes milk",
        "dog likes meat",
        "bird likes seeds",
        "cat and dog are pets",
        "fish and bone are food",
        "cat eats fish daily",
        "dog eats bone daily",
    ]

    print(f"\nCorpus ({len(corpus)} sentences):")
    for i, sent in enumerate(corpus, 1):
        print(f"  {i}. {sent}")

    dataset = Word2VecDataset(corpus, window_size=2)

    print(f"\nè¯æ±‡è¡¨å¤§å°: {dataset.vocab_size}")
    print(f"è®­ç»ƒå¯¹æ•°é‡: {len(dataset)}")
    print(f"è¯æ±‡è¡¨: {sorted(dataset.word2idx.keys())}")

    # DataLoader
    batch_size = 8
    train_loader = DataLoader(dataset, batch_size=batch_size, shuffle=True)

    # ========== 2. åˆ›å»ºæ¨¡å‹ ==========
    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
    print(f"\nä½¿ç”¨è®¾å¤‡: {device}")

    vocab_size = dataset.vocab_size
    embedding_dim = 10

    model = Word2VecModel(vocab_size, embedding_dim).to(device)

    print(f"\næ¨¡å‹ç»“æ„:")
    print(model)
    print(f"å‚æ•°é‡: {sum(p.numel() for p in model.parameters()):,}")

    # ========== 3. è®­ç»ƒ ==========
    criterion = nn.CrossEntropyLoss()
    optimizer = optim.Adam(model.parameters(), lr=0.01)

    n_epochs = 100

    print(f"\nå¼€å§‹è®­ç»ƒ...")
    print(f"  Epochs: {n_epochs}")
    print(f"  Batch size: {batch_size}")

    model.train()
    losses = []

    for epoch in range(n_epochs):
        total_loss = 0

        for center, context in train_loader:
            center = center.squeeze().to(device)
            context = context.squeeze().to(device)

            # å‰å‘ä¼ æ’­
            scores = model(center)

            # è®¡ç®—æŸå¤±
            loss = criterion(scores, context)

            # åå‘ä¼ æ’­
            optimizer.zero_grad()
            loss.backward()
            optimizer.step()

            total_loss += loss.item()

        avg_loss = total_loss / len(train_loader)
        losses.append(avg_loss)

        if epoch % 20 == 0:
            print(f"Epoch {epoch:3d}/{n_epochs} | Loss: {avg_loss:.4f}")

    print(f"\nè®­ç»ƒå®Œæˆï¼")

    # ========== 4. æµ‹è¯•è¯ç›¸ä¼¼åº¦ ==========
    print("\n" + "=" * 70)
    print("è¯ç›¸ä¼¼åº¦æµ‹è¯•")
    print("=" * 70)

    model.eval()

    # è·å–æ‰€æœ‰è¯çš„åµŒå…¥
    all_embeddings = model.embeddings.weight.detach().cpu().numpy()

    def get_most_similar(word, top_k=3):
        """æ‰¾æœ€ç›¸ä¼¼çš„è¯"""
        if word not in dataset.word2idx:
            return []

        word_idx = dataset.word2idx[word]
        word_emb = all_embeddings[word_idx]

        # è®¡ç®—ä½™å¼¦ç›¸ä¼¼åº¦
        word_emb_norm = word_emb / (np.linalg.norm(word_emb) + 1e-10)
        all_emb_norm = all_embeddings / (np.linalg.norm(all_embeddings, axis=1, keepdims=True) + 1e-10)
        similarities = np.dot(all_emb_norm, word_emb_norm)

        # æ’é™¤è‡ªå·±
        similarities[word_idx] = -np.inf
        top_indices = np.argsort(similarities)[::-1][:top_k]

        return [(dataset.idx2word[idx], similarities[idx]) for idx in top_indices]

    # æµ‹è¯•
    test_words = ["cat", "dog", "fish", "likes"]
    for word in test_words:
        if word in dataset.word2idx:
            similar = get_most_similar(word, top_k=3)
            print(f"\n'{word}' æœ€ç›¸ä¼¼çš„è¯:")
            for sim_word, sim_score in similar:
                print(f"  {sim_word:10s}: {sim_score:.4f}")

    # ========== 5. å¯è§†åŒ– ==========
    visualize_word2vec(dataset, all_embeddings, losses, n_epochs)

    return model, dataset


def visualize_word2vec(dataset, embeddings, losses, n_epochs):
    """å¯è§†åŒ– Word2Vec ç»“æœ"""
    print("\nå¯è§†åŒ– Word2Vec...")

    fig = plt.figure(figsize=(16, 6))

    # 1. è®­ç»ƒæŸå¤±
    ax1 = fig.add_subplot(131)
    epochs_range = range(len(losses))
    ax1.plot(epochs_range, losses, 'b-', linewidth=2)
    ax1.set_xlabel('Epoch', fontsize=11)
    ax1.set_ylabel('Loss', fontsize=11)
    ax1.set_title('Word2Vec Training Loss', fontsize=12, fontweight='bold')
    ax1.grid(alpha=0.3)

    # 2. è¯åµŒå…¥ 2D æŠ•å½±ï¼ˆPCAï¼‰
    ax2 = fig.add_subplot(132)

    pca = PCA(n_components=2)
    embeddings_2d = pca.fit_transform(embeddings)

    # åˆ†ç±»ç€è‰²
    categories = {
        'animals': ['cat', 'dog', 'bird'],
        'food': ['fish', 'bone', 'milk', 'meat', 'seeds'],
        'verbs': ['likes', 'eats', 'are'],
        'other': ['and', 'daily', 'pets', 'food']
    }

    colors_map = {'animals': 'red', 'food': 'green', 'verbs': 'blue', 'other': 'gray'}

    for category, words_in_cat in categories.items():
        indices = [dataset.word2idx[w] for w in words_in_cat if w in dataset.word2idx]
        if indices:
            x = embeddings_2d[indices, 0]
            y = embeddings_2d[indices, 1]
            ax2.scatter(x, y, c=colors_map[category], s=200, alpha=0.6, label=category.capitalize())

    # æ·»åŠ è¯æ ‡ç­¾
    for word, idx in dataset.word2idx.items():
        x, y = embeddings_2d[idx]
        ax2.annotate(word, (x, y), fontsize=9, fontweight='bold',
                    ha='center', va='center')

    ax2.set_xlabel('PCA Component 1', fontsize=11)
    ax2.set_ylabel('PCA Component 2', fontsize=11)
    ax2.set_title('Word Embeddings (2D Projection)\nSimilar words cluster together',
                 fontsize=12, fontweight='bold')
    ax2.legend(fontsize=9)
    ax2.grid(alpha=0.3)

    # 3. ç›¸ä¼¼åº¦çŸ©é˜µï¼ˆéƒ¨åˆ†è¯ï¼‰
    ax3 = fig.add_subplot(133)

    # é€‰æ‹©ä¸€äº›è¯å±•ç¤º
    selected_words = ['cat', 'dog', 'fish', 'bone', 'likes', 'eats']
    selected_indices = [dataset.word2idx[w] for w in selected_words if w in dataset.word2idx]

    if selected_indices:
        selected_embeddings = embeddings[selected_indices]

        # å½’ä¸€åŒ–
        selected_emb_norm = selected_embeddings / np.linalg.norm(selected_embeddings, axis=1, keepdims=True)

        # è®¡ç®—ç›¸ä¼¼åº¦çŸ©é˜µ
        similarity_matrix = np.dot(selected_emb_norm, selected_emb_norm.T)

        im = ax3.imshow(similarity_matrix, cmap='RdYlGn', vmin=-1, vmax=1)
        ax3.set_xticks(range(len(selected_words)))
        ax3.set_yticks(range(len(selected_words)))
        ax3.set_xticklabels(selected_words, rotation=45, ha='right')
        ax3.set_yticklabels(selected_words)
        ax3.set_title('Word Similarity Matrix\nGreen = Similar',
                     fontsize=12, fontweight='bold')

        # æ·»åŠ æ•°å€¼
        for i in range(len(selected_words)):
            for j in range(len(selected_words)):
                text = ax3.text(j, i, f'{similarity_matrix[i, j]:.2f}',
                              ha="center", va="center", color="black", fontsize=9)

        plt.colorbar(im, ax=ax3, label='Cosine Similarity')

    plt.tight_layout()
    plt.savefig('word2vec_pytorch.png', dpi=100, bbox_inches='tight')
    print("ğŸ“Š Word2Vec å¯è§†åŒ–å·²ä¿å­˜: word2vec_pytorch.png")
    plt.close()


# ==================== 3. æ¨èç³»ç»Ÿä¸­çš„ Item Embedding ====================
class MatrixFactorizationDataset(Dataset):
    """çŸ©é˜µåˆ†è§£æ•°æ®é›†"""

    def __init__(self, user_item_matrix):
        """
        user_item_matrix: (n_users, n_items) numpy array
        """
        self.interactions = []

        # åªä¿ç•™æ­£æ ·æœ¬ï¼ˆæœ‰äº¤äº’çš„ï¼‰
        users, items = np.where(user_item_matrix > 0)
        for u, i in zip(users, items):
            rating = user_item_matrix[u, i]
            self.interactions.append((u, i, rating))

    def __len__(self):
        return len(self.interactions)

    def __getitem__(self, idx):
        user, item, rating = self.interactions[idx]
        return torch.LongTensor([user]), torch.LongTensor([item]), torch.FloatTensor([rating])


class MatrixFactorization(nn.Module):
    """
    çŸ©é˜µåˆ†è§£æ¨¡å‹ï¼ˆç”¨äºæ¨èç³»ç»Ÿï¼‰

    ====================================================================
    ğŸ”‘ çŸ©é˜µåˆ†è§£ = å­¦ä¹  User & Item Embeddings
    ====================================================================

    ç›®æ ‡ï¼š
        R â‰ˆ U Ã— I^T
        å…¶ä¸­ R: user-item è¯„åˆ†çŸ©é˜µ
            U: user embeddings
            I: item embeddings

    é¢„æµ‹ï¼š
        rating(user, item) = user_embedding Â· item_embedding

    æ¶æ„ï¼š
        User ID â†’ User Embedding (embed_dim)
                       â†“
        Item ID â†’ Item Embedding (embed_dim)
                       â†“
                  Dot Product â†’ Predicted Rating

    åº”ç”¨ï¼š
        - ååŒè¿‡æ»¤
        - æ¨èç³»ç»Ÿ
        - Two-Tower æ¨¡å‹çš„åŸºç¡€
    """

    def __init__(self, n_users, n_items, embedding_dim):
        super(MatrixFactorization, self).__init__()

        # User embeddings
        self.user_embeddings = nn.Embedding(n_users, embedding_dim)

        # Item embeddings
        self.item_embeddings = nn.Embedding(n_items, embedding_dim)

        # åˆå§‹åŒ–ï¼ˆå°éšæœºå€¼ï¼‰
        self.user_embeddings.weight.data.uniform_(-0.05, 0.05)
        self.item_embeddings.weight.data.uniform_(-0.05, 0.05)

    def forward(self, user_ids, item_ids):
        """
        user_ids: (batch_size,)
        item_ids: (batch_size,)
        è¿”å›: (batch_size,) - é¢„æµ‹çš„è¯„åˆ†
        """
        # æŸ¥æ‰¾åµŒå…¥
        user_embeds = self.user_embeddings(user_ids)  # (batch_size, embed_dim)
        item_embeds = self.item_embeddings(item_ids)  # (batch_size, embed_dim)

        # ç‚¹ç§¯ï¼ˆé€å…ƒç´ ç›¸ä¹˜åæ±‚å’Œï¼‰
        predictions = (user_embeds * item_embeds).sum(dim=1)  # (batch_size,)

        return predictions


def train_matrix_factorization():
    """
    è®­ç»ƒçŸ©é˜µåˆ†è§£æ¨¡å‹ï¼ˆæ¨èç³»ç»Ÿï¼‰

    ====================================================================
    ğŸ”‘ æ¨èç³»ç»Ÿä¸­çš„ Embedding
    ====================================================================

    æ ¸å¿ƒæ€æƒ³ï¼š
        - å°† User å’Œ Item éƒ½æ˜ å°„åˆ°åŒä¸€ä¸ªåµŒå…¥ç©ºé—´
        - ç›¸ä¼¼çš„ User â†’ ç›¸ä¼¼çš„å‘é‡
        - ç›¸ä¼¼çš„ Item â†’ ç›¸ä¼¼çš„å‘é‡
        - é¢„æµ‹ = Userå‘é‡ Â· Itemå‘é‡

    è®­ç»ƒï¼š
        - è¾“å…¥: (user_id, item_id)
        - è¾“å‡º: predicted_rating
        - æŸå¤±: MSE(predicted_rating, true_rating)

    ç»“æœï¼š
        - å­¦åˆ°çš„ Item Embeddings å¯ç”¨äºæ¨èç›¸ä¼¼å•†å“
        - å­¦åˆ°çš„ User Embeddings å¯ç”¨äºç”¨æˆ·èšç±»

    ====================================================================
    """
    print("\n" + "=" * 70)
    print("3. è®­ç»ƒçŸ©é˜µåˆ†è§£æ¨¡å‹ï¼ˆæ¨èç³»ç»Ÿï¼‰")
    print("=" * 70)

    # ========== 1. å‡†å¤‡æ•°æ® ==========
    # æ¨¡æ‹Ÿç”¨æˆ·-ç‰©å“äº¤äº’çŸ©é˜µ
    user_item_matrix = np.array([
        [1, 1, 0, 0, 0, 0],  # User 0: likes item 0, 1 (action movies)
        [1, 1, 1, 0, 0, 0],  # User 1: likes item 0, 1, 2
        [0, 0, 0, 1, 1, 0],  # User 2: likes item 3, 4 (romance)
        [0, 0, 0, 1, 1, 1],  # User 3: likes item 3, 4, 5
        [1, 0, 0, 0, 1, 0],  # User 4: mixed preference
    ])

    items = ["Action-1", "Action-2", "Action-3", "Romance-1", "Romance-2", "Romance-3"]
    n_users, n_items = user_item_matrix.shape

    print(f"\nUser-Item äº¤äº’çŸ©é˜µ ({n_users} users, {n_items} items):")
    print(f"{'':10s}", end='')
    for item in items:
        print(f"{item:12s}", end='')
    print()
    for i in range(n_users):
        print(f"User {i:3d}:  ", end='')
        for j in range(n_items):
            print(f"{user_item_matrix[i, j]:12d}", end='')
        print()

    # åˆ›å»ºæ•°æ®é›†
    dataset = MatrixFactorizationDataset(user_item_matrix)
    train_loader = DataLoader(dataset, batch_size=4, shuffle=True)

    print(f"\nè®­ç»ƒæ ·æœ¬æ•°: {len(dataset)} (æ­£æ ·æœ¬)")

    # ========== 2. åˆ›å»ºæ¨¡å‹ ==========
    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
    print(f"\nä½¿ç”¨è®¾å¤‡: {device}")

    embedding_dim = 4
    model = MatrixFactorization(n_users, n_items, embedding_dim).to(device)

    print(f"\næ¨¡å‹ç»“æ„:")
    print(model)
    print(f"User embedding å‚æ•°: {n_users * embedding_dim}")
    print(f"Item embedding å‚æ•°: {n_items * embedding_dim}")
    print(f"æ€»å‚æ•°: {sum(p.numel() for p in model.parameters()):,}")

    # ========== 3. è®­ç»ƒ ==========
    criterion = nn.MSELoss()
    optimizer = optim.Adam(model.parameters(), lr=0.01)

    n_epochs = 200

    print(f"\nå¼€å§‹è®­ç»ƒ...")
    model.train()
    losses = []

    for epoch in range(n_epochs):
        total_loss = 0

        for user_ids, item_ids, ratings in train_loader:
            user_ids = user_ids.squeeze().to(device)
            item_ids = item_ids.squeeze().to(device)
            ratings = ratings.squeeze().to(device)

            # å‰å‘ä¼ æ’­
            predictions = model(user_ids, item_ids)

            # è®¡ç®—æŸå¤±
            loss = criterion(predictions, ratings)

            # åå‘ä¼ æ’­
            optimizer.zero_grad()
            loss.backward()
            optimizer.step()

            total_loss += loss.item()

        avg_loss = total_loss / len(train_loader)
        losses.append(avg_loss)

        if epoch % 40 == 0:
            print(f"Epoch {epoch:3d}/{n_epochs} | Loss: {avg_loss:.6f}")

    print(f"\nè®­ç»ƒå®Œæˆï¼")

    # ========== 4. åˆ†æ Item Embeddings ==========
    print("\n" + "=" * 70)
    print("Item Embeddings åˆ†æ")
    print("=" * 70)

    model.eval()

    # è·å– item embeddings
    item_embeddings = model.item_embeddings.weight.detach().cpu().numpy()

    print(f"\nå­¦åˆ°çš„ Item Embeddings ({embedding_dim}D):")
    for i, item in enumerate(items):
        print(f"  {item:12s}: {item_embeddings[i]}")

    # è®¡ç®— item ç›¸ä¼¼åº¦
    item_emb_norm = item_embeddings / np.linalg.norm(item_embeddings, axis=1, keepdims=True)
    similarity_matrix = np.dot(item_emb_norm, item_emb_norm.T)

    print(f"\nItem ç›¸ä¼¼åº¦çŸ©é˜µ (Cosine Similarity):")
    print(f"{'':12s}", end='')
    for item in items:
        print(f"{item:12s}", end='')
    print()
    for i in range(n_items):
        print(f"{items[i]:12s}", end='')
        for j in range(n_items):
            print(f"{similarity_matrix[i, j]:12.3f}", end='')
        print()

    # ========== 5. å¯è§†åŒ– ==========
    visualize_item_embeddings(items, item_embeddings, similarity_matrix, losses)

    return model


def visualize_item_embeddings(items, item_embeddings, similarity_matrix, losses):
    """å¯è§†åŒ– Item Embeddings"""
    print("\nå¯è§†åŒ– Item Embeddings...")

    fig = plt.figure(figsize=(16, 6))

    # 1. è®­ç»ƒæŸå¤±
    ax1 = fig.add_subplot(131)
    epochs_range = range(len(losses))
    ax1.plot(epochs_range, losses, 'r-', linewidth=2)
    ax1.set_xlabel('Epoch', fontsize=11)
    ax1.set_ylabel('Loss (MSE)', fontsize=11)
    ax1.set_title('Matrix Factorization Training Loss', fontsize=12, fontweight='bold')
    ax1.grid(alpha=0.3)

    # 2. Item Embeddings 2D æŠ•å½±
    ax2 = fig.add_subplot(132)

    pca = PCA(n_components=2)
    item_emb_2d = pca.fit_transform(item_embeddings)

    colors = ['red', 'red', 'red', 'blue', 'blue', 'blue']
    for i, (item, color) in enumerate(zip(items, colors)):
        ax2.scatter(item_emb_2d[i, 0], item_emb_2d[i, 1],
                   c=color, s=300, alpha=0.6)
        ax2.annotate(item, (item_emb_2d[i, 0], item_emb_2d[i, 1]),
                    fontsize=10, fontweight='bold', ha='center', va='center')

    from matplotlib.patches import Patch
    legend_elements = [Patch(facecolor='red', label='Action'),
                      Patch(facecolor='blue', label='Romance')]
    ax2.legend(handles=legend_elements, fontsize=10)

    ax2.set_xlabel('PCA Component 1', fontsize=11)
    ax2.set_ylabel('PCA Component 2', fontsize=11)
    ax2.set_title('Item Embeddings (2D Projection)\nSimilar items cluster',
                 fontsize=12, fontweight='bold')
    ax2.grid(alpha=0.3)

    # 3. ç›¸ä¼¼åº¦çƒ­å›¾
    ax3 = fig.add_subplot(133)

    im = ax3.imshow(similarity_matrix, cmap='RdYlGn', vmin=-1, vmax=1)
    ax3.set_xticks(range(len(items)))
    ax3.set_yticks(range(len(items)))
    ax3.set_xticklabels(items, rotation=45, ha='right')
    ax3.set_yticklabels(items)
    ax3.set_title('Item Similarity Heatmap\nGreen = Similar',
                 fontsize=12, fontweight='bold')

    # æ·»åŠ æ•°å€¼
    for i in range(len(items)):
        for j in range(len(items)):
            text = ax3.text(j, i, f'{similarity_matrix[i, j]:.2f}',
                          ha="center", va="center", color="black", fontsize=9)

    plt.colorbar(im, ax=ax3, label='Cosine Similarity')

    plt.tight_layout()
    plt.savefig('item_embeddings_pytorch.png', dpi=100, bbox_inches='tight')
    print("ğŸ“Š Item Embeddings å¯è§†åŒ–å·²ä¿å­˜: item_embeddings_pytorch.png")
    plt.close()


# ==================== 4. PyTorch vs NumPy å¯¹æ¯” ====================
def compare_pytorch_vs_numpy():
    """
    å¯¹æ¯” PyTorch å’Œ NumPy ç‰ˆæœ¬

    ====================================================================
    ğŸ”‘ PyTorch vs NumPy
    ====================================================================

    NumPy ç‰ˆæœ¬ï¼š
    âœ… ä¼˜ç‚¹ï¼š
      - ç†è§£ Embedding æŸ¥æ‰¾åŸç†
      - æ‰‹å†™æ¢¯åº¦æ›´æ–°
      - ä¸ä¾èµ–æ¡†æ¶

    âŒ ç¼ºç‚¹ï¼š
      - ä»£ç é‡å¤§
      - é€Ÿåº¦æ…¢ï¼ˆæ— GPUï¼‰
      - éš¾ä»¥æ‰©å±•åˆ°å¤§è§„æ¨¡

    PyTorch ç‰ˆæœ¬ï¼š
    âœ… ä¼˜ç‚¹ï¼š
      - ä»£ç ç®€æ´ï¼ˆnn.Embeddingï¼‰
      - GPU åŠ é€Ÿï¼ˆå¿«100å€ï¼‰
      - è‡ªåŠ¨å¾®åˆ†
      - æ˜“äºé›†æˆåˆ°å¤æ‚æ¨¡å‹
      - å·¥ä¸šç•Œæ ‡å‡†

    âŒ ç¼ºç‚¹ï¼š
      - æ¡†æ¶é»‘ç›’
      - éœ€è¦å­¦ä¹  API

    ====================================================================
    """
    print("\n" + "=" * 70)
    print("4. PyTorch vs NumPy å¯¹æ¯”")
    print("=" * 70)

    print("""
æ€§èƒ½å¯¹æ¯”ï¼ˆWord2Vecï¼‰ï¼š

+----------------+------------------+------------------+
|     æŒ‡æ ‡       |   NumPy ç‰ˆæœ¬     |  PyTorch ç‰ˆæœ¬    |
+----------------+------------------+------------------+
| ä»£ç é‡         | ~300 è¡Œ          | ~150 è¡Œ          |
| è®­ç»ƒæ—¶é—´       | ~30 ç§’ (CPU)     | ~5 ç§’ (GPU)      |
| è¯æ±‡è¡¨è§„æ¨¡     | < 10,000         | > 100,000        |
| GPU æ”¯æŒ       | âŒ               | âœ…               |
| è‡ªåŠ¨å¾®åˆ†       | âŒ (æ‰‹å†™)        | âœ…               |
| å¯æ‰©å±•æ€§       | âŒ               | âœ…               |
| å·¥ä¸šåº”ç”¨       | âŒ               | âœ…               |
+----------------+------------------+------------------+

ä»£ç å¯¹æ¯”ï¼š

NumPy ç‰ˆæœ¬ï¼ˆæ‰‹åŠ¨æŸ¥æ‰¾ + æ‰‹å†™æ¢¯åº¦ï¼‰ï¼š
```python
# å‰å‘ä¼ æ’­ï¼šæ‰‹åŠ¨æŸ¥æ‰¾
def forward(self, word_idx):
    hidden = self.W1[word_idx]  # æ‰‹åŠ¨ç´¢å¼•
    scores = np.dot(hidden, self.W2)
    probs = softmax(scores)
    return hidden, probs

# åå‘ä¼ æ’­ï¼šæ‰‹åŠ¨è®¡ç®—æ¢¯åº¦
def backward(self, word_idx, context_idx, hidden, probs):
    d_scores = probs.copy()
    d_scores[context_idx] -= 1
    d_W2 = np.outer(hidden, d_scores)
    self.W2 -= lr * d_W2  # æ‰‹åŠ¨æ›´æ–°
    # ...
```

PyTorch ç‰ˆæœ¬ï¼ˆè‡ªåŠ¨æŸ¥æ‰¾ + è‡ªåŠ¨å¾®åˆ†ï¼‰ï¼š
```python
# å‰å‘ä¼ æ’­ï¼šè‡ªåŠ¨æŸ¥æ‰¾
class Word2Vec(nn.Module):
    def __init__(self):
        self.embeddings = nn.Embedding(vocab_size, embed_dim)
        self.linear = nn.Linear(embed_dim, vocab_size)

    def forward(self, center_words):
        embeds = self.embeddings(center_words)  # è‡ªåŠ¨æŸ¥æ‰¾
        scores = self.linear(embeds)
        return scores

# è®­ç»ƒï¼šè‡ªåŠ¨å¾®åˆ†
output = model(center_words)
loss = criterion(output, context_words)
loss.backward()        # â† è‡ªåŠ¨è®¡ç®—æ¢¯åº¦ï¼
optimizer.step()       # â† è‡ªåŠ¨æ›´æ–°ï¼
```

æ€»ç»“ï¼š
- å­¦ä¹ åŸç† â†’ ç”¨ NumPyï¼ˆç†è§£æŸ¥æ‰¾ + æ¢¯åº¦ï¼‰
- å®é™…åº”ç”¨ â†’ ç”¨ PyTorchï¼ˆå·¥ä¸šæ ‡å‡†ï¼‰
- ä¸¤è€…ç»“åˆ â†’ æœ€ä½³ç†è§£ï¼
    """)


# ==================== 5. ä¸»ç¨‹åº ====================
def main():
    print("=" * 70)
    print("Embedding æŠ€æœ¯ - PyTorch å®ç°")
    print("=" * 70)

    # 1. PyTorch Embedding åŸºç¡€
    demo_pytorch_embedding()

    # 2. Word2Vec è®­ç»ƒ
    word2vec_model, word2vec_dataset = train_word2vec()

    # 3. æ¨èç³»ç»Ÿ Item Embedding
    mf_model = train_matrix_factorization()

    # 4. å¯¹æ¯” PyTorch vs NumPy
    compare_pytorch_vs_numpy()

    # 5. æ€»ç»“
    print("\n" + "=" * 70)
    print("âœ… æ ¸å¿ƒè¦ç‚¹æ€»ç»“")
    print("=" * 70)
    print("""
1. PyTorch Embedding åŸºç¡€

   åˆ›å»º Embedding å±‚ï¼š
   embedding = nn.Embedding(vocab_size, embedding_dim)

   æŸ¥æ‰¾ï¼š
   word_embedding = embedding(word_idx)  # è‡ªåŠ¨æŸ¥æ‰¾ + æ¢¯åº¦

   æ‰¹é‡æŸ¥æ‰¾ï¼š
   batch_embeddings = embedding(batch_indices)

2. Word2Vec (Skip-gram)

   æ¨¡å‹ç»“æ„ï¼š
   class Word2Vec(nn.Module):
       def __init__(self):
           self.embeddings = nn.Embedding(vocab_size, embed_dim)
           self.linear = nn.Linear(embed_dim, vocab_size)

   è®­ç»ƒç›®æ ‡ï¼š
   - ç»™å®šä¸­å¿ƒè¯ï¼Œé¢„æµ‹ä¸Šä¸‹æ–‡è¯
   - æœ€å¤§åŒ– P(context | center)

3. æ¨èç³»ç»Ÿä¸­çš„ Embedding

   çŸ©é˜µåˆ†è§£ï¼š
   class MatrixFactorization(nn.Module):
       def __init__(self):
           self.user_embeddings = nn.Embedding(n_users, embed_dim)
           self.item_embeddings = nn.Embedding(n_items, embed_dim)

       def forward(self, user_ids, item_ids):
           user_emb = self.user_embeddings(user_ids)
           item_emb = self.item_embeddings(item_ids)
           return (user_emb * item_emb).sum(dim=1)  # ç‚¹ç§¯

4. GPU åŠ é€Ÿ

   model = model.to(device)
   data = data.to(device)

   é€Ÿåº¦æå‡ï¼šCPU 30ç§’ â†’ GPU 5ç§’ï¼ˆ6å€ï¼‰

5. é¢„è®­ç»ƒ Embedding

   # åŠ è½½é¢„è®­ç»ƒæƒé‡
   pretrained = torch.randn(vocab_size, embed_dim)
   embedding = nn.Embedding.from_pretrained(
       pretrained,
       freeze=True  # å†»ç»“æƒé‡
   )

6. PyTorch vs NumPy

   NumPy:
   - ç†è§£åŸç†ï¼ˆæ‰‹åŠ¨æŸ¥æ‰¾ + æ¢¯åº¦ï¼‰
   - ä»£ç é‡å¤§
   - é€Ÿåº¦æ…¢

   PyTorch:
   - å·¥ä¸šå®è·µï¼ˆè‡ªåŠ¨å¾®åˆ†ï¼‰
   - ä»£ç ç®€æ´
   - é€Ÿåº¦å¿«100å€

7. Embedding åº”ç”¨

   NLP:
   - Word2Vec, GloVe, FastText
   - BERT, GPT (Transformer)
   - è¯­ä¹‰æœç´¢

   æ¨èç³»ç»Ÿ:
   - User/Item Embeddings
   - Two-Tower Models
   - ååŒè¿‡æ»¤

   å…¶ä»–:
   - Knowledge Graphs
   - ç¤¾äº¤ç½‘ç»œ
   - ç”Ÿç‰©ä¿¡æ¯å­¦

8. å®è·µå»ºè®®

   å­¦ä¹ è·¯å¾„ï¼š
   1. å…ˆçœ‹ NumPy ç‰ˆæœ¬ï¼ˆç†è§£æŸ¥æ‰¾åŸç†ï¼‰
   2. å†çœ‹ PyTorch ç‰ˆæœ¬ï¼ˆå­¦ä¹ æ¡†æ¶ï¼‰
   3. å¯¹æ¯”ä¸¤ä¸ªç‰ˆæœ¬

   å®é™…å·¥ä½œï¼š
   - 100% ç”¨ PyTorchï¼ˆæˆ– TensorFlowï¼‰
   - ä½¿ç”¨é¢„è®­ç»ƒ Embeddings when possible
   - æ ¹æ®ä»»åŠ¡é€‰æ‹© embedding_dim (50-512)

9. Two-Tower æ¨¡å‹è¿æ¥

   è¿™æ˜¯æ¨èç³»ç»Ÿçš„åŸºç¡€ï¼

   User Tower:                  Item Tower:
   User features                Item features
        â†“                            â†“
   User Embedding (64D)         Item Embedding (64D)
        â†“                            â†“
        â””â”€â”€â”€â”€â”€â”€â”€â”€ Dot Product â”€â”€â”€â”€â”€â”€â”€â”˜
                      â†“
                  Score (ç›¸å…³æ€§)

10. ä¸‹ä¸€æ­¥

    - å­¦ä¹  Attention æœºåˆ¶ï¼ˆTransformer åŸºç¡€ï¼‰
    - å®ç° Two-Tower æ¨èæ¨¡å‹
    - å°è¯•é¢„è®­ç»ƒ Embeddingsï¼ˆGloVe, Word2Vecï¼‰
    - å¯è§†åŒ–é«˜ç»´ Embeddingsï¼ˆt-SNEï¼‰
    """)


if __name__ == "__main__":
    main()

    print("\nğŸ’¡ ç»ƒä¹ å»ºè®®:")
    print("  1. åœ¨æ›´å¤§çš„è¯­æ–™åº“ä¸Šè®­ç»ƒ Word2Vec")
    print("  2. å®ç° CBOW æ¨¡å‹ï¼ˆWord2Vec çš„å¦ä¸€ä¸ªå˜ä½“ï¼‰")
    print("  3. ä½¿ç”¨é¢„è®­ç»ƒ Embeddingsï¼ˆGloVe, FastTextï¼‰")
    print("  4. å®ç° Two-Tower æ¨èæ¨¡å‹")
    print("  5. å°è¯•ä¸åŒçš„ embedding_dimï¼Œè§‚å¯Ÿæ•ˆæœ")
    print("  6. å¯è§†åŒ– t-SNEï¼ˆéçº¿æ€§é™ç»´ï¼‰")
