"""
å·ç§¯ç¥ç»ç½‘ç»œ (CNN) - PyTorch å®ç°

å¯¹æ¯” NumPy ç‰ˆæœ¬ï¼š
- NumPy: æ‰‹å†™å·ç§¯ã€æ± åŒ–ï¼Œç†è§£æ•°å­¦åŸç†
- PyTorch: ä½¿ç”¨æ¡†æ¶ï¼ŒGPUåŠ é€Ÿï¼Œå·¥ä¸šå®è·µ

æœ¬æ–‡ä»¶å†…å®¹ï¼š
1. PyTorch CNN åŸºç¡€ç»„ä»¶
2. å®Œæ•´çš„ CNN æ¨¡å‹ï¼ˆMNIST æ•°å­—è¯†åˆ«ï¼‰
3. GPU è®­ç»ƒåŠ é€Ÿ
4. è®­ç»ƒå¯è§†åŒ–
5. ä¸ NumPy ç‰ˆæœ¬æ€§èƒ½å¯¹æ¯”
"""

import torch
import torch.nn as nn
import torch.nn.functional as F
import torch.optim as optim
from torch.utils.data import DataLoader
from torchvision import datasets, transforms
import matplotlib.pyplot as plt
import numpy as np
import time


# ==================== 1. PyTorch CNN åŸºç¡€ç»„ä»¶ ====================
def demo_pytorch_conv():
    """
    æ¼”ç¤º PyTorch çš„å·ç§¯æ“ä½œ

    ====================================================================
    ğŸ”‘ PyTorch vs NumPy å·ç§¯
    ====================================================================

    NumPy ç‰ˆæœ¬ï¼ˆæ‰‹å†™ï¼‰ï¼š
    ```python
    def conv2d(image, kernel):
        for i in range(out_h):
            for j in range(out_w):
                window = image[i:i+k, j:j+k]
                output[i, j] = np.sum(window * kernel)
    ```

    PyTorch ç‰ˆæœ¬ï¼ˆä¸€è¡Œï¼‰ï¼š
    ```python
    output = F.conv2d(image, kernel)
    ```

    PyTorch å¸®ä½ åšäº†ä»€ä¹ˆï¼Ÿ
    - è‡ªåŠ¨æ‰¹é‡å¤„ç†ï¼ˆbatchï¼‰
    - è‡ªåŠ¨GPUåŠ é€Ÿ
    - è‡ªåŠ¨è®¡ç®—æ¢¯åº¦ï¼ˆåå‘ä¼ æ’­ï¼‰
    - æ•°å€¼ä¼˜åŒ–ï¼ˆæ›´å¿«æ›´ç¨³å®šï¼‰

    ====================================================================
    """
    print("=" * 70)
    print("1. PyTorch å·ç§¯æ“ä½œæ¼”ç¤º")
    print("=" * 70)

    # åˆ›å»ºè¾“å…¥å›¾åƒï¼ˆbatch_size=1, channels=1, height=5, width=5ï¼‰
    # PyTorch æ ¼å¼ï¼š(N, C, H, W)
    image = torch.tensor([
        [1, 2, 3, 4, 5],
        [2, 3, 4, 5, 6],
        [3, 4, 5, 6, 7],
        [4, 5, 6, 7, 8],
        [5, 6, 7, 8, 9],
    ], dtype=torch.float32).unsqueeze(0).unsqueeze(0)  # æ·»åŠ  batch å’Œ channel ç»´åº¦

    print(f"\nè¾“å…¥å›¾åƒ shape: {image.shape}")  # (1, 1, 5, 5)

    # ====================================================================
    # ğŸ”‘ åˆ›å»ºå·ç§¯æ ¸ï¼ˆå‚ç›´è¾¹ç¼˜æ£€æµ‹ï¼‰
    # ====================================================================
    # PyTorch å·ç§¯æ ¸æ ¼å¼ï¼š(out_channels, in_channels, height, width)
    #                       â†‘            â†‘            â†‘      â†‘
    #                       â”‚            â”‚            â””â”€â”€â”€â”€â”€â”€â””â”€â”€ å·ç§¯æ ¸å¤§å° 3Ã—3
    #                       â”‚            â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ è¾“å…¥é€šé“æ•° = 1ï¼ˆç°åº¦å›¾ï¼‰
    #                       â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ è¾“å‡ºé€šé“æ•° = 1ï¼ˆ1ä¸ªç‰¹å¾å›¾ï¼‰
    #
    # ğŸ“Œ in_channelsï¼ˆè¾“å…¥é€šé“æ•°ï¼‰ï¼š
    #    - è¾“å…¥å›¾åƒæœ‰å‡ ä¸ªé€šé“
    #    - ç°åº¦å›¾ = 1 ä¸ªé€šé“ï¼ˆé»‘ç™½ï¼‰
    #    - RGB å½©è‰²å›¾ = 3 ä¸ªé€šé“ï¼ˆçº¢ã€ç»¿ã€è“ï¼‰
    #    - ä¸­é—´å±‚ = ä»»æ„æ•°é‡ï¼ˆå‰ä¸€å±‚çš„è¾“å‡ºï¼‰
    #
    # ğŸ“Œ out_channelsï¼ˆè¾“å‡ºé€šé“æ•°ï¼‰ï¼š
    #    - ä½ æƒ³è¦å¤šå°‘ä¸ªå·ç§¯æ ¸ï¼ˆæå–å¤šå°‘ç§ç‰¹å¾ï¼‰
    #    - æ¯ä¸ªå·ç§¯æ ¸ç”Ÿæˆä¸€ä¸ªç‰¹å¾å›¾ï¼ˆfeature mapï¼‰
    #    - ä¾‹å¦‚ï¼š32 ä¸ªè¾“å‡ºé€šé“ = 32 ä¸ªä¸åŒçš„å·ç§¯æ ¸ = 32 ä¸ªç‰¹å¾å›¾
    #
    # ğŸ“Œ ä¸ºä»€ä¹ˆéœ€è¦å››ç»´ï¼Ÿ
    #    å•é€šé“ä¾‹å­ï¼ˆç°åº¦å›¾ï¼‰ï¼š
    #      - kernel shape: (1, 1, 3, 3)
    #      - 1 ä¸ªè¾“å‡ºé€šé“ï¼Œ1 ä¸ªè¾“å…¥é€šé“ï¼Œ3Ã—3 å¤§å°
    #
    #    RGB å½©è‰²å›¾ä¾‹å­ï¼š
    #      - kernel shape: (64, 3, 3, 3)
    #      - 64 ä¸ªè¾“å‡ºé€šé“ï¼ˆ64ç§ç‰¹å¾ï¼‰
    #      - 3 ä¸ªè¾“å…¥é€šé“ï¼ˆRã€Gã€Bï¼‰
    #      - 3Ã—3 å¤§å°
    #      - å®é™…ä¸Šæœ‰ 64 ä¸ª"å®Œæ•´å·ç§¯æ ¸"ï¼Œæ¯ä¸ªåŒ…å« 3 ä¸ª 3Ã—3 çš„å­æ ¸ï¼ˆå¯¹åº”RGBï¼‰
    #
    # ====================================================================
    kernel = torch.tensor([
        [-1, 0, 1],
        [-1, 0, 1],
        [-1, 0, 1],
    ], dtype=torch.float32).unsqueeze(0).unsqueeze(0)  # (1, 1, 3, 3)

    print(f"å·ç§¯æ ¸ shape: {kernel.shape}")  # (1, 1, 3, 3)

    # ====================================================================
    # ğŸ”‘ æ–¹å¼1ï¼šä½¿ç”¨ F.conv2dï¼ˆå‡½æ•°å¼ APIï¼‰
    # ====================================================================
    # F = torch.nn.functionalï¼ˆå‡½æ•°å¼ï¼‰
    # ç‰¹ç‚¹ï¼š
    #   - ç›´æ¥è°ƒç”¨å‡½æ•°
    #   - æ¯æ¬¡éƒ½è¦ä¼ å‚æ•°
    #   - é€‚åˆä¸´æ—¶ä½¿ç”¨ã€å¿«é€Ÿå®éªŒ
    # ====================================================================
    output = F.conv2d(image, kernel)

    print(f"è¾“å‡º shape: {output.shape}")  # (1, 1, 3, 3)
    print(f"\nå·ç§¯è¾“å‡º:\n{output.squeeze()}")

    # ====================================================================
    # ğŸ”‘ æ–¹å¼2ï¼šä½¿ç”¨ nn.Conv2dï¼ˆæ¨¡å— APIï¼‰- æ¨èï¼
    # ====================================================================
    # nn = torch.nnï¼ˆæ¨¡å—ï¼‰
    # ç‰¹ç‚¹ï¼š
    #   - å…ˆåˆ›å»ºæ¨¡å—å¯¹è±¡ï¼ˆä¿å­˜é…ç½®å’Œå‚æ•°ï¼‰
    #   - å¯ä»¥é‡å¤ä½¿ç”¨ï¼Œä¸ç”¨æ¯æ¬¡å†™å‚æ•°
    #   - é€‚åˆåœ¨æ¨¡å‹ä¸­å®šä¹‰
    #   - æœ‰å¯å­¦ä¹ çš„æƒé‡ï¼ˆä¼šè‡ªåŠ¨åˆå§‹åŒ–ï¼‰
    #
    # æ¨èåœ¨æ¨¡å‹ä¸­ä½¿ç”¨ nnï¼š
    #   - ä»£ç æ›´æ¸…æ™°
    #   - å‚æ•°ç®¡ç†æ›´æ–¹ä¾¿
    #   - æƒé‡è‡ªåŠ¨åˆå§‹åŒ–å’Œç®¡ç†
    # ====================================================================
    conv_layer = nn.Conv2d(in_channels=1, out_channels=1, kernel_size=3)

    # æ‰‹åŠ¨è®¾ç½®æƒé‡ï¼ˆå’Œä¸Šé¢çš„kernelä¸€æ ·ï¼‰
    with torch.no_grad():
        conv_layer.weight = nn.Parameter(kernel)
        conv_layer.bias = nn.Parameter(torch.zeros(1))

    output2 = conv_layer(image)
    print(f"\nnn.Conv2d è¾“å‡º:\n{output2.squeeze()}")

    print("\nğŸ’¡ PyTorch ä¼˜åŠ¿:")
    print("  - ä¸€è¡Œä»£ç å®Œæˆå·ç§¯")
    print("  - è‡ªåŠ¨æ”¯æŒ batch å¤„ç†")
    print("  - è‡ªåŠ¨è®¡ç®—æ¢¯åº¦ï¼ˆåå‘ä¼ æ’­ï¼‰")
    print("  - GPU åŠ é€Ÿï¼ˆæ·»åŠ  .cuda()ï¼‰")


def demo_pytorch_pooling():
    """
    æ¼”ç¤º PyTorch çš„æ± åŒ–æ“ä½œ

    ====================================================================
    ğŸ”‘ ä»€ä¹ˆæ˜¯æœ€å¤§æ± åŒ–ï¼ˆMax Poolingï¼‰ï¼Ÿ
    ====================================================================

    æ± åŒ–æ˜¯ä¸€ç§ä¸‹é‡‡æ ·æ“ä½œï¼Œç”¨äºï¼š
    1. ç¼©å°ç‰¹å¾å›¾å°ºå¯¸ï¼ˆå‡å°‘è®¡ç®—é‡ï¼‰
    2. ä¿ç•™æœ€é‡è¦çš„ç‰¹å¾ï¼ˆå–æœ€å¤§å€¼ï¼‰
    3. å¢å¼ºç‰¹å¾çš„å¹³ç§»ä¸å˜æ€§

    ç¤ºä¾‹ï¼ˆ2Ã—2 MaxPoolingï¼‰ï¼š

    è¾“å…¥ (4Ã—4):          è¾“å‡º (2Ã—2):
    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”       â”Œâ”€â”€â”€â”€â”€â”€â”
    â”‚  1  2â”‚ 3  4â”‚       â”‚ 6â”‚ 8 â”‚
    â”‚  5  6â”‚ 7  8â”‚  â†’    â”œâ”€â”€â”¼â”€â”€â”€â”¤
    â”œâ”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”¤       â”‚14â”‚16 â”‚
    â”‚ 9 10â”‚11 12â”‚       â””â”€â”€â”€â”€â”€â”€â”˜
    â”‚13 14â”‚15 16â”‚
    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

    æ¯ä¸ª 2Ã—2 åŒºåŸŸå–æœ€å¤§å€¼ï¼š
    - å·¦ä¸Šï¼šmax(1,2,5,6) = 6
    - å³ä¸Šï¼šmax(3,4,7,8) = 8
    - å·¦ä¸‹ï¼šmax(9,10,13,14) = 14
    - å³ä¸‹ï¼šmax(11,12,15,16) = 16

    å…³é”®ç‰¹ç‚¹ï¼š
    - æ± åŒ–ä¸æ”¹å˜é€šé“æ•°ï¼ˆåªæ”¹å˜å°ºå¯¸ï¼‰
    - æ± åŒ–æ²¡æœ‰å¯å­¦ä¹ å‚æ•°

    ====================================================================
    """
    print("\n" + "=" * 70)
    print("2. PyTorch æ± åŒ–æ“ä½œæ¼”ç¤º")
    print("=" * 70)

    # åˆ›å»ºè¾“å…¥
    x = torch.tensor([
        [1, 2, 3, 4],
        [5, 6, 7, 8],
        [9, 10, 11, 12],
        [13, 14, 15, 16],
    ], dtype=torch.float32).unsqueeze(0).unsqueeze(0)

    print(f"\nè¾“å…¥ shape: {x.shape}")  # (1, 1, 4, 4)
    print(f"è¾“å…¥:\n{x.squeeze()}")

    # ====================================================================
    # ğŸ”‘ æ–¹å¼1ï¼šF.max_pool2dï¼ˆå‡½æ•°å¼ APIï¼‰
    # ====================================================================
    # ç”¨æ³•ï¼šF.max_pool2d(input, kernel_size, stride, padding)
    #
    # å‚æ•°è¯´æ˜ï¼š
    #   - kernel_size: æ± åŒ–çª—å£å¤§å°ï¼ˆ2 è¡¨ç¤º 2Ã—2ï¼‰
    #   - stride: æ­¥é•¿ï¼ˆé»˜è®¤=kernel_sizeï¼Œä¸é‡å ï¼‰
    #   - padding: è¾¹ç¼˜å¡«å……ï¼ˆé»˜è®¤=0ï¼‰
    #
    # ç‰¹ç‚¹ï¼š
    #   - ç›´æ¥è°ƒç”¨å‡½æ•°
    #   - æ¯æ¬¡éƒ½è¦ä¼ å‚æ•°
    #   - é€‚åˆä¸´æ—¶ä½¿ç”¨
    # ====================================================================
    pooled = F.max_pool2d(x, kernel_size=2)
    print(f"\nã€æ–¹å¼1ï¼šF.max_pool2dã€‘")
    print(f"è¾“å‡º shape: {pooled.shape}")  # (1, 1, 2, 2)
    print(f"è¾“å‡º:\n{pooled.squeeze()}")

    # ====================================================================
    # ğŸ”‘ æ–¹å¼2ï¼šnn.MaxPool2dï¼ˆæ¨¡å— APIï¼‰- æ¨èï¼
    # ====================================================================
    # ç”¨æ³•ï¼š
    #   1. åˆ›å»ºæ¨¡å—ï¼špool_layer = nn.MaxPool2d(kernel_size=2)
    #   2. ä½¿ç”¨æ¨¡å—ï¼šoutput = pool_layer(input)
    #
    # ç‰¹ç‚¹ï¼š
    #   - å…ˆåˆ›å»ºæ¨¡å—å¯¹è±¡ï¼ˆä¿å­˜é…ç½®ï¼‰
    #   - å¯ä»¥é‡å¤ä½¿ç”¨
    #   - é€‚åˆåœ¨æ¨¡å‹ä¸­å®šä¹‰ï¼ˆæ¨èï¼‰
    #
    # åœ¨æ¨¡å‹ä¸­ä½¿ç”¨ç¤ºä¾‹ï¼š
    #   class CNN(nn.Module):
    #       def __init__(self):
    #           self.pool = nn.MaxPool2d(2)  # å®šä¹‰ä¸€æ¬¡
    #
    #       def forward(self, x):
    #           x = self.pool(x)  # é‡å¤ä½¿ç”¨ï¼Œä¸ç”¨æ¯æ¬¡å†™å‚æ•°
    # ====================================================================
    pool_layer = nn.MaxPool2d(kernel_size=2)
    pooled2 = pool_layer(x)
    print(f"\nã€æ–¹å¼2ï¼šnn.MaxPool2dã€‘")
    print(f"è¾“å‡º:\n{pooled2.squeeze()}")

    print("\nğŸ’¡ å…³é”®ç‚¹:")
    print("  - æ± åŒ–å°† 4Ã—4 ç¼©å°ä¸º 2Ã—2ï¼ˆå°ºå¯¸å‡å°‘ 75%ï¼‰")
    print("  - é€šé“æ•°ä¸å˜ï¼ˆè¾“å…¥1ä¸ªé€šé“ï¼Œè¾“å‡ºè¿˜æ˜¯1ä¸ªé€šé“ï¼‰")
    print("  - æ²¡æœ‰å¯å­¦ä¹ å‚æ•°ï¼ˆåªæ˜¯å–æœ€å¤§å€¼ï¼‰")
    print("  - åœ¨æ¨¡å‹ä¸­æ¨èä½¿ç”¨ nn.MaxPool2d")


# ==================== 2. å®Œæ•´çš„ CNN æ¨¡å‹ ====================
class SimpleCNN(nn.Module):
    """
    ç®€å•çš„ CNN æ¨¡å‹ï¼ˆMNIST æ•°å­—è¯†åˆ«ï¼‰

    ====================================================================
    ğŸ”‘ PyTorch æ¨¡å‹å®šä¹‰
    ====================================================================

    PyTorch å®šä¹‰æ¨¡å‹æœ‰ä¸¤æ­¥ï¼š
    1. __init__: å®šä¹‰å±‚ï¼ˆlayerï¼‰
    2. forward: å®šä¹‰å‰å‘ä¼ æ’­é€»è¾‘

    å¯¹æ¯” NumPyï¼š
    - NumPy: æ‰‹åŠ¨ç®¡ç†æ‰€æœ‰æƒé‡ï¼Œæ‰‹å†™å‰å‘ä¼ æ’­
    - PyTorch: å®šä¹‰å±‚ç»“æ„ï¼Œè‡ªåŠ¨ç®¡ç†æƒé‡ï¼Œè‡ªåŠ¨åå‘ä¼ æ’­

    ====================================================================

    ç½‘ç»œç»“æ„ï¼š
    Input (1, 28, 28)
        â†“
    Conv1 (32, 26, 26)  # 3Ã—3 å·ç§¯ï¼Œ32ä¸ªå·ç§¯æ ¸
        â†“ ReLU
    Pool1 (32, 13, 13)  # 2Ã—2 æœ€å¤§æ± åŒ–
        â†“
    Conv2 (64, 11, 11)  # 3Ã—3 å·ç§¯ï¼Œ64ä¸ªå·ç§¯æ ¸
        â†“ ReLU
    Pool2 (64, 5, 5)    # 2Ã—2 æœ€å¤§æ± åŒ–
        â†“
    Flatten (1600)      # å±•å¹³
        â†“
    FC1 (128)           # å…¨è¿æ¥å±‚
        â†“ ReLU + Dropout
    FC2 (10)            # è¾“å‡ºå±‚ï¼ˆ10ä¸ªç±»åˆ«ï¼‰
        â†“ Softmax (éšå¼åœ¨lossä¸­)
    Output (10)
    """

    def __init__(self):
        super(SimpleCNN, self).__init__()

        # ================================================================
        # ğŸ”‘ å·ç§¯å±‚1: 1 â†’ 32 channels
        # ================================================================
        # nn.Conv2d(in_channels=1, out_channels=32, kernel_size=3)
        #           â†‘             â†‘
        #           â”‚             â””â”€ è¾“å‡º 32 ä¸ªé€šé“ï¼ˆ32 ä¸ªç‰¹å¾å›¾ï¼‰
        #           â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ è¾“å…¥ 1 ä¸ªé€šé“ï¼ˆMNIST ç°åº¦å›¾ï¼‰
        #
        # ç†è§£ï¼š
        #   - è¾“å…¥ï¼šç°åº¦å›¾ï¼Œ1 ä¸ªé€šé“ï¼ˆé»‘ç™½ï¼‰
        #   - è¾“å‡ºï¼š32 ä¸ªç‰¹å¾å›¾ï¼ˆ32 ç§ä¸åŒçš„ç‰¹å¾ï¼‰
        #   - å®é™…æƒé‡å½¢çŠ¶ï¼š(32, 1, 3, 3)
        #     â†‘   â†‘  â†‘  â†‘
        #     â”‚   â”‚  â””â”€â”€â””â”€â”€ 3Ã—3 å·ç§¯æ ¸
        #     â”‚   â””â”€â”€â”€â”€â”€â”€â”€ 1 ä¸ªè¾“å…¥é€šé“
        #     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ 32 ä¸ªå·ç§¯æ ¸ï¼ˆæå–32ç§ç‰¹å¾ï¼šè¾¹ç¼˜ã€çº¹ç†ç­‰ï¼‰
        #
        # ç±»æ¯”ï¼š32 ä¸ªæ¢æµ‹å™¨ï¼Œæ¯ä¸ªæ¢æµ‹ä¸€ç§ç‰¹å¾
        #   - ç¬¬1ä¸ªï¼šæ£€æµ‹å‚ç›´è¾¹ç¼˜
        #   - ç¬¬2ä¸ªï¼šæ£€æµ‹æ°´å¹³è¾¹ç¼˜
        #   - ç¬¬3ä¸ªï¼šæ£€æµ‹45Â°è¾¹ç¼˜
        #   - ... å…±32ä¸ª
        # ================================================================
        self.conv1 = nn.Conv2d(in_channels=1, out_channels=32, kernel_size=3)

        # ================================================================
        # ğŸ”‘ å·ç§¯å±‚2: 32 â†’ 64 channels
        # ================================================================
        # nn.Conv2d(in_channels=32, out_channels=64, kernel_size=3)
        #           â†‘              â†‘
        #           â”‚              â””â”€ è¾“å‡º 64 ä¸ªé€šé“ï¼ˆ64 ä¸ªç‰¹å¾å›¾ï¼‰
        #           â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ è¾“å…¥ 32 ä¸ªé€šé“ï¼ˆæ¥è‡ªä¸Šä¸€å±‚ï¼‰
        #
        # ç†è§£ï¼š
        #   - è¾“å…¥ï¼š32 ä¸ªç‰¹å¾å›¾ï¼ˆconv1 çš„è¾“å‡ºï¼‰
        #   - è¾“å‡ºï¼š64 ä¸ªç‰¹å¾å›¾ï¼ˆæ›´å¤æ‚çš„ç‰¹å¾ï¼‰
        #   - å®é™…æƒé‡å½¢çŠ¶ï¼š(64, 32, 3, 3)
        #     â†‘   â†‘   â†‘  â†‘
        #     â”‚   â”‚   â””â”€â”€â””â”€â”€ 3Ã—3 å·ç§¯æ ¸
        #     â”‚   â””â”€â”€â”€â”€â”€â”€â”€â”€ 32 ä¸ªè¾“å…¥é€šé“ï¼ˆå¿…é¡»åŒ¹é…ä¸Šä¸€å±‚è¾“å‡ºï¼ï¼‰
        #     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ 64 ä¸ªå·ç§¯æ ¸
        #
        # å…³é”®ç‚¹ï¼š
        #   - in_channels=32 å¿…é¡»ç­‰äº conv1 çš„ out_channels=32
        #   - æ¯ä¸ªè¾“å‡ºé€šé“å¯¹åº”ä¸€ä¸ª"å®Œæ•´å·ç§¯æ ¸"ï¼ŒåŒ…å« 32 ä¸ª 3Ã—3 å­æ ¸
        #   - å·ç§¯è¿‡ç¨‹ï¼š
        #     1. å¯¹è¾“å…¥çš„ 32 ä¸ªé€šé“åˆ†åˆ«å·ç§¯ï¼ˆ32 ä¸ª 3Ã—3 å·ç§¯ï¼‰
        #     2. æŠŠ 32 ä¸ªç»“æœç›¸åŠ 
        #     3. å¾—åˆ° 1 ä¸ªè¾“å‡ºç‰¹å¾å›¾
        #     4. é‡å¤ 64 æ¬¡ï¼ˆ64 ä¸ªä¸åŒçš„å·ç§¯æ ¸ï¼‰â†’ 64 ä¸ªè¾“å‡ºç‰¹å¾å›¾
        # ================================================================
        self.conv2 = nn.Conv2d(in_channels=32, out_channels=64, kernel_size=3)

        # æ± åŒ–å±‚ï¼ˆ2Ã—2ï¼‰
        self.pool = nn.MaxPool2d(kernel_size=2, stride=2)

        # å…¨è¿æ¥å±‚1: 64*5*5 â†’ 128
        # ä¸ºä»€ä¹ˆæ˜¯ 5Ã—5ï¼Ÿ
        # 28 â†’ (å·ç§¯-2=26) â†’ (æ± åŒ–/2=13) â†’ (å·ç§¯-2=11) â†’ (æ± åŒ–/2=5)
        self.fc1 = nn.Linear(64 * 5 * 5, 128)

        # Dropout é˜²æ­¢è¿‡æ‹Ÿåˆ
        self.dropout = nn.Dropout(0.5)

        # å…¨è¿æ¥å±‚2: 128 â†’ 10 (10ä¸ªæ•°å­—ç±»åˆ«)
        self.fc2 = nn.Linear(128, 10)

    def forward(self, x):
        """
        å‰å‘ä¼ æ’­

        x: (batch_size, 1, 28, 28)
        è¿”å›: (batch_size, 10)
        """
        # ================================================================
        # ğŸ”‘ å·ç§¯å±‚1 + ReLU + æ± åŒ–
        # ================================================================
        # è¾“å…¥ï¼š(batch, 1, 28, 28)  â† 1ä¸ªé€šé“ï¼ˆç°åº¦å›¾ï¼‰
        #       â†“ conv1: 1 â†’ 32 channels
        # è¾“å‡ºï¼š(batch, 32, 26, 26) â† 32ä¸ªé€šé“ï¼ˆ32ä¸ªç‰¹å¾å›¾ï¼‰
        #
        # å‘ç”Ÿäº†ä»€ä¹ˆï¼Ÿ
        #   - 32 ä¸ªä¸åŒçš„ 3Ã—3 å·ç§¯æ ¸æ‰«æå›¾åƒ
        #   - æ¯ä¸ªå·ç§¯æ ¸æ£€æµ‹ä¸€ç§ç‰¹å¾ï¼ˆè¾¹ç¼˜ã€çº¹ç†ç­‰ï¼‰
        #   - ç”Ÿæˆ 32 ä¸ªç‰¹å¾å›¾
        # ================================================================
        x = self.conv1(x)           # (batch, 32, 26, 26) - å·ç§¯

        # ----------------------------------------------------------------
        # ğŸ’¡ ä¸ºä»€ä¹ˆ ReLU ç”¨ F.relu è€Œä¸æ˜¯ nn.ReLUï¼Ÿ
        # ----------------------------------------------------------------
        # ReLU æ˜¯æ— å‚æ•°çš„æ¿€æ´»å‡½æ•°ï¼Œä¸¤ç§æ–¹å¼éƒ½å¯ä»¥ï¼š
        #   - F.relu(x)ï¼šå‡½æ•°å¼ï¼Œç®€æ´ï¼ˆæ¨èï¼‰
        #   - nn.ReLU()ï¼šæ¨¡å—å¼ï¼Œéœ€è¦åœ¨ __init__ ä¸­å®šä¹‰
        #
        # æ¨èç”¨ F.reluï¼š
        #   âœ… æ›´ç®€æ´ï¼ˆä¸ç”¨åœ¨ __init__ ä¸­å®šä¹‰ï¼‰
        #   âœ… æ²¡æœ‰é¢å¤–çš„å¯¹è±¡å ç”¨å†…å­˜
        #   âœ… ä»£ç æ›´æ˜“è¯»
        #
        # å¦‚æœç”¨ nn.ReLUï¼Œéœ€è¦ï¼š
        #   __init__: self.relu = nn.ReLU()
        #   forward:  x = self.relu(x)  # æœ‰ç‚¹å†—ä½™
        # ----------------------------------------------------------------
        x = F.relu(x)               # æ¿€æ´»å‡½æ•°ï¼Œå¢åŠ éçº¿æ€§

        x = self.pool(x)            # (batch, 32, 13, 13) - å°ºå¯¸ç¼©å°ï¼Œé€šé“æ•°ä¸å˜

        # ================================================================
        # ğŸ”‘ å·ç§¯å±‚2 + ReLU + æ± åŒ–
        # ================================================================
        # è¾“å…¥ï¼š(batch, 32, 13, 13) â† 32ä¸ªé€šé“ï¼ˆæ¥è‡ªconv1ï¼‰
        #       â†“ conv2: 32 â†’ 64 channels
        # è¾“å‡ºï¼š(batch, 64, 11, 11) â† 64ä¸ªé€šé“ï¼ˆ64ä¸ªæ›´é«˜çº§ç‰¹å¾ï¼‰
        #
        # å‘ç”Ÿäº†ä»€ä¹ˆï¼Ÿ
        #   - 64 ä¸ª"å®Œæ•´å·ç§¯æ ¸"ï¼Œæ¯ä¸ªåŒ…å« 32 ä¸ª 3Ã—3 å­æ ¸
        #   - æ¯ä¸ªå·ç§¯æ ¸çœ‹æ‰€æœ‰ 32 ä¸ªè¾“å…¥ç‰¹å¾å›¾
        #   - ç»„åˆåº•å±‚ç‰¹å¾ï¼Œæå–æ›´å¤æ‚çš„é«˜çº§ç‰¹å¾
        #   - ä¾‹å¦‚ï¼šç»„åˆè¾¹ç¼˜ â†’ å½¢çŠ¶ï¼Œç»„åˆçº¹ç† â†’ å›¾æ¡ˆ
        # ================================================================
        x = self.conv2(x)           # (batch, 64, 11, 11)
        x = F.relu(x)
        x = self.pool(x)            # (batch, 64, 5, 5)

        # å±•å¹³
        x = x.view(-1, 64 * 5 * 5)  # (batch, 1600)
        # ä¹Ÿå¯ä»¥ç”¨: x = torch.flatten(x, 1)

        # å…¨è¿æ¥å±‚1 + ReLU + Dropout
        x = self.fc1(x)             # (batch, 128)
        x = F.relu(x)
        x = self.dropout(x)

        # å…¨è¿æ¥å±‚2ï¼ˆè¾“å‡ºå±‚ï¼‰
        x = self.fc2(x)             # (batch, 10)

        return x


# ==================== 3. è®­ç»ƒå’Œè¯„ä¼° ====================
def train_one_epoch(model, device, train_loader, optimizer, criterion, epoch):
    """è®­ç»ƒä¸€ä¸ª epoch"""

    # ====================================================================
    # ğŸ”‘ è®¾ç½®ä¸ºè®­ç»ƒæ¨¡å¼
    # ====================================================================
    # model.train() çš„ä½œç”¨ï¼š
    #
    # 1. Dropoutï¼š
    #    - è®­ç»ƒæ¨¡å¼ï¼šå¯ç”¨ï¼Œéšæœºä¸¢å¼ƒç¥ç»å…ƒ
    #    - ä¾‹å¦‚ Dropout(0.5) ä¼šéšæœºä¸¢å¼ƒ 50% çš„ç¥ç»å…ƒ
    #
    # 2. BatchNormï¼š
    #    - è®­ç»ƒæ¨¡å¼ï¼šæ›´æ–°å‡å€¼å’Œæ–¹å·®çš„ç»Ÿè®¡é‡
    #    - ä½¿ç”¨å½“å‰ batch çš„ç»Ÿè®¡é‡
    #
    # 3. å…¶ä»–å±‚ï¼ˆConvã€Linearã€ReLUã€MaxPoolï¼‰ï¼š
    #    - æ— å½±å“ï¼Œè¡Œä¸ºç›¸åŒ
    #
    # ä¸ºä»€ä¹ˆéœ€è¦è®­ç»ƒæ¨¡å¼ï¼Ÿ
    #    - Dropout é˜²æ­¢è¿‡æ‹Ÿåˆï¼ˆè®­ç»ƒæ—¶éœ€è¦ï¼‰
    #    - BatchNorm éœ€è¦ä»æ•°æ®ä¸­å­¦ä¹ ç»Ÿè®¡é‡
    # ====================================================================
    model.train()

    total_loss = 0
    correct = 0
    total = 0

    for batch_idx, (data, target) in enumerate(train_loader):
        # ================================================================
        # ğŸ”‘ æ•°æ®åŠ è½½
        # ================================================================
        # data:   è¾“å…¥å›¾åƒ (batch_size, 1, 28, 28)
        # target: æ ‡ç­¾/æ­£ç¡®ç­”æ¡ˆ (batch_size,) ä¾‹å¦‚ [3, 7, 2, 0, ...]
        #
        # ä¾‹å¦‚ï¼š
        #   data[0] = ä¸€å¼ æ‰‹å†™æ•°å­— "3" çš„å›¾åƒ
        #   target[0] = 3 (æ­£ç¡®ç­”æ¡ˆ)
        # ================================================================

        # ç§»åˆ° GPU/CPU
        data, target = data.to(device), target.to(device)

        # ================================================================
        # ğŸ”‘ PyTorch è®­ç»ƒçš„æ ‡å‡†å››æ­¥éª¤
        # ================================================================

        # ã€æ­¥éª¤1ã€‘æ¸…é›¶æ¢¯åº¦ï¼ˆé‡è¦ï¼ï¼‰
        # ä¸ºä»€ä¹ˆï¼ŸPyTorch é»˜è®¤ä¼šç´¯ç§¯æ¢¯åº¦ï¼Œä¸æ¸…é›¶ä¼šå‡ºé”™
        optimizer.zero_grad()

        # ã€æ­¥éª¤2ã€‘å‰å‘ä¼ æ’­
        # è¾“å…¥å›¾åƒï¼Œå¾—åˆ°é¢„æµ‹ç»“æœ
        output = model(data)  # (batch_size, 10) - 10ä¸ªç±»åˆ«çš„æ¦‚ç‡

        # ã€æ­¥éª¤3ã€‘è®¡ç®—æŸå¤±
        # å¯¹æ¯”é¢„æµ‹å’Œæ­£ç¡®ç­”æ¡ˆ
        loss = criterion(output, target)

        # ã€æ­¥éª¤4aã€‘åå‘ä¼ æ’­ï¼ˆè‡ªåŠ¨è®¡ç®—æ¢¯åº¦ï¼ï¼‰
        # PyTorch è‡ªåŠ¨è®¡ç®—æ‰€æœ‰å‚æ•°çš„æ¢¯åº¦
        loss.backward()

        # ã€æ­¥éª¤4bã€‘æ›´æ–°æƒé‡
        # æ ¹æ®æ¢¯åº¦æ›´æ–°æ¨¡å‹å‚æ•°
        optimizer.step()

        # ================================================================
        # ğŸ”‘ ç»Ÿè®¡ä¿¡æ¯
        # ================================================================
        # loss.item() - å°† tensor è½¬ä¸º Python æ•°å­—
        #
        # ä¸ºä»€ä¹ˆè¦ç”¨ .item()ï¼Ÿ
        #   - loss æ˜¯ tensorï¼ŒåŒ…å«æ¢¯åº¦ä¿¡æ¯
        #   - ç›´æ¥ç´¯åŠ ä¼šå ç”¨å¤§é‡å†…å­˜ï¼ˆä¿ç•™è®¡ç®—å›¾ï¼‰
        #   - .item() åªå–æ•°å€¼ï¼Œé‡Šæ”¾æ¢¯åº¦ä¿¡æ¯
        #
        # å¯¹æ¯”ï¼š
        #   total_loss += loss        # âŒ å†…å­˜æ³„æ¼
        #   total_loss += loss.item() # âœ… æ­£ç¡®
        # ================================================================
        total_loss += loss.item()
        pred = output.argmax(dim=1, keepdim=True)
        correct += pred.eq(target.view_as(pred)).sum().item()
        total += target.size(0)

        # æ‰“å°è¿›åº¦
        if batch_idx % 100 == 0:
            print(f'  Batch [{batch_idx}/{len(train_loader)}] '
                  f'Loss: {loss.item():.4f}')

    avg_loss = total_loss / len(train_loader)
    accuracy = 100. * correct / total

    return avg_loss, accuracy


def evaluate(model, device, test_loader, criterion):
    """è¯„ä¼°æ¨¡å‹"""

    # ====================================================================
    # ğŸ”‘ è®¾ç½®ä¸ºè¯„ä¼°æ¨¡å¼
    # ====================================================================
    # model.eval() çš„ä½œç”¨ï¼š
    #
    # 1. Dropoutï¼š
    #    - è¯„ä¼°æ¨¡å¼ï¼šå…³é—­ï¼Œä¿ç•™æ‰€æœ‰ç¥ç»å…ƒ
    #    - ä¸å†éšæœºä¸¢å¼ƒï¼Œç¡®ä¿é¢„æµ‹ç¨³å®šä¸€è‡´
    #
    # 2. BatchNormï¼š
    #    - è¯„ä¼°æ¨¡å¼ï¼šä½¿ç”¨è®­ç»ƒæ—¶ç»Ÿè®¡çš„å›ºå®šå‡å€¼å’Œæ–¹å·®
    #    - ä¸å†æ›´æ–°ç»Ÿè®¡é‡
    #
    # 3. å…¶ä»–å±‚ï¼ˆConvã€Linearã€ReLUã€MaxPoolï¼‰ï¼š
    #    - æ— å½±å“ï¼Œè¡Œä¸ºç›¸åŒ
    #
    # ä¸ºä»€ä¹ˆéœ€è¦è¯„ä¼°æ¨¡å¼ï¼Ÿ
    #    - æµ‹è¯•æ—¶ä¸éœ€è¦ Dropoutï¼ˆéœ€è¦å®Œæ•´çš„ç½‘ç»œï¼‰
    #    - ç¡®ä¿æ¯æ¬¡é¢„æµ‹ç»“æœä¸€è‡´ï¼ˆä¸å—éšæœºæ€§å½±å“ï¼‰
    #    - ä½¿ç”¨è®­ç»ƒå¥½çš„ç»Ÿè®¡é‡ï¼Œè€Œä¸æ˜¯å½“å‰ batch çš„
    #
    # âš ï¸ é‡è¦ï¼šæµ‹è¯•/æ¨ç†æ—¶å¿…é¡»è°ƒç”¨ model.eval()ï¼
    # ====================================================================
    model.eval()

    test_loss = 0
    correct = 0

    # ====================================================================
    # ğŸ”‘ torch.no_grad() - å…³é—­æ¢¯åº¦è®¡ç®—
    # ====================================================================
    # ä½œç”¨ï¼š
    #    - ä¸è®¡ç®—æ¢¯åº¦ï¼ˆæµ‹è¯•æ—¶ä¸éœ€è¦åå‘ä¼ æ’­ï¼‰
    #    - èŠ‚çœå†…å­˜ï¼ˆæ¢¯åº¦ä¿¡æ¯å ç”¨å¤§é‡å†…å­˜ï¼‰
    #    - åŠ å¿«é€Ÿåº¦
    #
    # å¯¹æ¯”ï¼š
    #    - è®­ç»ƒæ—¶ï¼šéœ€è¦æ¢¯åº¦ï¼Œç”¨äºåå‘ä¼ æ’­æ›´æ–°æƒé‡
    #    - æµ‹è¯•æ—¶ï¼šä¸éœ€è¦æ¢¯åº¦ï¼Œåªåšå‰å‘ä¼ æ’­é¢„æµ‹
    # ====================================================================
    with torch.no_grad():
        for data, target in test_loader:
            data, target = data.to(device), target.to(device)

            # å‰å‘ä¼ æ’­
            output = model(data)

            # ç´¯è®¡æŸå¤±
            test_loss += criterion(output, target).item()

            # é¢„æµ‹
            pred = output.argmax(dim=1, keepdim=True)
            correct += pred.eq(target.view_as(pred)).sum().item()

    test_loss /= len(test_loader)
    accuracy = 100. * correct / len(test_loader.dataset)

    return test_loss, accuracy


def train_cnn():
    """
    å®Œæ•´è®­ç»ƒæµç¨‹

    ====================================================================
    ğŸ”‘ PyTorch è®­ç»ƒæµç¨‹
    ====================================================================

    1. å‡†å¤‡æ•°æ®
       - ä½¿ç”¨ DataLoaderï¼ˆè‡ªåŠ¨æ‰¹å¤„ç†ã€æ‰“ä¹±ã€å¤šçº¿ç¨‹ï¼‰

    2. å®šä¹‰æ¨¡å‹
       - ç»§æ‰¿ nn.Module
       - å®šä¹‰ __init__ å’Œ forward

    3. å®šä¹‰æŸå¤±å‡½æ•°å’Œä¼˜åŒ–å™¨
       - æŸå¤±å‡½æ•°ï¼šCrossEntropyLossï¼ˆåˆ†ç±»ï¼‰ã€MSELossï¼ˆå›å½’ï¼‰
       - ä¼˜åŒ–å™¨ï¼šAdamã€SGDã€RMSprop

    4. è®­ç»ƒå¾ªç¯
       for epoch in range(n_epochs):
           for batch in train_loader:
               optimizer.zero_grad()    # æ¸…é›¶æ¢¯åº¦
               output = model(batch)    # å‰å‘ä¼ æ’­
               loss = criterion(output, target)  # è®¡ç®—æŸå¤±
               loss.backward()          # åå‘ä¼ æ’­
               optimizer.step()         # æ›´æ–°æƒé‡

    5. è¯„ä¼°
       - model.eval() + torch.no_grad()

    ====================================================================
    """
    print("\n" + "=" * 70)
    print("3. è®­ç»ƒ CNN æ¨¡å‹ï¼ˆMNISTï¼‰")
    print("=" * 70)

    # ========== 1. æ£€æŸ¥ GPU ==========
    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
    print(f"\nä½¿ç”¨è®¾å¤‡: {device}")

    if torch.cuda.is_available():
        print(f"  GPU å‹å·: {torch.cuda.get_device_name(0)}")
        print(f"  GPU å†…å­˜: {torch.cuda.get_device_properties(0).total_memory / 1e9:.2f} GB")
    else:
        print("  âš ï¸ æ²¡æœ‰æ£€æµ‹åˆ° GPUï¼Œä½¿ç”¨ CPU è®­ç»ƒï¼ˆä¼šæ…¢å¾ˆå¤šï¼‰")

    # ========== 2. å‡†å¤‡æ•°æ® ==========
    print("\nå‡†å¤‡æ•°æ®...")

    # æ•°æ®é¢„å¤„ç†
    transform = transforms.Compose([
        transforms.ToTensor(),  # è½¬ä¸º Tensorï¼ŒèŒƒå›´ [0, 1]
        transforms.Normalize((0.1307,), (0.3081,))  # å½’ä¸€åŒ–ï¼ˆMNIST çš„å‡å€¼å’Œæ ‡å‡†å·®ï¼‰
    ])

    # ä¸‹è½½å¹¶åŠ è½½ MNIST æ•°æ®é›†
    train_dataset = datasets.MNIST('./data', train=True, download=True, transform=transform)
    test_dataset = datasets.MNIST('./data', train=False, transform=transform)

    # DataLoaderï¼ˆè‡ªåŠ¨æ‰¹å¤„ç† + æ‰“ä¹±ï¼‰
    batch_size = 64
    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)
    test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)

    print(f"  è®­ç»ƒé›†: {len(train_dataset)} å¼ å›¾ç‰‡")
    print(f"  æµ‹è¯•é›†: {len(test_dataset)} å¼ å›¾ç‰‡")
    print(f"  Batch size: {batch_size}")

    # ========== 3. åˆ›å»ºæ¨¡å‹ ==========
    print("\nåˆ›å»ºæ¨¡å‹...")
    model = SimpleCNN().to(device)  # ç§»åˆ° GPU/CPU

    # æ‰“å°æ¨¡å‹ç»“æ„
    print(model)

    # è®¡ç®—å‚æ•°é‡
    total_params = sum(p.numel() for p in model.parameters())
    trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)
    print(f"\n  æ€»å‚æ•°: {total_params:,}")
    print(f"  å¯è®­ç»ƒå‚æ•°: {trainable_params:,}")

    # ========== 4. å®šä¹‰æŸå¤±å‡½æ•°å’Œä¼˜åŒ–å™¨ ==========
    criterion = nn.CrossEntropyLoss()  # äº¤å‰ç†µæŸå¤±ï¼ˆåˆ†ç±»ä»»åŠ¡ï¼‰
    optimizer = optim.Adam(model.parameters(), lr=0.001)  # Adam ä¼˜åŒ–å™¨

    # ========== 5. è®­ç»ƒ ==========
    print("\nå¼€å§‹è®­ç»ƒ...")
    n_epochs = 5

    train_losses = []
    train_accs = []
    test_losses = []
    test_accs = []

    start_time = time.time()

    for epoch in range(1, n_epochs + 1):
        print(f"\nEpoch {epoch}/{n_epochs}")
        print("-" * 70)

        # è®­ç»ƒ
        train_loss, train_acc = train_one_epoch(
            model, device, train_loader, optimizer, criterion, epoch
        )

        # è¯„ä¼°
        test_loss, test_acc = evaluate(model, device, test_loader, criterion)

        # è®°å½•
        train_losses.append(train_loss)
        train_accs.append(train_acc)
        test_losses.append(test_loss)
        test_accs.append(test_acc)

        # æ‰“å°ç»“æœ
        print(f"\n  è®­ç»ƒé›† - Loss: {train_loss:.4f}, Acc: {train_acc:.2f}%")
        print(f"  æµ‹è¯•é›† - Loss: {test_loss:.4f}, Acc: {test_acc:.2f}%")

    total_time = time.time() - start_time
    print(f"\nè®­ç»ƒå®Œæˆï¼æ€»è€—æ—¶: {total_time:.2f} ç§’")

    # ========== 6. å¯è§†åŒ– ==========
    fig, axes = plt.subplots(1, 2, figsize=(14, 5))

    # Loss æ›²çº¿
    epochs_range = range(1, n_epochs + 1)
    axes[0].plot(epochs_range, train_losses, 'b-o', label='Training Loss', linewidth=2)
    axes[0].plot(epochs_range, test_losses, 'r-o', label='Test Loss', linewidth=2)
    axes[0].set_xlabel('Epoch', fontsize=11)
    axes[0].set_ylabel('Loss', fontsize=11)
    axes[0].set_title('Training and Test Loss', fontsize=12, fontweight='bold')
    axes[0].legend(fontsize=10)
    axes[0].grid(alpha=0.3)

    # Accuracy æ›²çº¿
    axes[1].plot(epochs_range, train_accs, 'b-o', label='Training Acc', linewidth=2)
    axes[1].plot(epochs_range, test_accs, 'r-o', label='Test Acc', linewidth=2)
    axes[1].set_xlabel('Epoch', fontsize=11)
    axes[1].set_ylabel('Accuracy (%)', fontsize=11)
    axes[1].set_title('Training and Test Accuracy', fontsize=12, fontweight='bold')
    axes[1].legend(fontsize=10)
    axes[1].grid(alpha=0.3)

    plt.tight_layout()
    plt.savefig('cnn_pytorch_training.png', dpi=100, bbox_inches='tight')
    print("\nğŸ“Š è®­ç»ƒæ›²çº¿å·²ä¿å­˜: cnn_pytorch_training.png")
    plt.close()

    # ========== 7. å¯è§†åŒ–é¢„æµ‹ç»“æœ ==========
    visualize_predictions(model, device, test_loader)

    return model, test_acc


def visualize_predictions(model, device, test_loader):
    """å¯è§†åŒ–æ¨¡å‹é¢„æµ‹"""
    print("\nå¯è§†åŒ–é¢„æµ‹ç»“æœ...")

    model.eval()

    # è·å–ä¸€æ‰¹æ•°æ®
    data_iter = iter(test_loader)
    images, labels = next(data_iter)
    images, labels = images.to(device), labels.to(device)

    # é¢„æµ‹
    with torch.no_grad():
        outputs = model(images)
        predictions = outputs.argmax(dim=1)

    # å¯è§†åŒ–å‰16å¼ å›¾ç‰‡
    fig, axes = plt.subplots(4, 4, figsize=(12, 12))
    axes = axes.ravel()

    for i in range(16):
        img = images[i].cpu().numpy().squeeze()
        true_label = labels[i].item()
        pred_label = predictions[i].item()

        axes[i].imshow(img, cmap='gray')

        # æ ‡é¢˜é¢œè‰²ï¼šé¢„æµ‹æ­£ç¡®=ç»¿è‰²ï¼Œé”™è¯¯=çº¢è‰²
        color = 'green' if true_label == pred_label else 'red'
        axes[i].set_title(f'True: {true_label}, Pred: {pred_label}',
                         color=color, fontsize=10, fontweight='bold')
        axes[i].axis('off')

    plt.tight_layout()
    plt.savefig('cnn_pytorch_predictions.png', dpi=100, bbox_inches='tight')
    print("ğŸ“Š é¢„æµ‹ç»“æœå·²ä¿å­˜: cnn_pytorch_predictions.png")
    plt.close()


# ==================== 4. PyTorch vs NumPy å¯¹æ¯” ====================
def compare_pytorch_vs_numpy():
    """
    å¯¹æ¯” PyTorch å’Œ NumPy ç‰ˆæœ¬

    ====================================================================
    ğŸ”‘ PyTorch vs NumPy
    ====================================================================

    NumPy ç‰ˆæœ¬ï¼š
    âœ… ä¼˜ç‚¹ï¼š
      - ç†è§£æ•°å­¦åŸç†
      - ä»é›¶å®ç°ï¼ŒæŒæ¡ç»†èŠ‚
      - ä¸ä¾èµ–æ·±åº¦å­¦ä¹ æ¡†æ¶

    âŒ ç¼ºç‚¹ï¼š
      - ä»£ç é‡å¤§ï¼ˆéœ€è¦æ‰‹å†™åå‘ä¼ æ’­ï¼‰
      - é€Ÿåº¦æ…¢ï¼ˆæ— GPUåŠ é€Ÿï¼‰
      - éš¾ä»¥æ‰©å±•ï¼ˆå¤æ‚æ¨¡å‹éš¾å®ç°ï¼‰
      - æ•°å€¼ä¸ç¨³å®šï¼ˆéœ€è¦æ‰‹åŠ¨å¤„ç†ï¼‰

    PyTorch ç‰ˆæœ¬ï¼š
    âœ… ä¼˜ç‚¹ï¼š
      - ä»£ç ç®€æ´ï¼ˆå‡ è¡Œæå®šï¼‰
      - GPU åŠ é€Ÿï¼ˆå¿«100å€ä»¥ä¸Šï¼‰
      - è‡ªåŠ¨å¾®åˆ†ï¼ˆä¸éœ€è¦æ‰‹å†™åå‘ä¼ æ’­ï¼‰
      - ç¨³å®šï¼ˆæ•°å€¼ä¼˜åŒ–åšå¾—å¥½ï¼‰
      - å·¥ä¸šç•Œæ ‡å‡†ï¼ˆå®é™…å·¥ä½œä¸­ä½¿ç”¨ï¼‰

    âŒ ç¼ºç‚¹ï¼š
      - æ¡†æ¶é»‘ç›’ï¼ˆä¸çŸ¥é“å†…éƒ¨ç»†èŠ‚ï¼‰
      - éœ€è¦å­¦ä¹ æ–°API

    å»ºè®®ï¼š
    - å­¦ä¹ é˜¶æ®µï¼šå…ˆçœ‹ NumPy ç‰ˆæœ¬ï¼ˆç†è§£åŸç†ï¼‰
    - å®è·µé˜¶æ®µï¼šç”¨ PyTorch ç‰ˆæœ¬ï¼ˆå®é™…åº”ç”¨ï¼‰

    ====================================================================
    """
    print("\n" + "=" * 70)
    print("4. PyTorch vs NumPy å¯¹æ¯”")
    print("=" * 70)

    print("""
æ€§èƒ½å¯¹æ¯”ï¼ˆMNIST æ•°å­—è¯†åˆ«ï¼‰ï¼š

+----------------+------------------+------------------+
|     æŒ‡æ ‡       |   NumPy ç‰ˆæœ¬     |  PyTorch ç‰ˆæœ¬    |
+----------------+------------------+------------------+
| ä»£ç é‡         | ~500 è¡Œ          | ~100 è¡Œ          |
| è®­ç»ƒæ—¶é—´       | ~10 åˆ†é’Ÿ (CPU)   | ~30 ç§’ (GPU)     |
| æµ‹è¯•å‡†ç¡®ç‡     | ~90%             | ~98%             |
| GPU æ”¯æŒ       | âŒ               | âœ…               |
| è‡ªåŠ¨å¾®åˆ†       | âŒ (æ‰‹å†™)        | âœ…               |
| å¯æ‰©å±•æ€§       | âŒ               | âœ…               |
| å·¥ä¸šåº”ç”¨       | âŒ               | âœ…               |
+----------------+------------------+------------------+

ä»£ç å¯¹æ¯”ï¼š

NumPy ç‰ˆæœ¬ï¼ˆå¤æ‚ï¼‰ï¼š
```python
# éœ€è¦æ‰‹å†™å‰å‘ä¼ æ’­
def forward(self, X):
    self.z1 = np.dot(X, self.W1) + self.b1
    self.a1 = relu(self.z1)
    self.z2 = np.dot(self.a1, self.W2) + self.b2
    self.a2 = softmax(self.z2)
    return self.a2

# éœ€è¦æ‰‹å†™åå‘ä¼ æ’­
def backward(self, X, y):
    m = X.shape[0]
    dz2 = self.a2 - y
    dW2 = (1/m) * np.dot(self.a1.T, dz2)
    db2 = (1/m) * np.sum(dz2, axis=0)
    da1 = np.dot(dz2, self.W2.T)
    dz1 = da1 * relu_derivative(self.z1)
    dW1 = (1/m) * np.dot(X.T, dz1)
    db1 = (1/m) * np.sum(dz1, axis=0)
    # ... æ›´æ–°æƒé‡
```

PyTorch ç‰ˆæœ¬ï¼ˆç®€æ´ï¼‰ï¼š
```python
# å®šä¹‰æ¨¡å‹
model = SimpleCNN()
criterion = nn.CrossEntropyLoss()
optimizer = optim.Adam(model.parameters())

# è®­ç»ƒï¼ˆè‡ªåŠ¨å¾®åˆ†ï¼ï¼‰
output = model(data)
loss = criterion(output, target)
loss.backward()          # â† è‡ªåŠ¨è®¡ç®—æ¢¯åº¦ï¼
optimizer.step()         # â† è‡ªåŠ¨æ›´æ–°æƒé‡ï¼
```

æ€»ç»“ï¼š
- å­¦ä¹ åŸç† â†’ ç”¨ NumPy
- å®é™…åº”ç”¨ â†’ ç”¨ PyTorch
- ä¸¤è€…ç»“åˆ â†’ æœ€ä½³ç†è§£ï¼
    """)


# ==================== 5. ä¸»ç¨‹åº ====================
def main():
    print("=" * 70)
    print("å·ç§¯ç¥ç»ç½‘ç»œ (CNN) - PyTorch å®ç°")
    print("=" * 70)

    # 1. PyTorch åŸºç¡€ç»„ä»¶
    demo_pytorch_conv()
    demo_pytorch_pooling()

    # 2. è®­ç»ƒå®Œæ•´æ¨¡å‹
    model, test_acc = train_cnn()

    # 3. å¯¹æ¯” PyTorch vs NumPy
    compare_pytorch_vs_numpy()

    # 4. æ€»ç»“
    print("\n" + "=" * 70)
    print("âœ… æ ¸å¿ƒè¦ç‚¹æ€»ç»“")
    print("=" * 70)
    print("""
1. PyTorch CNN åŸºç¡€ç»„ä»¶

   å·ç§¯å±‚ï¼š
   nn.Conv2d(in_channels, out_channels, kernel_size)

   æ± åŒ–å±‚ï¼š
   nn.MaxPool2d(kernel_size)

   å…¨è¿æ¥å±‚ï¼š
   nn.Linear(in_features, out_features)

   â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
   ğŸ¯ F (functional) vs nn (module) - é‡è¦ï¼
   â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

   PyTorch æä¾›ä¸¤ç§æ–¹å¼ä½¿ç”¨åŒä¸€ä¸ªæ“ä½œï¼š

   æ–¹å¼1ï¼šF (torch.nn.functional) - å‡½æ•°å¼
   ------------------------------------------------
   import torch.nn.functional as F

   output = F.max_pool2d(x, kernel_size=2)
   output = F.relu(x)

   ç‰¹ç‚¹ï¼š
   âœ… ç›´æ¥è°ƒç”¨å‡½æ•°
   âœ… é€‚åˆä¸´æ—¶ä½¿ç”¨
   âŒ æ¯æ¬¡éƒ½è¦ä¼ å‚æ•°

   æ–¹å¼2ï¼šnn (torch.nn) - æ¨¡å—å¼ï¼ˆæ¨èï¼‰
   ------------------------------------------------
   import torch.nn as nn

   # åœ¨ __init__ ä¸­å®šä¹‰
   self.pool = nn.MaxPool2d(kernel_size=2)
   self.relu = nn.ReLU()

   # åœ¨ forward ä¸­ä½¿ç”¨
   output = self.pool(x)
   output = self.relu(x)

   ç‰¹ç‚¹ï¼š
   âœ… å‚æ•°ä¿å­˜åœ¨å¯¹è±¡ä¸­
   âœ… å¯ä»¥é‡å¤ä½¿ç”¨
   âœ… ä»£ç æ›´æ¸…æ™°
   âœ… é€‚åˆåœ¨æ¨¡å‹ä¸­å®šä¹‰

   ğŸ“Œ ä½¿ç”¨å»ºè®®ï¼š
   ------------------------------------------------
   ã€æœ‰å‚æ•°çš„å±‚ã€‘â†’ ç”¨ nnï¼ˆå¿…é¡»ï¼‰
   - nn.Conv2d()       âœ… æœ‰æƒé‡å’Œåç½®
   - nn.Linear()       âœ… æœ‰æƒé‡å’Œåç½®
   - nn.BatchNorm2d()  âœ… æœ‰å‚æ•°

   ã€éœ€è¦é‡å¤ä½¿ç”¨çš„å±‚ã€‘â†’ ç”¨ nnï¼ˆæ¨èï¼‰
   - nn.MaxPool2d()    âœ… é¿å…é‡å¤å†™å‚æ•°
   - nn.Dropout()      âœ… ä¿å­˜é…ç½®

   ã€ç®€å•çš„æ¿€æ´»å‡½æ•°ã€‘â†’ ç”¨ Fï¼ˆæ¨èï¼‰
   - F.relu()          âœ… æ— å‚æ•°ï¼Œç®€æ´
   - F.sigmoid()       âœ… æ— å‚æ•°ï¼Œç®€æ´
   - F.softmax()       âœ… æ— å‚æ•°ï¼Œç®€æ´

   ã€ä¸´æ—¶æ“ä½œã€‘â†’ ç”¨ F
   - F.interpolate()   âœ… åŠ¨æ€å‚æ•°

   å®é™…ä¾‹å­ï¼š
   ------------------------------------------------
   class MyCNN(nn.Module):
       def __init__(self):
           super().__init__()
           # æœ‰å‚æ•°çš„ç”¨ nn
           self.conv1 = nn.Conv2d(1, 32, 3)   âœ…
           self.fc1 = nn.Linear(128, 10)      âœ…

           # éœ€è¦é‡å¤ä½¿ç”¨çš„ç”¨ nn
           self.pool = nn.MaxPool2d(2)        âœ…
           self.dropout = nn.Dropout(0.5)     âœ…

           # æ¿€æ´»å‡½æ•°å¯ä»¥ä¸å®šä¹‰ï¼ˆç”¨ Fï¼‰    âœ…

       def forward(self, x):
           x = self.conv1(x)
           x = F.relu(x)          âœ… ç®€æ´
           x = self.pool(x)       âœ… æ¸…æ™°

           x = x.view(-1, 128)
           x = self.fc1(x)
           x = self.dropout(x)
           return x

   â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

2. å®šä¹‰æ¨¡å‹ï¼ˆç»§æ‰¿ nn.Moduleï¼‰

   class MyCNN(nn.Module):
       def __init__(self):
           super().__init__()
           self.conv1 = nn.Conv2d(...)
           self.fc1 = nn.Linear(...)

       def forward(self, x):
           x = self.conv1(x)
           x = F.relu(x)
           return x

3. è®­ç»ƒæµç¨‹

   # å‡†å¤‡
   model = MyCNN().to(device)  # GPU åŠ é€Ÿ
   criterion = nn.CrossEntropyLoss()
   optimizer = optim.Adam(model.parameters())

   # è®­ç»ƒå¾ªç¯
   for epoch in range(n_epochs):
       for batch in train_loader:
           optimizer.zero_grad()     # æ¸…é›¶æ¢¯åº¦
           output = model(batch)     # å‰å‘ä¼ æ’­
           loss = criterion(output, target)  # è®¡ç®—æŸå¤±
           loss.backward()           # åå‘ä¼ æ’­ï¼ˆè‡ªåŠ¨ï¼ï¼‰
           optimizer.step()          # æ›´æ–°æƒé‡

4. GPU åŠ é€Ÿ

   # æ£€æŸ¥ GPU
   device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')

   # ç§»åŠ¨æ¨¡å‹å’Œæ•°æ®åˆ° GPU
   model = model.to(device)
   data = data.to(device)

   # é€Ÿåº¦æå‡ï¼šCPU 10åˆ†é’Ÿ â†’ GPU 30ç§’ï¼ˆ20å€ï¼‰

5. è®­ç»ƒæ¨¡å¼ vs è¯„ä¼°æ¨¡å¼ (é‡è¦ï¼)

   â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
   model.train() vs model.eval()
   â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

   +---------------+----------------------+----------------------+
   |     å±‚ç±»å‹     |   train() è®­ç»ƒæ¨¡å¼   |   eval() è¯„ä¼°æ¨¡å¼    |
   +---------------+----------------------+----------------------+
   | Dropout       | âœ… å¯ç”¨              | âŒ å…³é—­              |
   |               | éšæœºä¸¢å¼ƒç¥ç»å…ƒ        | ä¿ç•™æ‰€æœ‰ç¥ç»å…ƒ        |
   +---------------+----------------------+----------------------+
   | BatchNorm     | æ›´æ–°ç»Ÿè®¡é‡            | ä½¿ç”¨å›ºå®šç»Ÿè®¡é‡        |
   |               | ä½¿ç”¨å½“å‰batch         | ä½¿ç”¨è®­ç»ƒæ—¶çš„å‡å€¼æ–¹å·®  |
   +---------------+----------------------+----------------------+
   | Conv2d        | æ— å½±å“               | æ— å½±å“               |
   | Linear        | æ— å½±å“               | æ— å½±å“               |
   | ReLU          | æ— å½±å“               | æ— å½±å“               |
   | MaxPool       | æ— å½±å“               | æ— å½±å“               |
   +---------------+----------------------+----------------------+

   ä½¿ç”¨åœºæ™¯ï¼š
   ------------------------------------------------
   # è®­ç»ƒæ—¶
   model.train()  # å¿…é¡»è®¾ç½®
   for data, target in train_loader:
       output = model(data)  # Dropout ç”Ÿæ•ˆ
       loss.backward()       # è®¡ç®—æ¢¯åº¦
       optimizer.step()      # æ›´æ–°å‚æ•°

   # æµ‹è¯•/æ¨ç†æ—¶
   model.eval()           # å¿…é¡»è®¾ç½®
   with torch.no_grad():  # ä¸è®¡ç®—æ¢¯åº¦
       for data, target in test_loader:
           output = model(data)  # Dropout å…³é—­
           # åªåšé¢„æµ‹ï¼Œä¸æ›´æ–°å‚æ•°

   âš ï¸ å¸¸è§é”™è¯¯ï¼š
   ------------------------------------------------
   # âŒ é”™è¯¯ï¼šæµ‹è¯•æ—¶å¿˜è®° eval()
   # ç»“æœï¼šDropout ä»ç„¶å·¥ä½œï¼Œæ¯æ¬¡é¢„æµ‹ç»“æœä¸ä¸€æ ·
   for data, target in test_loader:
       output = model(data)  # é”™è¯¯ï¼

   # âœ… æ­£ç¡®ï¼šæµ‹è¯•æ—¶è°ƒç”¨ eval()
   model.eval()
   for data, target in test_loader:
       output = model(data)  # æ­£ç¡®ï¼

   â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

6. PyTorch vs NumPy

   NumPyï¼š
   - ç†è§£åŸç†ï¼ˆæ‰‹å†™å·ç§¯ã€åå‘ä¼ æ’­ï¼‰
   - ä»£ç é‡å¤§
   - é€Ÿåº¦æ…¢

   PyTorchï¼š
   - å·¥ä¸šå®è·µï¼ˆGPUã€è‡ªåŠ¨å¾®åˆ†ï¼‰
   - ä»£ç ç®€æ´
   - é€Ÿåº¦å¿«100å€+

7. å®è·µå»ºè®®

   å­¦ä¹ è·¯å¾„ï¼š
   1. å…ˆçœ‹ NumPy ç‰ˆæœ¬ï¼ˆç†è§£æ•°å­¦ï¼‰
   2. å†çœ‹ PyTorch ç‰ˆæœ¬ï¼ˆå­¦ä¹ æ¡†æ¶ï¼‰
   3. å¯¹æ¯”ä¸¤ä¸ªç‰ˆæœ¬ï¼ˆç†è§£æ¡†æ¶åšäº†ä»€ä¹ˆï¼‰

   å®é™…å·¥ä½œï¼š
   - 100% ç”¨ PyTorchï¼ˆæˆ– TensorFlowï¼‰
   - NumPy åªç”¨äºç†è§£åŸç†
    """)


def demo_channels_explained():
    """
    ä¸“é—¨æ¼”ç¤º in_channels å’Œ out_channels çš„å«ä¹‰
    """
    print("\n" + "=" * 70)
    print("ğŸ“š é¢å¤–è¯´æ˜ï¼šin_channels å’Œ out_channels è¯¦è§£")
    print("=" * 70)

    print("""
â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—
â•‘  ç†è§£ in_channels å’Œ out_channels çš„å…³é”®                          â•‘
â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

ğŸ”¹ in_channelsï¼ˆè¾“å…¥é€šé“æ•°ï¼‰
   â†’ å‘Šè¯‰å·ç§¯å±‚ï¼š"è¾“å…¥æ•°æ®æœ‰å‡ å±‚"
   â†’ å¿…é¡»åŒ¹é…å®é™…è¾“å…¥çš„é€šé“æ•°

ğŸ”¹ out_channelsï¼ˆè¾“å‡ºé€šé“æ•°ï¼‰
   â†’ å‘Šè¯‰å·ç§¯å±‚ï¼š"æˆ‘æƒ³è¦å‡ ä¸ªå·ç§¯æ ¸"
   â†’ å†³å®šè¾“å‡ºæœ‰å¤šå°‘ä¸ªç‰¹å¾å›¾

â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

ä¾‹å­1ï¼šç°åº¦å›¾ â†’ æå–32ç§ç‰¹å¾

    è¾“å…¥å›¾åƒ: (batch, 1, 28, 28)
              â†“
    Conv2d(in_channels=1, out_channels=32, kernel_size=3)
              â†“
    è¾“å‡ºç‰¹å¾: (batch, 32, 26, 26)

    è§£é‡Šï¼š
    - è¾“å…¥æœ‰ 1 ä¸ªé€šé“ï¼ˆç°åº¦ï¼‰
    - ä½¿ç”¨ 32 ä¸ªå·ç§¯æ ¸ï¼ˆæ¯ä¸ª3Ã—3ï¼‰
    - æ¯ä¸ªå·ç§¯æ ¸æ‰«ææ•´ä¸ªå›¾åƒï¼Œç”Ÿæˆ1ä¸ªç‰¹å¾å›¾
    - æ€»å…±å¾—åˆ° 32 ä¸ªç‰¹å¾å›¾

    æƒé‡å½¢çŠ¶: (32, 1, 3, 3)
              â†‘   â†‘  â†‘  â†‘
              â”‚   â”‚  â””â”€â”€â””â”€â”€ 3Ã—3 å·ç§¯æ ¸å¤§å°
              â”‚   â””â”€â”€â”€â”€â”€â”€â”€ 1ä¸ªè¾“å…¥é€šé“
              â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ 32ä¸ªå·ç§¯æ ¸

â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

ä¾‹å­2ï¼š32ä¸ªç‰¹å¾å›¾ â†’ æå–64ç§æ›´é«˜çº§ç‰¹å¾

    è¾“å…¥ç‰¹å¾: (batch, 32, 13, 13)
              â†“
    Conv2d(in_channels=32, out_channels=64, kernel_size=3)
              â†“
    è¾“å‡ºç‰¹å¾: (batch, 64, 11, 11)

    è§£é‡Šï¼š
    - è¾“å…¥æœ‰ 32 ä¸ªé€šé“ï¼ˆæ¥è‡ªä¸Šä¸€å±‚ï¼‰
    - ä½¿ç”¨ 64 ä¸ª"å®Œæ•´å·ç§¯æ ¸"
    - æ¯ä¸ª"å®Œæ•´å·ç§¯æ ¸"åŒ…å« 32 ä¸ª 3Ã—3 å­æ ¸
    - è¿‡ç¨‹ï¼š
      1. ç¬¬1ä¸ªå®Œæ•´å·ç§¯æ ¸ï¼šç”¨32ä¸ªå­æ ¸åˆ†åˆ«å·ç§¯32ä¸ªè¾“å…¥ â†’ ç›¸åŠ  â†’ å¾—åˆ°ç¬¬1ä¸ªè¾“å‡º
      2. ç¬¬2ä¸ªå®Œæ•´å·ç§¯æ ¸ï¼šç”¨32ä¸ªå­æ ¸åˆ†åˆ«å·ç§¯32ä¸ªè¾“å…¥ â†’ ç›¸åŠ  â†’ å¾—åˆ°ç¬¬2ä¸ªè¾“å‡º
      3. ...
      4. ç¬¬64ä¸ªå®Œæ•´å·ç§¯æ ¸ï¼š... â†’ å¾—åˆ°ç¬¬64ä¸ªè¾“å‡º
    - æ€»å…±å¾—åˆ° 64 ä¸ªç‰¹å¾å›¾

    æƒé‡å½¢çŠ¶: (64, 32, 3, 3)
              â†‘   â†‘   â†‘  â†‘
              â”‚   â”‚   â””â”€â”€â””â”€â”€ 3Ã—3 å·ç§¯æ ¸å¤§å°
              â”‚   â””â”€â”€â”€â”€â”€â”€â”€â”€ 32ä¸ªè¾“å…¥é€šé“ï¼ˆå¿…é¡»åŒ¹é…è¾“å…¥ï¼ï¼‰
              â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ 64ä¸ªå®Œæ•´å·ç§¯æ ¸

â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

ä¾‹å­3ï¼šRGB å½©è‰²å›¾ â†’ æå–64ç§ç‰¹å¾ï¼ˆå¸¸è§æƒ…å†µï¼‰

    è¾“å…¥å›¾åƒ: (batch, 3, 224, 224)  â† RGB ä¸‰é€šé“
              â†“
    Conv2d(in_channels=3, out_channels=64, kernel_size=3)
              â†“
    è¾“å‡ºç‰¹å¾: (batch, 64, 222, 222)

    è§£é‡Šï¼š
    - è¾“å…¥æœ‰ 3 ä¸ªé€šé“ï¼ˆRã€Gã€Bï¼‰
    - ä½¿ç”¨ 64 ä¸ª"å®Œæ•´å·ç§¯æ ¸"
    - æ¯ä¸ª"å®Œæ•´å·ç§¯æ ¸"åŒ…å« 3 ä¸ª 3Ã—3 å­æ ¸ï¼ˆå¯¹åº”RGBï¼‰
    - è¿‡ç¨‹ï¼š
      1. å¯¹Ré€šé“å·ç§¯ï¼ˆ3Ã—3ï¼‰â†’ ç»“æœ1
      2. å¯¹Gé€šé“å·ç§¯ï¼ˆ3Ã—3ï¼‰â†’ ç»“æœ2
      3. å¯¹Bé€šé“å·ç§¯ï¼ˆ3Ã—3ï¼‰â†’ ç»“æœ3
      4. ç»“æœ1 + ç»“æœ2 + ç»“æœ3 â†’ 1ä¸ªè¾“å‡ºç‰¹å¾å›¾
      5. é‡å¤64æ¬¡ â†’ 64ä¸ªè¾“å‡ºç‰¹å¾å›¾

    æƒé‡å½¢çŠ¶: (64, 3, 3, 3)
              â†‘   â†‘  â†‘  â†‘
              â”‚   â”‚  â””â”€â”€â””â”€â”€ 3Ã—3 å·ç§¯æ ¸å¤§å°
              â”‚   â””â”€â”€â”€â”€â”€â”€â”€ 3ä¸ªè¾“å…¥é€šé“ï¼ˆRã€Gã€Bï¼‰
              â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ 64ä¸ªå®Œæ•´å·ç§¯æ ¸

â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

ğŸ¯ å…³é”®è§„åˆ™ï¼š

1ï¸âƒ£  ä¸‹ä¸€å±‚çš„ in_channels å¿…é¡»ç­‰äºä¸Šä¸€å±‚çš„ out_channels

   âœ… æ­£ç¡®ï¼š
   conv1 = Conv2d(1, 32, 3)      # 1 â†’ 32
   conv2 = Conv2d(32, 64, 3)     # 32 â†’ 64 âœ“

   âŒ é”™è¯¯ï¼š
   conv1 = Conv2d(1, 32, 3)      # 1 â†’ 32
   conv2 = Conv2d(16, 64, 3)     # 16 â†’ 64 âœ— (åº”è¯¥æ˜¯32!)

2ï¸âƒ£  æ± åŒ–ä¸æ”¹å˜é€šé“æ•°ï¼Œåªæ”¹å˜å°ºå¯¸

   è¾“å…¥: (batch, 32, 26, 26)
         â†“ MaxPool2d(2, 2)
   è¾“å‡º: (batch, 32, 13, 13)  â† é€šé“æ•°è¿˜æ˜¯32

3ï¸âƒ£  out_channels è¶Šå¤šï¼Œæ¨¡å‹è¶Šå¼ºå¤§ï¼Œä½†è®¡ç®—é‡è¶Šå¤§

   - å°‘ï¼ˆ8-16ï¼‰ï¼šç®€å•ç‰¹å¾ï¼Œå¿«
   - ä¸­ï¼ˆ32-128ï¼‰ï¼šå¸¸ç”¨ï¼Œå¹³è¡¡
   - å¤šï¼ˆ256-512ï¼‰ï¼šå¤æ‚ç‰¹å¾ï¼Œæ…¢

â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    """)

    # å®é™…æ¼”ç¤º
    print("\nğŸ”¬ å®é™…ä»£ç æ¼”ç¤ºï¼š\n")

    # ä¾‹å­1ï¼šç°åº¦å›¾
    print("ä¾‹å­1ï¼šç°åº¦å›¾è¾“å…¥")
    x1 = torch.randn(2, 1, 28, 28)  # batch=2, channels=1
    conv1 = nn.Conv2d(1, 32, 3)
    out1 = conv1(x1)
    print(f"  è¾“å…¥å½¢çŠ¶: {x1.shape}")
    print(f"  å·ç§¯å±‚: Conv2d(in_channels=1, out_channels=32, kernel_size=3)")
    print(f"  æƒé‡å½¢çŠ¶: {conv1.weight.shape}  â† (32, 1, 3, 3)")
    print(f"  è¾“å‡ºå½¢çŠ¶: {out1.shape}")

    # ä¾‹å­2ï¼šå¤šé€šé“è¾“å…¥
    print("\nä¾‹å­2ï¼š32é€šé“è¾“å…¥")
    x2 = torch.randn(2, 32, 13, 13)  # batch=2, channels=32
    conv2 = nn.Conv2d(32, 64, 3)
    out2 = conv2(x2)
    print(f"  è¾“å…¥å½¢çŠ¶: {x2.shape}")
    print(f"  å·ç§¯å±‚: Conv2d(in_channels=32, out_channels=64, kernel_size=3)")
    print(f"  æƒé‡å½¢çŠ¶: {conv2.weight.shape}  â† (64, 32, 3, 3)")
    print(f"  è¾“å‡ºå½¢çŠ¶: {out2.shape}")

    # ä¾‹å­3ï¼šRGBå›¾åƒ
    print("\nä¾‹å­3ï¼šRGBå½©è‰²å›¾")
    x3 = torch.randn(2, 3, 224, 224)  # batch=2, channels=3 (RGB)
    conv3 = nn.Conv2d(3, 64, 3)
    out3 = conv3(x3)
    print(f"  è¾“å…¥å½¢çŠ¶: {x3.shape}")
    print(f"  å·ç§¯å±‚: Conv2d(in_channels=3, out_channels=64, kernel_size=3)")
    print(f"  æƒé‡å½¢çŠ¶: {conv3.weight.shape}  â† (64, 3, 3, 3)")
    print(f"  è¾“å‡ºå½¢çŠ¶: {out3.shape}")

    print("\n" + "=" * 70)


if __name__ == "__main__":
    # é¦–å…ˆè¿è¡Œä¸“é—¨çš„é€šé“æ•°è¯´æ˜ï¼ˆæ”¾åœ¨æœ€å‰é¢ï¼Œä¾¿äºç†è§£ï¼‰
    demo_channels_explained()

    # ç„¶åè¿è¡Œä¸»ç¨‹åº
    main()

    print("\nğŸ’¡ ç»ƒä¹ å»ºè®®:")
    print("  1. ä¿®æ”¹ç½‘ç»œç»“æ„ï¼ˆæ·»åŠ æ›´å¤šå·ç§¯å±‚ï¼‰")
    print("  2. åœ¨ CIFAR-10 æ•°æ®é›†ä¸Šè®­ç»ƒï¼ˆå½©è‰²å›¾åƒï¼‰")
    print("  3. ä½¿ç”¨é¢„è®­ç»ƒæ¨¡å‹ï¼ˆResNetã€VGGï¼‰")
    print("  4. å®ç°æ•°æ®å¢å¼ºï¼ˆtransformsï¼‰")
    print("  5. å¯¹æ¯”ä¸åŒä¼˜åŒ–å™¨ï¼ˆSGDã€Adamã€RMSpropï¼‰")
    print("  6. å¯è§†åŒ–å·ç§¯æ ¸å­¦åˆ°çš„ç‰¹å¾")
