"""
å·ç§¯ç¥ç»ç½‘ç»œ (CNN) - PyTorch å®ç°

å¯¹æ¯” NumPy ç‰ˆæœ¬ï¼š
- NumPy: æ‰‹å†™å·ç§¯ã€æ± åŒ–ï¼Œç†è§£æ•°å­¦åŸç†
- PyTorch: ä½¿ç”¨æ¡†æ¶ï¼ŒGPUåŠ é€Ÿï¼Œå·¥ä¸šå®è·µ

æœ¬æ–‡ä»¶å†…å®¹ï¼š
1. PyTorch CNN åŸºç¡€ç»„ä»¶
2. å®Œæ•´çš„ CNN æ¨¡å‹ï¼ˆMNIST æ•°å­—è¯†åˆ«ï¼‰
3. GPU è®­ç»ƒåŠ é€Ÿ
4. è®­ç»ƒå¯è§†åŒ–
5. ä¸ NumPy ç‰ˆæœ¬æ€§èƒ½å¯¹æ¯”
"""

import torch
import torch.nn as nn
import torch.nn.functional as F
import torch.optim as optim
from torch.utils.data import DataLoader
from torchvision import datasets, transforms
import matplotlib.pyplot as plt
import numpy as np
import time


# ==================== 1. PyTorch CNN åŸºç¡€ç»„ä»¶ ====================
def demo_pytorch_conv():
    """
    æ¼”ç¤º PyTorch çš„å·ç§¯æ“ä½œ

    ====================================================================
    ğŸ”‘ PyTorch vs NumPy å·ç§¯
    ====================================================================

    NumPy ç‰ˆæœ¬ï¼ˆæ‰‹å†™ï¼‰ï¼š
    ```python
    def conv2d(image, kernel):
        for i in range(out_h):
            for j in range(out_w):
                window = image[i:i+k, j:j+k]
                output[i, j] = np.sum(window * kernel)
    ```

    PyTorch ç‰ˆæœ¬ï¼ˆä¸€è¡Œï¼‰ï¼š
    ```python
    output = F.conv2d(image, kernel)
    ```

    PyTorch å¸®ä½ åšäº†ä»€ä¹ˆï¼Ÿ
    - è‡ªåŠ¨æ‰¹é‡å¤„ç†ï¼ˆbatchï¼‰
    - è‡ªåŠ¨GPUåŠ é€Ÿ
    - è‡ªåŠ¨è®¡ç®—æ¢¯åº¦ï¼ˆåå‘ä¼ æ’­ï¼‰
    - æ•°å€¼ä¼˜åŒ–ï¼ˆæ›´å¿«æ›´ç¨³å®šï¼‰

    ====================================================================
    """
    print("=" * 70)
    print("1. PyTorch å·ç§¯æ“ä½œæ¼”ç¤º")
    print("=" * 70)

    # åˆ›å»ºè¾“å…¥å›¾åƒï¼ˆbatch_size=1, channels=1, height=5, width=5ï¼‰
    # PyTorch æ ¼å¼ï¼š(N, C, H, W)
    image = torch.tensor([
        [1, 2, 3, 4, 5],
        [2, 3, 4, 5, 6],
        [3, 4, 5, 6, 7],
        [4, 5, 6, 7, 8],
        [5, 6, 7, 8, 9],
    ], dtype=torch.float32).unsqueeze(0).unsqueeze(0)  # æ·»åŠ  batch å’Œ channel ç»´åº¦

    print(f"\nè¾“å…¥å›¾åƒ shape: {image.shape}")  # (1, 1, 5, 5)

    # åˆ›å»ºå·ç§¯æ ¸ï¼ˆå‚ç›´è¾¹ç¼˜æ£€æµ‹ï¼‰
    # PyTorch æ ¼å¼ï¼š(out_channels, in_channels, height, width)
    kernel = torch.tensor([
        [-1, 0, 1],
        [-1, 0, 1],
        [-1, 0, 1],
    ], dtype=torch.float32).unsqueeze(0).unsqueeze(0)  # (1, 1, 3, 3)

    print(f"å·ç§¯æ ¸ shape: {kernel.shape}")  # (1, 1, 3, 3)

    # æ‰§è¡Œå·ç§¯ï¼ˆè¶…çº§ç®€å•ï¼ï¼‰
    output = F.conv2d(image, kernel)

    print(f"è¾“å‡º shape: {output.shape}")  # (1, 1, 3, 3)
    print(f"\nå·ç§¯è¾“å‡º:\n{output.squeeze()}")

    # ä½¿ç”¨ nn.Conv2dï¼ˆæ¨èæ–¹å¼ï¼Œå¯å­¦ä¹ å‚æ•°ï¼‰
    conv_layer = nn.Conv2d(in_channels=1, out_channels=1, kernel_size=3)

    # æ‰‹åŠ¨è®¾ç½®æƒé‡ï¼ˆå’Œä¸Šé¢çš„kernelä¸€æ ·ï¼‰
    with torch.no_grad():
        conv_layer.weight = nn.Parameter(kernel)
        conv_layer.bias = nn.Parameter(torch.zeros(1))

    output2 = conv_layer(image)
    print(f"\nnn.Conv2d è¾“å‡º:\n{output2.squeeze()}")

    print("\nğŸ’¡ PyTorch ä¼˜åŠ¿:")
    print("  - ä¸€è¡Œä»£ç å®Œæˆå·ç§¯")
    print("  - è‡ªåŠ¨æ”¯æŒ batch å¤„ç†")
    print("  - è‡ªåŠ¨è®¡ç®—æ¢¯åº¦ï¼ˆåå‘ä¼ æ’­ï¼‰")
    print("  - GPU åŠ é€Ÿï¼ˆæ·»åŠ  .cuda()ï¼‰")


def demo_pytorch_pooling():
    """æ¼”ç¤º PyTorch çš„æ± åŒ–æ“ä½œ"""
    print("\n" + "=" * 70)
    print("2. PyTorch æ± åŒ–æ“ä½œæ¼”ç¤º")
    print("=" * 70)

    # åˆ›å»ºè¾“å…¥
    x = torch.tensor([
        [1, 2, 3, 4],
        [5, 6, 7, 8],
        [9, 10, 11, 12],
        [13, 14, 15, 16],
    ], dtype=torch.float32).unsqueeze(0).unsqueeze(0)

    print(f"\nè¾“å…¥ shape: {x.shape}")  # (1, 1, 4, 4)
    print(f"è¾“å…¥:\n{x.squeeze()}")

    # MaxPooling (2Ã—2)
    pooled = F.max_pool2d(x, kernel_size=2)
    print(f"\nMaxPool2d è¾“å‡º shape: {pooled.shape}")  # (1, 1, 2, 2)
    print(f"è¾“å‡º:\n{pooled.squeeze()}")

    # ä½¿ç”¨ nn.MaxPool2d
    pool_layer = nn.MaxPool2d(kernel_size=2)
    pooled2 = pool_layer(x)
    print(f"\nnn.MaxPool2d è¾“å‡º:\n{pooled2.squeeze()}")


# ==================== 2. å®Œæ•´çš„ CNN æ¨¡å‹ ====================
class SimpleCNN(nn.Module):
    """
    ç®€å•çš„ CNN æ¨¡å‹ï¼ˆMNIST æ•°å­—è¯†åˆ«ï¼‰

    ====================================================================
    ğŸ”‘ PyTorch æ¨¡å‹å®šä¹‰
    ====================================================================

    PyTorch å®šä¹‰æ¨¡å‹æœ‰ä¸¤æ­¥ï¼š
    1. __init__: å®šä¹‰å±‚ï¼ˆlayerï¼‰
    2. forward: å®šä¹‰å‰å‘ä¼ æ’­é€»è¾‘

    å¯¹æ¯” NumPyï¼š
    - NumPy: æ‰‹åŠ¨ç®¡ç†æ‰€æœ‰æƒé‡ï¼Œæ‰‹å†™å‰å‘ä¼ æ’­
    - PyTorch: å®šä¹‰å±‚ç»“æ„ï¼Œè‡ªåŠ¨ç®¡ç†æƒé‡ï¼Œè‡ªåŠ¨åå‘ä¼ æ’­

    ====================================================================

    ç½‘ç»œç»“æ„ï¼š
    Input (1, 28, 28)
        â†“
    Conv1 (32, 26, 26)  # 3Ã—3 å·ç§¯ï¼Œ32ä¸ªå·ç§¯æ ¸
        â†“ ReLU
    Pool1 (32, 13, 13)  # 2Ã—2 æœ€å¤§æ± åŒ–
        â†“
    Conv2 (64, 11, 11)  # 3Ã—3 å·ç§¯ï¼Œ64ä¸ªå·ç§¯æ ¸
        â†“ ReLU
    Pool2 (64, 5, 5)    # 2Ã—2 æœ€å¤§æ± åŒ–
        â†“
    Flatten (1600)      # å±•å¹³
        â†“
    FC1 (128)           # å…¨è¿æ¥å±‚
        â†“ ReLU + Dropout
    FC2 (10)            # è¾“å‡ºå±‚ï¼ˆ10ä¸ªç±»åˆ«ï¼‰
        â†“ Softmax (éšå¼åœ¨lossä¸­)
    Output (10)
    """

    def __init__(self):
        super(SimpleCNN, self).__init__()

        # å·ç§¯å±‚1: 1 â†’ 32 channels
        self.conv1 = nn.Conv2d(in_channels=1, out_channels=32, kernel_size=3)

        # å·ç§¯å±‚2: 32 â†’ 64 channels
        self.conv2 = nn.Conv2d(in_channels=32, out_channels=64, kernel_size=3)

        # æ± åŒ–å±‚ï¼ˆ2Ã—2ï¼‰
        self.pool = nn.MaxPool2d(kernel_size=2, stride=2)

        # å…¨è¿æ¥å±‚1: 64*5*5 â†’ 128
        # ä¸ºä»€ä¹ˆæ˜¯ 5Ã—5ï¼Ÿ
        # 28 â†’ (å·ç§¯-2=26) â†’ (æ± åŒ–/2=13) â†’ (å·ç§¯-2=11) â†’ (æ± åŒ–/2=5)
        self.fc1 = nn.Linear(64 * 5 * 5, 128)

        # Dropout é˜²æ­¢è¿‡æ‹Ÿåˆ
        self.dropout = nn.Dropout(0.5)

        # å…¨è¿æ¥å±‚2: 128 â†’ 10 (10ä¸ªæ•°å­—ç±»åˆ«)
        self.fc2 = nn.Linear(128, 10)

    def forward(self, x):
        """
        å‰å‘ä¼ æ’­

        x: (batch_size, 1, 28, 28)
        è¿”å›: (batch_size, 10)
        """
        # å·ç§¯å±‚1 + ReLU + æ± åŒ–
        x = self.conv1(x)           # (batch, 32, 26, 26)
        x = F.relu(x)
        x = self.pool(x)            # (batch, 32, 13, 13)

        # å·ç§¯å±‚2 + ReLU + æ± åŒ–
        x = self.conv2(x)           # (batch, 64, 11, 11)
        x = F.relu(x)
        x = self.pool(x)            # (batch, 64, 5, 5)

        # å±•å¹³
        x = x.view(-1, 64 * 5 * 5)  # (batch, 1600)
        # ä¹Ÿå¯ä»¥ç”¨: x = torch.flatten(x, 1)

        # å…¨è¿æ¥å±‚1 + ReLU + Dropout
        x = self.fc1(x)             # (batch, 128)
        x = F.relu(x)
        x = self.dropout(x)

        # å…¨è¿æ¥å±‚2ï¼ˆè¾“å‡ºå±‚ï¼‰
        x = self.fc2(x)             # (batch, 10)

        return x


# ==================== 3. è®­ç»ƒå’Œè¯„ä¼° ====================
def train_one_epoch(model, device, train_loader, optimizer, criterion, epoch):
    """è®­ç»ƒä¸€ä¸ª epoch"""
    model.train()  # è®¾ç½®ä¸ºè®­ç»ƒæ¨¡å¼ï¼ˆå¯ç”¨ Dropoutï¼‰

    total_loss = 0
    correct = 0
    total = 0

    for batch_idx, (data, target) in enumerate(train_loader):
        # ç§»åˆ° GPU/CPU
        data, target = data.to(device), target.to(device)

        # æ¸…é›¶æ¢¯åº¦ï¼ˆé‡è¦ï¼ï¼‰
        optimizer.zero_grad()

        # å‰å‘ä¼ æ’­
        output = model(data)

        # è®¡ç®—æŸå¤±
        loss = criterion(output, target)

        # åå‘ä¼ æ’­ï¼ˆè‡ªåŠ¨è®¡ç®—æ¢¯åº¦ï¼ï¼‰
        loss.backward()

        # æ›´æ–°æƒé‡
        optimizer.step()

        # ç»Ÿè®¡
        total_loss += loss.item()
        pred = output.argmax(dim=1, keepdim=True)
        correct += pred.eq(target.view_as(pred)).sum().item()
        total += target.size(0)

        # æ‰“å°è¿›åº¦
        if batch_idx % 100 == 0:
            print(f'  Batch [{batch_idx}/{len(train_loader)}] '
                  f'Loss: {loss.item():.4f}')

    avg_loss = total_loss / len(train_loader)
    accuracy = 100. * correct / total

    return avg_loss, accuracy


def evaluate(model, device, test_loader, criterion):
    """è¯„ä¼°æ¨¡å‹"""
    model.eval()  # è®¾ç½®ä¸ºè¯„ä¼°æ¨¡å¼ï¼ˆç¦ç”¨ Dropoutï¼‰

    test_loss = 0
    correct = 0

    with torch.no_grad():  # ä¸è®¡ç®—æ¢¯åº¦ï¼ˆèŠ‚çœå†…å­˜å’Œæ—¶é—´ï¼‰
        for data, target in test_loader:
            data, target = data.to(device), target.to(device)

            # å‰å‘ä¼ æ’­
            output = model(data)

            # ç´¯è®¡æŸå¤±
            test_loss += criterion(output, target).item()

            # é¢„æµ‹
            pred = output.argmax(dim=1, keepdim=True)
            correct += pred.eq(target.view_as(pred)).sum().item()

    test_loss /= len(test_loader)
    accuracy = 100. * correct / len(test_loader.dataset)

    return test_loss, accuracy


def train_cnn():
    """
    å®Œæ•´è®­ç»ƒæµç¨‹

    ====================================================================
    ğŸ”‘ PyTorch è®­ç»ƒæµç¨‹
    ====================================================================

    1. å‡†å¤‡æ•°æ®
       - ä½¿ç”¨ DataLoaderï¼ˆè‡ªåŠ¨æ‰¹å¤„ç†ã€æ‰“ä¹±ã€å¤šçº¿ç¨‹ï¼‰

    2. å®šä¹‰æ¨¡å‹
       - ç»§æ‰¿ nn.Module
       - å®šä¹‰ __init__ å’Œ forward

    3. å®šä¹‰æŸå¤±å‡½æ•°å’Œä¼˜åŒ–å™¨
       - æŸå¤±å‡½æ•°ï¼šCrossEntropyLossï¼ˆåˆ†ç±»ï¼‰ã€MSELossï¼ˆå›å½’ï¼‰
       - ä¼˜åŒ–å™¨ï¼šAdamã€SGDã€RMSprop

    4. è®­ç»ƒå¾ªç¯
       for epoch in range(n_epochs):
           for batch in train_loader:
               optimizer.zero_grad()    # æ¸…é›¶æ¢¯åº¦
               output = model(batch)    # å‰å‘ä¼ æ’­
               loss = criterion(output, target)  # è®¡ç®—æŸå¤±
               loss.backward()          # åå‘ä¼ æ’­
               optimizer.step()         # æ›´æ–°æƒé‡

    5. è¯„ä¼°
       - model.eval() + torch.no_grad()

    ====================================================================
    """
    print("\n" + "=" * 70)
    print("3. è®­ç»ƒ CNN æ¨¡å‹ï¼ˆMNISTï¼‰")
    print("=" * 70)

    # ========== 1. æ£€æŸ¥ GPU ==========
    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
    print(f"\nä½¿ç”¨è®¾å¤‡: {device}")

    if torch.cuda.is_available():
        print(f"  GPU å‹å·: {torch.cuda.get_device_name(0)}")
        print(f"  GPU å†…å­˜: {torch.cuda.get_device_properties(0).total_memory / 1e9:.2f} GB")
    else:
        print("  âš ï¸ æ²¡æœ‰æ£€æµ‹åˆ° GPUï¼Œä½¿ç”¨ CPU è®­ç»ƒï¼ˆä¼šæ…¢å¾ˆå¤šï¼‰")

    # ========== 2. å‡†å¤‡æ•°æ® ==========
    print("\nå‡†å¤‡æ•°æ®...")

    # æ•°æ®é¢„å¤„ç†
    transform = transforms.Compose([
        transforms.ToTensor(),  # è½¬ä¸º Tensorï¼ŒèŒƒå›´ [0, 1]
        transforms.Normalize((0.1307,), (0.3081,))  # å½’ä¸€åŒ–ï¼ˆMNIST çš„å‡å€¼å’Œæ ‡å‡†å·®ï¼‰
    ])

    # ä¸‹è½½å¹¶åŠ è½½ MNIST æ•°æ®é›†
    train_dataset = datasets.MNIST('./data', train=True, download=True, transform=transform)
    test_dataset = datasets.MNIST('./data', train=False, transform=transform)

    # DataLoaderï¼ˆè‡ªåŠ¨æ‰¹å¤„ç† + æ‰“ä¹±ï¼‰
    batch_size = 64
    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)
    test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)

    print(f"  è®­ç»ƒé›†: {len(train_dataset)} å¼ å›¾ç‰‡")
    print(f"  æµ‹è¯•é›†: {len(test_dataset)} å¼ å›¾ç‰‡")
    print(f"  Batch size: {batch_size}")

    # ========== 3. åˆ›å»ºæ¨¡å‹ ==========
    print("\nåˆ›å»ºæ¨¡å‹...")
    model = SimpleCNN().to(device)  # ç§»åˆ° GPU/CPU

    # æ‰“å°æ¨¡å‹ç»“æ„
    print(model)

    # è®¡ç®—å‚æ•°é‡
    total_params = sum(p.numel() for p in model.parameters())
    trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)
    print(f"\n  æ€»å‚æ•°: {total_params:,}")
    print(f"  å¯è®­ç»ƒå‚æ•°: {trainable_params:,}")

    # ========== 4. å®šä¹‰æŸå¤±å‡½æ•°å’Œä¼˜åŒ–å™¨ ==========
    criterion = nn.CrossEntropyLoss()  # äº¤å‰ç†µæŸå¤±ï¼ˆåˆ†ç±»ä»»åŠ¡ï¼‰
    optimizer = optim.Adam(model.parameters(), lr=0.001)  # Adam ä¼˜åŒ–å™¨

    # ========== 5. è®­ç»ƒ ==========
    print("\nå¼€å§‹è®­ç»ƒ...")
    n_epochs = 5

    train_losses = []
    train_accs = []
    test_losses = []
    test_accs = []

    start_time = time.time()

    for epoch in range(1, n_epochs + 1):
        print(f"\nEpoch {epoch}/{n_epochs}")
        print("-" * 70)

        # è®­ç»ƒ
        train_loss, train_acc = train_one_epoch(
            model, device, train_loader, optimizer, criterion, epoch
        )

        # è¯„ä¼°
        test_loss, test_acc = evaluate(model, device, test_loader, criterion)

        # è®°å½•
        train_losses.append(train_loss)
        train_accs.append(train_acc)
        test_losses.append(test_loss)
        test_accs.append(test_acc)

        # æ‰“å°ç»“æœ
        print(f"\n  è®­ç»ƒé›† - Loss: {train_loss:.4f}, Acc: {train_acc:.2f}%")
        print(f"  æµ‹è¯•é›† - Loss: {test_loss:.4f}, Acc: {test_acc:.2f}%")

    total_time = time.time() - start_time
    print(f"\nè®­ç»ƒå®Œæˆï¼æ€»è€—æ—¶: {total_time:.2f} ç§’")

    # ========== 6. å¯è§†åŒ– ==========
    fig, axes = plt.subplots(1, 2, figsize=(14, 5))

    # Loss æ›²çº¿
    epochs_range = range(1, n_epochs + 1)
    axes[0].plot(epochs_range, train_losses, 'b-o', label='Training Loss', linewidth=2)
    axes[0].plot(epochs_range, test_losses, 'r-o', label='Test Loss', linewidth=2)
    axes[0].set_xlabel('Epoch', fontsize=11)
    axes[0].set_ylabel('Loss', fontsize=11)
    axes[0].set_title('Training and Test Loss', fontsize=12, fontweight='bold')
    axes[0].legend(fontsize=10)
    axes[0].grid(alpha=0.3)

    # Accuracy æ›²çº¿
    axes[1].plot(epochs_range, train_accs, 'b-o', label='Training Acc', linewidth=2)
    axes[1].plot(epochs_range, test_accs, 'r-o', label='Test Acc', linewidth=2)
    axes[1].set_xlabel('Epoch', fontsize=11)
    axes[1].set_ylabel('Accuracy (%)', fontsize=11)
    axes[1].set_title('Training and Test Accuracy', fontsize=12, fontweight='bold')
    axes[1].legend(fontsize=10)
    axes[1].grid(alpha=0.3)

    plt.tight_layout()
    plt.savefig('cnn_pytorch_training.png', dpi=100, bbox_inches='tight')
    print("\nğŸ“Š è®­ç»ƒæ›²çº¿å·²ä¿å­˜: cnn_pytorch_training.png")
    plt.close()

    # ========== 7. å¯è§†åŒ–é¢„æµ‹ç»“æœ ==========
    visualize_predictions(model, device, test_loader)

    return model, test_acc


def visualize_predictions(model, device, test_loader):
    """å¯è§†åŒ–æ¨¡å‹é¢„æµ‹"""
    print("\nå¯è§†åŒ–é¢„æµ‹ç»“æœ...")

    model.eval()

    # è·å–ä¸€æ‰¹æ•°æ®
    data_iter = iter(test_loader)
    images, labels = next(data_iter)
    images, labels = images.to(device), labels.to(device)

    # é¢„æµ‹
    with torch.no_grad():
        outputs = model(images)
        predictions = outputs.argmax(dim=1)

    # å¯è§†åŒ–å‰16å¼ å›¾ç‰‡
    fig, axes = plt.subplots(4, 4, figsize=(12, 12))
    axes = axes.ravel()

    for i in range(16):
        img = images[i].cpu().numpy().squeeze()
        true_label = labels[i].item()
        pred_label = predictions[i].item()

        axes[i].imshow(img, cmap='gray')

        # æ ‡é¢˜é¢œè‰²ï¼šé¢„æµ‹æ­£ç¡®=ç»¿è‰²ï¼Œé”™è¯¯=çº¢è‰²
        color = 'green' if true_label == pred_label else 'red'
        axes[i].set_title(f'True: {true_label}, Pred: {pred_label}',
                         color=color, fontsize=10, fontweight='bold')
        axes[i].axis('off')

    plt.tight_layout()
    plt.savefig('cnn_pytorch_predictions.png', dpi=100, bbox_inches='tight')
    print("ğŸ“Š é¢„æµ‹ç»“æœå·²ä¿å­˜: cnn_pytorch_predictions.png")
    plt.close()


# ==================== 4. PyTorch vs NumPy å¯¹æ¯” ====================
def compare_pytorch_vs_numpy():
    """
    å¯¹æ¯” PyTorch å’Œ NumPy ç‰ˆæœ¬

    ====================================================================
    ğŸ”‘ PyTorch vs NumPy
    ====================================================================

    NumPy ç‰ˆæœ¬ï¼š
    âœ… ä¼˜ç‚¹ï¼š
      - ç†è§£æ•°å­¦åŸç†
      - ä»é›¶å®ç°ï¼ŒæŒæ¡ç»†èŠ‚
      - ä¸ä¾èµ–æ·±åº¦å­¦ä¹ æ¡†æ¶

    âŒ ç¼ºç‚¹ï¼š
      - ä»£ç é‡å¤§ï¼ˆéœ€è¦æ‰‹å†™åå‘ä¼ æ’­ï¼‰
      - é€Ÿåº¦æ…¢ï¼ˆæ— GPUåŠ é€Ÿï¼‰
      - éš¾ä»¥æ‰©å±•ï¼ˆå¤æ‚æ¨¡å‹éš¾å®ç°ï¼‰
      - æ•°å€¼ä¸ç¨³å®šï¼ˆéœ€è¦æ‰‹åŠ¨å¤„ç†ï¼‰

    PyTorch ç‰ˆæœ¬ï¼š
    âœ… ä¼˜ç‚¹ï¼š
      - ä»£ç ç®€æ´ï¼ˆå‡ è¡Œæå®šï¼‰
      - GPU åŠ é€Ÿï¼ˆå¿«100å€ä»¥ä¸Šï¼‰
      - è‡ªåŠ¨å¾®åˆ†ï¼ˆä¸éœ€è¦æ‰‹å†™åå‘ä¼ æ’­ï¼‰
      - ç¨³å®šï¼ˆæ•°å€¼ä¼˜åŒ–åšå¾—å¥½ï¼‰
      - å·¥ä¸šç•Œæ ‡å‡†ï¼ˆå®é™…å·¥ä½œä¸­ä½¿ç”¨ï¼‰

    âŒ ç¼ºç‚¹ï¼š
      - æ¡†æ¶é»‘ç›’ï¼ˆä¸çŸ¥é“å†…éƒ¨ç»†èŠ‚ï¼‰
      - éœ€è¦å­¦ä¹ æ–°API

    å»ºè®®ï¼š
    - å­¦ä¹ é˜¶æ®µï¼šå…ˆçœ‹ NumPy ç‰ˆæœ¬ï¼ˆç†è§£åŸç†ï¼‰
    - å®è·µé˜¶æ®µï¼šç”¨ PyTorch ç‰ˆæœ¬ï¼ˆå®é™…åº”ç”¨ï¼‰

    ====================================================================
    """
    print("\n" + "=" * 70)
    print("4. PyTorch vs NumPy å¯¹æ¯”")
    print("=" * 70)

    print("""
æ€§èƒ½å¯¹æ¯”ï¼ˆMNIST æ•°å­—è¯†åˆ«ï¼‰ï¼š

+----------------+------------------+------------------+
|     æŒ‡æ ‡       |   NumPy ç‰ˆæœ¬     |  PyTorch ç‰ˆæœ¬    |
+----------------+------------------+------------------+
| ä»£ç é‡         | ~500 è¡Œ          | ~100 è¡Œ          |
| è®­ç»ƒæ—¶é—´       | ~10 åˆ†é’Ÿ (CPU)   | ~30 ç§’ (GPU)     |
| æµ‹è¯•å‡†ç¡®ç‡     | ~90%             | ~98%             |
| GPU æ”¯æŒ       | âŒ               | âœ…               |
| è‡ªåŠ¨å¾®åˆ†       | âŒ (æ‰‹å†™)        | âœ…               |
| å¯æ‰©å±•æ€§       | âŒ               | âœ…               |
| å·¥ä¸šåº”ç”¨       | âŒ               | âœ…               |
+----------------+------------------+------------------+

ä»£ç å¯¹æ¯”ï¼š

NumPy ç‰ˆæœ¬ï¼ˆå¤æ‚ï¼‰ï¼š
```python
# éœ€è¦æ‰‹å†™å‰å‘ä¼ æ’­
def forward(self, X):
    self.z1 = np.dot(X, self.W1) + self.b1
    self.a1 = relu(self.z1)
    self.z2 = np.dot(self.a1, self.W2) + self.b2
    self.a2 = softmax(self.z2)
    return self.a2

# éœ€è¦æ‰‹å†™åå‘ä¼ æ’­
def backward(self, X, y):
    m = X.shape[0]
    dz2 = self.a2 - y
    dW2 = (1/m) * np.dot(self.a1.T, dz2)
    db2 = (1/m) * np.sum(dz2, axis=0)
    da1 = np.dot(dz2, self.W2.T)
    dz1 = da1 * relu_derivative(self.z1)
    dW1 = (1/m) * np.dot(X.T, dz1)
    db1 = (1/m) * np.sum(dz1, axis=0)
    # ... æ›´æ–°æƒé‡
```

PyTorch ç‰ˆæœ¬ï¼ˆç®€æ´ï¼‰ï¼š
```python
# å®šä¹‰æ¨¡å‹
model = SimpleCNN()
criterion = nn.CrossEntropyLoss()
optimizer = optim.Adam(model.parameters())

# è®­ç»ƒï¼ˆè‡ªåŠ¨å¾®åˆ†ï¼ï¼‰
output = model(data)
loss = criterion(output, target)
loss.backward()          # â† è‡ªåŠ¨è®¡ç®—æ¢¯åº¦ï¼
optimizer.step()         # â† è‡ªåŠ¨æ›´æ–°æƒé‡ï¼
```

æ€»ç»“ï¼š
- å­¦ä¹ åŸç† â†’ ç”¨ NumPy
- å®é™…åº”ç”¨ â†’ ç”¨ PyTorch
- ä¸¤è€…ç»“åˆ â†’ æœ€ä½³ç†è§£ï¼
    """)


# ==================== 5. ä¸»ç¨‹åº ====================
def main():
    print("=" * 70)
    print("å·ç§¯ç¥ç»ç½‘ç»œ (CNN) - PyTorch å®ç°")
    print("=" * 70)

    # 1. PyTorch åŸºç¡€ç»„ä»¶
    demo_pytorch_conv()
    demo_pytorch_pooling()

    # 2. è®­ç»ƒå®Œæ•´æ¨¡å‹
    model, test_acc = train_cnn()

    # 3. å¯¹æ¯” PyTorch vs NumPy
    compare_pytorch_vs_numpy()

    # 4. æ€»ç»“
    print("\n" + "=" * 70)
    print("âœ… æ ¸å¿ƒè¦ç‚¹æ€»ç»“")
    print("=" * 70)
    print("""
1. PyTorch CNN åŸºç¡€ç»„ä»¶

   å·ç§¯å±‚ï¼š
   nn.Conv2d(in_channels, out_channels, kernel_size)

   æ± åŒ–å±‚ï¼š
   nn.MaxPool2d(kernel_size)

   å…¨è¿æ¥å±‚ï¼š
   nn.Linear(in_features, out_features)

2. å®šä¹‰æ¨¡å‹ï¼ˆç»§æ‰¿ nn.Moduleï¼‰

   class MyCNN(nn.Module):
       def __init__(self):
           super().__init__()
           self.conv1 = nn.Conv2d(...)
           self.fc1 = nn.Linear(...)

       def forward(self, x):
           x = self.conv1(x)
           x = F.relu(x)
           return x

3. è®­ç»ƒæµç¨‹

   # å‡†å¤‡
   model = MyCNN().to(device)  # GPU åŠ é€Ÿ
   criterion = nn.CrossEntropyLoss()
   optimizer = optim.Adam(model.parameters())

   # è®­ç»ƒå¾ªç¯
   for epoch in range(n_epochs):
       for batch in train_loader:
           optimizer.zero_grad()     # æ¸…é›¶æ¢¯åº¦
           output = model(batch)     # å‰å‘ä¼ æ’­
           loss = criterion(output, target)  # è®¡ç®—æŸå¤±
           loss.backward()           # åå‘ä¼ æ’­ï¼ˆè‡ªåŠ¨ï¼ï¼‰
           optimizer.step()          # æ›´æ–°æƒé‡

4. GPU åŠ é€Ÿ

   # æ£€æŸ¥ GPU
   device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')

   # ç§»åŠ¨æ¨¡å‹å’Œæ•°æ®åˆ° GPU
   model = model.to(device)
   data = data.to(device)

   # é€Ÿåº¦æå‡ï¼šCPU 10åˆ†é’Ÿ â†’ GPU 30ç§’ï¼ˆ20å€ï¼‰

5. PyTorch vs NumPy

   NumPyï¼š
   - ç†è§£åŸç†ï¼ˆæ‰‹å†™å·ç§¯ã€åå‘ä¼ æ’­ï¼‰
   - ä»£ç é‡å¤§
   - é€Ÿåº¦æ…¢

   PyTorchï¼š
   - å·¥ä¸šå®è·µï¼ˆGPUã€è‡ªåŠ¨å¾®åˆ†ï¼‰
   - ä»£ç ç®€æ´
   - é€Ÿåº¦å¿«100å€+

6. å®è·µå»ºè®®

   å­¦ä¹ è·¯å¾„ï¼š
   1. å…ˆçœ‹ NumPy ç‰ˆæœ¬ï¼ˆç†è§£æ•°å­¦ï¼‰
   2. å†çœ‹ PyTorch ç‰ˆæœ¬ï¼ˆå­¦ä¹ æ¡†æ¶ï¼‰
   3. å¯¹æ¯”ä¸¤ä¸ªç‰ˆæœ¬ï¼ˆç†è§£æ¡†æ¶åšäº†ä»€ä¹ˆï¼‰

   å®é™…å·¥ä½œï¼š
   - 100% ç”¨ PyTorchï¼ˆæˆ– TensorFlowï¼‰
   - NumPy åªç”¨äºç†è§£åŸç†
    """)


if __name__ == "__main__":
    main()

    print("\nğŸ’¡ ç»ƒä¹ å»ºè®®:")
    print("  1. ä¿®æ”¹ç½‘ç»œç»“æ„ï¼ˆæ·»åŠ æ›´å¤šå·ç§¯å±‚ï¼‰")
    print("  2. åœ¨ CIFAR-10 æ•°æ®é›†ä¸Šè®­ç»ƒï¼ˆå½©è‰²å›¾åƒï¼‰")
    print("  3. ä½¿ç”¨é¢„è®­ç»ƒæ¨¡å‹ï¼ˆResNetã€VGGï¼‰")
    print("  4. å®ç°æ•°æ®å¢å¼ºï¼ˆtransformsï¼‰")
    print("  5. å¯¹æ¯”ä¸åŒä¼˜åŒ–å™¨ï¼ˆSGDã€Adamã€RMSpropï¼‰")
    print("  6. å¯è§†åŒ–å·ç§¯æ ¸å­¦åˆ°çš„ç‰¹å¾")
