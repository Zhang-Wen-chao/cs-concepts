"""
Optimization and Regularization Techniques

Problem: How to train deep networks effectively and prevent overfitting?
Goal: Master the essential techniques for training robust neural networks

Core Concepts:
1. Optimizers: How to update weights (SGD, Momentum, Adam)
2. Learning Rate Scheduling: Adaptive learning rates
3. Regularization: Prevent overfitting (Dropout, BatchNorm, L2)
4. Gradient Clipping: Prevent exploding gradients
5. Weight Initialization: Start with good weights

Why These Matter?
- Bad optimizer â†’ Slow convergence or stuck in local minima
- Wrong learning rate â†’ Divergence or too slow
- No regularization â†’ Overfitting (good on training, bad on test)
- Poor initialization â†’ Dead neurons or exploding activations
"""

import numpy as np
import matplotlib.pyplot as plt
from sklearn.datasets import make_moons, make_circles
from sklearn.model_selection import train_test_split
import warnings
warnings.filterwarnings('ignore')


# ==================== 1. Optimizers ====================
class SGD:
    """
    Stochastic Gradient Descent (æœ€åŸºç¡€çš„ä¼˜åŒ–å™¨)

    ====================================================================
    ğŸ”‘ What is SGD?
    ====================================================================

    SGD = éšæœºæ¢¯åº¦ä¸‹é™ï¼Œæœ€ç®€å•çš„ä¼˜åŒ–æ–¹æ³•

    æ›´æ–°è§„åˆ™ï¼š
        Î¸ = Î¸ - learning_rate Ã— gradient

    ä¾‹å­ï¼š
        æƒé‡ = 0.5
        æ¢¯åº¦ = 0.3ï¼ˆè¡¨ç¤ºå¾€æ­£æ–¹å‘èµ°ä¼šå¢å¤§æŸå¤±ï¼‰
        å­¦ä¹ ç‡ = 0.1

        æ–°æƒé‡ = 0.5 - 0.1 Ã— 0.3 = 0.47

    ====================================================================
    ğŸ”‘ SGD çš„é—®é¢˜
    ====================================================================

    é—®é¢˜1ï¼šå­¦ä¹ ç‡å›ºå®š
        - å¤ªå¤§ï¼šéœ‡è¡ï¼Œä¸æ”¶æ•›
        - å¤ªå°ï¼šæ”¶æ•›å¤ªæ…¢

    é—®é¢˜2ï¼šæ‰€æœ‰å‚æ•°ç”¨åŒä¸€ä¸ªå­¦ä¹ ç‡
        - æœ‰äº›å‚æ•°éœ€è¦å¤§æ­¥èµ°
        - æœ‰äº›å‚æ•°éœ€è¦å°æ­¥èµ°

    é—®é¢˜3ï¼šå®¹æ˜“å¡åœ¨éç‚¹ï¼ˆsaddle pointï¼‰
        - æ¢¯åº¦æ¥è¿‘0çš„å¹³å¦åŒºåŸŸ
        - æ— æ³•å¿«é€Ÿé€ƒç¦»

    ====================================================================
    """

    def __init__(self, learning_rate=0.01):
        self.lr = learning_rate

    def update(self, params, grads):
        """
        æ›´æ–°å‚æ•°

        params: å‚æ•°åˆ—è¡¨ [W1, b1, W2, b2, ...]
        grads: æ¢¯åº¦åˆ—è¡¨ [dW1, db1, dW2, db2, ...]
        """
        updated_params = []
        for param, grad in zip(params, grads):
            # ç®€å•çš„æ¢¯åº¦ä¸‹é™
            param = param - self.lr * grad
            updated_params.append(param)
        return updated_params


class Momentum:
    """
    Momentum ä¼˜åŒ–å™¨ï¼ˆå¸¦åŠ¨é‡ï¼‰

    ====================================================================
    ğŸ”‘ What is Momentum?
    ====================================================================

    Momentum = åŠ¨é‡ï¼Œåƒæ»šé›ªçƒä¸€æ ·ç´¯ç§¯æ¢¯åº¦

    æ ¸å¿ƒæ€æƒ³ï¼š
        - ä¸åªçœ‹å½“å‰æ¢¯åº¦
        - è¿˜è¦è€ƒè™‘ä¹‹å‰çš„"æƒ¯æ€§"

    æ›´æ–°è§„åˆ™ï¼š
        velocity = momentum Ã— velocity - learning_rate Ã— gradient
        Î¸ = Î¸ + velocity

    ç±»æ¯”ï¼šæ»šçƒä¸‹å±±
        - ä¸æ˜¯æ¯æ¬¡éƒ½æŒ‰å½“å‰å¡åº¦èµ°
        - è€Œæ˜¯ç§¯ç´¯äº†é€Ÿåº¦ï¼Œæœ‰æƒ¯æ€§
        - å¯ä»¥å†²è¿‡å°å‘ï¼ˆå±€éƒ¨æœ€å°å€¼ï¼‰

    ====================================================================
    ğŸ”‘ Momentum vs SGD
    ====================================================================

    SGDï¼š
        åƒèµ°è·¯ï¼Œæ¯æ­¥éƒ½é‡æ–°çœ‹æ–¹å‘
        é‡åˆ°å°å‘å°±åœä¸‹äº†

    Momentumï¼š
        åƒæ»šçƒï¼Œæœ‰æƒ¯æ€§
        å¯ä»¥å†²è¿‡å°å‘
        åœ¨ä¸€è‡´çš„æ–¹å‘ä¸ŠåŠ é€Ÿ

    å‚æ•° Î²ï¼ˆmomentum ç³»æ•°ï¼‰ï¼š
        - Î² = 0: é€€åŒ–ä¸º SGD
        - Î² = 0.9: å¸¸ç”¨å€¼ï¼Œä¿ç•™90%çš„æ—§é€Ÿåº¦
        - Î² = 0.99: æ›´å¼ºçš„æƒ¯æ€§

    ====================================================================
    """

    def __init__(self, learning_rate=0.01, momentum=0.9):
        self.lr = learning_rate
        self.momentum = momentum
        self.velocity = None

    def update(self, params, grads):
        if self.velocity is None:
            # ç¬¬ä¸€æ¬¡ï¼Œåˆå§‹åŒ–é€Ÿåº¦ä¸º0
            self.velocity = [np.zeros_like(p) for p in params]

        updated_params = []
        for i, (param, grad) in enumerate(zip(params, grads)):
            # æ›´æ–°é€Ÿåº¦ï¼šæ—§é€Ÿåº¦ Ã— momentum - å­¦ä¹ ç‡ Ã— æ¢¯åº¦
            self.velocity[i] = self.momentum * self.velocity[i] - self.lr * grad

            # æ›´æ–°å‚æ•°
            param = param + self.velocity[i]
            updated_params.append(param)

        return updated_params


class RMSprop:
    """
    RMSprop ä¼˜åŒ–å™¨ï¼ˆå‡æ–¹æ ¹ä¼ æ’­ï¼‰

    ====================================================================
    ğŸ”‘ What is RMSprop?
    ====================================================================

    RMSprop = Root Mean Square Propagation

    æ ¸å¿ƒæ€æƒ³ï¼š
        - è‡ªé€‚åº”å­¦ä¹ ç‡
        - å¯¹é¢‘ç¹å˜åŒ–çš„å‚æ•°ç”¨å°å­¦ä¹ ç‡
        - å¯¹ç¨€ç–å˜åŒ–çš„å‚æ•°ç”¨å¤§å­¦ä¹ ç‡

    æ›´æ–°è§„åˆ™ï¼š
        cache = decay Ã— cache + (1-decay) Ã— gradientÂ²
        Î¸ = Î¸ - learning_rate Ã— gradient / (âˆšcache + Îµ)

    è§£é‡Šï¼š
        - cacheï¼šç´¯ç§¯çš„æ¢¯åº¦å¹³æ–¹ï¼ˆä»£è¡¨æ¢¯åº¦çš„"å†å²å¤§å°"ï¼‰
        - æ¢¯åº¦å¤§çš„å‚æ•° â†’ cacheå¤§ â†’ é™¤ä»¥å¤§æ•° â†’ æ­¥é•¿å˜å°
        - æ¢¯åº¦å°çš„å‚æ•° â†’ cacheå° â†’ é™¤ä»¥å°æ•° â†’ æ­¥é•¿å˜å¤§

    ====================================================================
    ğŸ”‘ ä¸ºä»€ä¹ˆæœ‰æ•ˆï¼Ÿ
    ====================================================================

    é—®é¢˜åœºæ™¯ï¼š
        å‚æ•°1ï¼šæ¢¯åº¦ä¸€ç›´å¾ˆå¤§ï¼ˆéœ‡è¡ï¼‰
        å‚æ•°2ï¼šæ¢¯åº¦ä¸€ç›´å¾ˆå°ï¼ˆæ…¢ï¼‰

    SGDï¼š
        å‚æ•°1 â†’ æ­¥é•¿å¤§ â†’ éœ‡è¡ä¸æ”¶æ•›
        å‚æ•°2 â†’ æ­¥é•¿å° â†’ æ”¶æ•›å¤ªæ…¢

    RMSpropï¼š
        å‚æ•°1 â†’ cacheå¤§ â†’ è‡ªåŠ¨å‡å°æ­¥é•¿ â†’ å¹³ç¨³
        å‚æ•°2 â†’ cacheå° â†’ è‡ªåŠ¨å¢å¤§æ­¥é•¿ â†’ åŠ é€Ÿ

    ====================================================================
    """

    def __init__(self, learning_rate=0.01, decay=0.9, epsilon=1e-8):
        self.lr = learning_rate
        self.decay = decay
        self.epsilon = epsilon
        self.cache = None

    def update(self, params, grads):
        if self.cache is None:
            self.cache = [np.zeros_like(p) for p in params]

        updated_params = []
        for i, (param, grad) in enumerate(zip(params, grads)):
            # ç´¯ç§¯æ¢¯åº¦å¹³æ–¹
            self.cache[i] = self.decay * self.cache[i] + (1 - self.decay) * grad**2

            # è‡ªé€‚åº”å­¦ä¹ ç‡æ›´æ–°
            param = param - self.lr * grad / (np.sqrt(self.cache[i]) + self.epsilon)
            updated_params.append(param)

        return updated_params


class Adam:
    """
    Adam ä¼˜åŒ–å™¨ï¼ˆAdaptive Moment Estimationï¼‰

    ====================================================================
    ğŸ”‘ What is Adam?
    ====================================================================

    Adam = Momentum + RMSprop çš„ç»“åˆä½“ï¼ˆæœ€æµè¡Œçš„ä¼˜åŒ–å™¨ï¼ï¼‰

    ç»“åˆäº†ä¸¤ä¸ªä¼˜ç‚¹ï¼š
        1. Momentumï¼šä¿ç•™æ¢¯åº¦çš„æ–¹å‘ï¼ˆä¸€é˜¶çŸ©ï¼‰
        2. RMSpropï¼šè‡ªé€‚åº”å­¦ä¹ ç‡ï¼ˆäºŒé˜¶çŸ©ï¼‰

    æ›´æ–°è§„åˆ™ï¼š
        # ä¸€é˜¶çŸ©ï¼ˆåŠ¨é‡ï¼‰
        m = Î²â‚ Ã— m + (1-Î²â‚) Ã— gradient

        # äºŒé˜¶çŸ©ï¼ˆè‡ªé€‚åº”å­¦ä¹ ç‡ï¼‰
        v = Î²â‚‚ Ã— v + (1-Î²â‚‚) Ã— gradientÂ²

        # åå·®ä¿®æ­£ï¼ˆbias correctionï¼‰
        m_hat = m / (1 - Î²â‚áµ—)
        v_hat = v / (1 - Î²â‚‚áµ—)

        # æ›´æ–°å‚æ•°
        Î¸ = Î¸ - learning_rate Ã— m_hat / (âˆšv_hat + Îµ)

    ====================================================================
    ğŸ”‘ ä¸ºä»€ä¹ˆ Adam è¿™ä¹ˆå¥½ï¼Ÿ
    ====================================================================

    1. è‡ªé€‚åº”å­¦ä¹ ç‡ï¼ˆæ¥è‡ª RMSpropï¼‰
       - ä¸åŒå‚æ•°è‡ªåŠ¨è°ƒæ•´æ­¥é•¿
       - ä¸éœ€è¦æ‰‹åŠ¨è°ƒå­¦ä¹ ç‡

    2. åŠ¨é‡åŠ é€Ÿï¼ˆæ¥è‡ª Momentumï¼‰
       - åŠ é€Ÿæ”¶æ•›
       - å¯ä»¥å†²è¿‡å°å‘

    3. åå·®ä¿®æ­£
       - å¼€å§‹æ—¶ m å’Œ v æ¥è¿‘0ï¼ˆåˆå§‹åŒ–ï¼‰
       - ä¿®æ­£åæ›´å‡†ç¡®

    4. é²æ£’æ€§å¼º
       - é»˜è®¤å‚æ•° (Î²â‚=0.9, Î²â‚‚=0.999) é€šå¸¸å°±å¾ˆå¥½
       - é€‚ç”¨äºå¤§å¤šæ•°é—®é¢˜

    ====================================================================
    ğŸ”‘ è¶…å‚æ•°é€‰æ‹©
    ====================================================================

    learning_rate (Î±):
        - é»˜è®¤ï¼š0.001
        - èŒƒå›´ï¼š0.0001 ~ 0.01

    Î²â‚ (momentum):
        - é»˜è®¤ï¼š0.9
        - ä¸€èˆ¬ä¸éœ€è¦æ”¹

    Î²â‚‚ (RMSprop decay):
        - é»˜è®¤ï¼š0.999
        - ä¸€èˆ¬ä¸éœ€è¦æ”¹

    Îµ (æ•°å€¼ç¨³å®šæ€§):
        - é»˜è®¤ï¼š1e-8
        - é˜²æ­¢é™¤ä»¥0

    ====================================================================
    """

    def __init__(self, learning_rate=0.001, beta1=0.9, beta2=0.999, epsilon=1e-8):
        self.lr = learning_rate
        self.beta1 = beta1
        self.beta2 = beta2
        self.epsilon = epsilon
        self.m = None  # ä¸€é˜¶çŸ©ï¼ˆåŠ¨é‡ï¼‰
        self.v = None  # äºŒé˜¶çŸ©ï¼ˆè‡ªé€‚åº”å­¦ä¹ ç‡ï¼‰
        self.t = 0     # æ—¶é—´æ­¥

    def update(self, params, grads):
        if self.m is None:
            self.m = [np.zeros_like(p) for p in params]
            self.v = [np.zeros_like(p) for p in params]

        self.t += 1  # æ—¶é—´æ­¥ +1

        updated_params = []
        for i, (param, grad) in enumerate(zip(params, grads)):
            # æ›´æ–°ä¸€é˜¶çŸ©ï¼ˆåŠ¨é‡ï¼‰
            self.m[i] = self.beta1 * self.m[i] + (1 - self.beta1) * grad

            # æ›´æ–°äºŒé˜¶çŸ©ï¼ˆè‡ªé€‚åº”å­¦ä¹ ç‡ï¼‰
            self.v[i] = self.beta2 * self.v[i] + (1 - self.beta2) * grad**2

            # åå·®ä¿®æ­£
            m_hat = self.m[i] / (1 - self.beta1**self.t)
            v_hat = self.v[i] / (1 - self.beta2**self.t)

            # æ›´æ–°å‚æ•°
            param = param - self.lr * m_hat / (np.sqrt(v_hat) + self.epsilon)
            updated_params.append(param)

        return updated_params


def compare_optimizers():
    """æ¯”è¾ƒä¸åŒä¼˜åŒ–å™¨çš„æ”¶æ•›é€Ÿåº¦"""
    print("=" * 70)
    print("Comparison: Different Optimizers")
    print("=" * 70)

    # ç”Ÿæˆéå‡¸ä¼˜åŒ–é—®é¢˜ï¼ˆBealeå‡½æ•°ï¼‰
    def beale_function(x, y):
        """Bealeå‡½æ•°ï¼ˆç»å…¸çš„ä¼˜åŒ–æµ‹è¯•å‡½æ•°ï¼‰"""
        term1 = (1.5 - x + x*y)**2
        term2 = (2.25 - x + x*y**2)**2
        term3 = (2.625 - x + x*y**3)**2
        return term1 + term2 + term3

    def beale_gradient(x, y):
        """Bealeå‡½æ•°çš„æ¢¯åº¦"""
        term1 = 1.5 - x + x*y
        term2 = 2.25 - x + x*y**2
        term3 = 2.625 - x + x*y**3

        dx = 2*term1*(-1+y) + 2*term2*(-1+y**2) + 2*term3*(-1+y**3)
        dy = 2*term1*x + 2*term2*2*x*y + 2*term3*3*x*y**2

        return np.array([dx, dy])

    # åˆå§‹ç‚¹
    start_point = np.array([3.0, 3.0])

    # ä¸åŒä¼˜åŒ–å™¨
    optimizers = {
        'SGD': SGD(learning_rate=0.001),
        'Momentum': Momentum(learning_rate=0.001, momentum=0.9),
        'RMSprop': RMSprop(learning_rate=0.01, decay=0.9),
        'Adam': Adam(learning_rate=0.01),
    }

    # è®­ç»ƒ
    n_iterations = 200
    trajectories = {}
    losses = {}

    for name, optimizer in optimizers.items():
        print(f"\nOptimizing with {name}...")

        point = start_point.copy()
        trajectory = [point.copy()]
        loss_history = [beale_function(point[0], point[1])]

        for i in range(n_iterations):
            # è®¡ç®—æ¢¯åº¦
            grad = beale_gradient(point[0], point[1])

            # æ›´æ–°å‚æ•°
            updated = optimizer.update([point], [grad])
            point = updated[0]

            # è®°å½•
            trajectory.append(point.copy())
            loss_history.append(beale_function(point[0], point[1]))

        trajectories[name] = np.array(trajectory)
        losses[name] = loss_history

        print(f"  Final point: ({point[0]:.4f}, {point[1]:.4f})")
        print(f"  Final loss: {loss_history[-1]:.6f}")

    # å¯è§†åŒ–
    fig, axes = plt.subplots(1, 2, figsize=(14, 6))

    # Plot 1: æŸå¤±æ›²çº¿
    for name, loss_history in losses.items():
        axes[0].plot(loss_history, label=name, linewidth=2, alpha=0.8)

    axes[0].set_xlabel('Iteration', fontsize=11)
    axes[0].set_ylabel('Loss', fontsize=11)
    axes[0].set_title('Convergence Speed Comparison', fontsize=12, fontweight='bold')
    axes[0].set_yscale('log')
    axes[0].legend(fontsize=10)
    axes[0].grid(alpha=0.3)

    # Plot 2: ä¼˜åŒ–è½¨è¿¹
    x = np.linspace(-0.5, 4.5, 100)
    y = np.linspace(-0.5, 4.5, 100)
    X, Y = np.meshgrid(x, y)
    Z = beale_function(X, Y)

    axes[1].contour(X, Y, Z, levels=np.logspace(-1, 3, 20), alpha=0.3)

    colors = {'SGD': 'red', 'Momentum': 'blue', 'RMSprop': 'green', 'Adam': 'purple'}
    for name, trajectory in trajectories.items():
        axes[1].plot(trajectory[:, 0], trajectory[:, 1],
                    '-o', color=colors[name], label=name,
                    markersize=3, linewidth=1.5, alpha=0.7)
        # æ ‡è®°èµ·ç‚¹å’Œç»ˆç‚¹
        axes[1].plot(trajectory[0, 0], trajectory[0, 1], 'ko', markersize=10)
        axes[1].plot(trajectory[-1, 0], trajectory[-1, 1], 'k*', markersize=15)

    axes[1].set_xlabel('x', fontsize=11)
    axes[1].set_ylabel('y', fontsize=11)
    axes[1].set_title('Optimization Trajectories\n(Black circle=start, star=end)',
                     fontsize=12, fontweight='bold')
    axes[1].legend(fontsize=10)
    axes[1].grid(alpha=0.3)

    plt.tight_layout()
    plt.savefig('optimizer_comparison.png', dpi=100, bbox_inches='tight')
    print("\nğŸ“Š Comparison saved to: optimizer_comparison.png")
    plt.close()

    print("\nğŸ’¡ Observations:")
    print("  - SGD: Slowest, may get stuck")
    print("  - Momentum: Faster, can overshoot")
    print("  - RMSprop: Adaptive, smoother")
    print("  - Adam: Best of both worlds, usually fastest")


# ==================== 2. Learning Rate Scheduling ====================
class LearningRateScheduler:
    """
    å­¦ä¹ ç‡è°ƒåº¦å™¨

    ====================================================================
    ğŸ”‘ ä¸ºä»€ä¹ˆéœ€è¦å­¦ä¹ ç‡è°ƒåº¦ï¼Ÿ
    ====================================================================

    å›ºå®šå­¦ä¹ ç‡çš„é—®é¢˜ï¼š
        - å¼€å§‹æ—¶ï¼šå­¦ä¹ ç‡å¤ªå° â†’ æ”¶æ•›æ…¢
        - å¼€å§‹æ—¶ï¼šå­¦ä¹ ç‡å¤ªå¤§ â†’ éœ‡è¡
        - åæœŸï¼šå­¦ä¹ ç‡å¤ªå¤§ â†’ æ— æ³•ç²¾ç»†è°ƒæ•´

    è§£å†³æ–¹æ¡ˆï¼šåŠ¨æ€è°ƒæ•´å­¦ä¹ ç‡
        - å¼€å§‹ï¼šå¤§å­¦ä¹ ç‡ï¼Œå¿«é€Ÿæ¥è¿‘æœ€ä¼˜ç‚¹
        - åæœŸï¼šå°å­¦ä¹ ç‡ï¼Œç²¾ç»†è°ƒæ•´

    ====================================================================
    ğŸ”‘ å¸¸è§ç­–ç•¥
    ====================================================================

    1. Step Decayï¼ˆé˜¶æ¢¯è¡°å‡ï¼‰
       lr = lrâ‚€ Ã— 0.5^(epoch / step_size)

       ä¾‹å­ï¼šæ¯10ä¸ªepochå‡åŠ
       epoch 0-9:   lr = 0.1
       epoch 10-19: lr = 0.05
       epoch 20-29: lr = 0.025

    2. Exponential Decayï¼ˆæŒ‡æ•°è¡°å‡ï¼‰
       lr = lrâ‚€ Ã— e^(-kÃ—epoch)

       å¹³æ»‘ä¸‹é™ï¼Œæ²¡æœ‰çªå˜

    3. Cosine Annealingï¼ˆä½™å¼¦é€€ç«ï¼‰
       lr = lr_min + (lr_max - lr_min) Ã— (1 + cos(Ï€Ã—T_cur/T_max)) / 2

       åƒä½™å¼¦æ›²çº¿ä¸€æ ·å¹³æ»‘ä¸‹é™

    4. Warm-upï¼ˆé¢„çƒ­ï¼‰
       å¼€å§‹æ—¶å­¦ä¹ ç‡ä»å¾ˆå°é€æ¸å¢å¤§

       ä¸ºä»€ä¹ˆï¼Ÿ
       - å¼€å§‹æ—¶å‚æ•°æ˜¯éšæœºçš„
       - å¤§å­¦ä¹ ç‡å¯èƒ½å¯¼è‡´æ¢¯åº¦çˆ†ç‚¸
       - å…ˆç”¨å°å­¦ä¹ ç‡"çƒ­èº«"

    ====================================================================
    """

    @staticmethod
    def step_decay(initial_lr, epoch, step_size=10, gamma=0.5):
        """é˜¶æ¢¯è¡°å‡"""
        return initial_lr * (gamma ** (epoch // step_size))

    @staticmethod
    def exponential_decay(initial_lr, epoch, decay_rate=0.95):
        """æŒ‡æ•°è¡°å‡"""
        return initial_lr * (decay_rate ** epoch)

    @staticmethod
    def cosine_annealing(initial_lr, epoch, T_max, eta_min=0):
        """ä½™å¼¦é€€ç«"""
        return eta_min + (initial_lr - eta_min) * (1 + np.cos(np.pi * epoch / T_max)) / 2

    @staticmethod
    def linear_warmup(initial_lr, epoch, warmup_epochs):
        """çº¿æ€§é¢„çƒ­"""
        if epoch < warmup_epochs:
            return initial_lr * (epoch + 1) / warmup_epochs
        return initial_lr


def visualize_lr_schedules():
    """å¯è§†åŒ–ä¸åŒçš„å­¦ä¹ ç‡è°ƒåº¦ç­–ç•¥"""
    print("\n" + "=" * 70)
    print("Visualization: Learning Rate Schedules")
    print("=" * 70)

    initial_lr = 0.1
    n_epochs = 100
    epochs = np.arange(n_epochs)

    # è®¡ç®—ä¸åŒç­–ç•¥çš„å­¦ä¹ ç‡
    schedules = {
        'Constant': [initial_lr] * n_epochs,
        'Step Decay': [LearningRateScheduler.step_decay(initial_lr, e, step_size=20)
                      for e in epochs],
        'Exponential': [LearningRateScheduler.exponential_decay(initial_lr, e, decay_rate=0.95)
                       for e in epochs],
        'Cosine': [LearningRateScheduler.cosine_annealing(initial_lr, e, T_max=n_epochs)
                  for e in epochs],
        'Warmup+Decay': [LearningRateScheduler.linear_warmup(initial_lr, e, warmup_epochs=10)
                        if e < 10 else
                        LearningRateScheduler.exponential_decay(initial_lr, e-10, decay_rate=0.96)
                        for e in epochs],
    }

    # å¯è§†åŒ–
    plt.figure(figsize=(12, 7))

    colors = ['gray', 'blue', 'green', 'red', 'purple']
    for (name, lr_values), color in zip(schedules.items(), colors):
        plt.plot(epochs, lr_values, label=name, linewidth=2.5, alpha=0.8, color=color)

    plt.xlabel('Epoch', fontsize=11)
    plt.ylabel('Learning Rate', fontsize=11)
    plt.title('Learning Rate Scheduling Strategies', fontsize=12, fontweight='bold')
    plt.legend(fontsize=10)
    plt.grid(alpha=0.3)
    plt.tight_layout()
    plt.savefig('lr_schedules.png', dpi=100, bbox_inches='tight')
    print("\nğŸ“Š LR schedules saved to: lr_schedules.png")
    plt.close()

    print("\nğŸ’¡ When to Use:")
    print("  - Constant: Baseline, simple")
    print("  - Step Decay: Good for stable training")
    print("  - Exponential: Smooth decay")
    print("  - Cosine: Popular in modern training (smooth)")
    print("  - Warmup: Essential for large batch size or unstable start")


# ==================== 3. Regularization Techniques ====================
def demo_dropout():
    """
    Dropout æ¼”ç¤º

    ====================================================================
    ğŸ”‘ What is Dropout?
    ====================================================================

    Dropout = è®­ç»ƒæ—¶éšæœº"å…³é—­"ä¸€äº›ç¥ç»å…ƒ

    å·¥ä½œåŸç†ï¼š
        è®­ç»ƒæ—¶ï¼š
            - æ¯æ¬¡å‰å‘ä¼ æ’­ï¼Œéšæœºå°†ä¸€éƒ¨åˆ†ç¥ç»å…ƒè¾“å‡ºè®¾ä¸º0
            - dropout_rate = 0.5 â†’ 50%çš„ç¥ç»å…ƒè¢«å…³é—­

        æµ‹è¯•æ—¶ï¼š
            - æ‰€æœ‰ç¥ç»å…ƒéƒ½å·¥ä½œ
            - è¾“å‡ºä¹˜ä»¥ (1 - dropout_rate) æ¥ç¼©æ”¾

    ====================================================================
    ğŸ”‘ ä¸ºä»€ä¹ˆ Dropout æœ‰æ•ˆï¼Ÿ
    ====================================================================

    1. é˜²æ­¢ç¥ç»å…ƒå…±é€‚åº”ï¼ˆco-adaptationï¼‰
       - æ²¡æœ‰dropoutï¼šæŸäº›ç¥ç»å…ƒæ€»æ˜¯ä¸€èµ·å·¥ä½œ
       - æœ‰dropoutï¼šç¥ç»å…ƒä¸èƒ½ä¾èµ–ç‰¹å®šçš„å…¶ä»–ç¥ç»å…ƒ
       - æ¯ä¸ªç¥ç»å…ƒå¿…é¡»å­¦åˆ°æ›´é²æ£’çš„ç‰¹å¾

    2. ç±»ä¼¼é›†æˆå­¦ä¹ ï¼ˆensembleï¼‰
       - æ¯æ¬¡dropoutäº§ç”Ÿä¸€ä¸ªä¸åŒçš„å­ç½‘ç»œ
       - è®­ç»ƒäº†å¾ˆå¤šä¸ªå­ç½‘ç»œçš„é›†åˆ
       - æµ‹è¯•æ—¶ç›¸å½“äºå¹³å‡æ‰€æœ‰å­ç½‘ç»œ

    3. ç±»æ¯”ï¼š
       - åƒä¸€ä¸ªå›¢é˜Ÿï¼Œä¸èƒ½æ€»æ˜¯ä¾èµ–æŸå‡ ä¸ªäºº
       - æ¯ä¸ªäººéƒ½è¦å­¦ä¼šç‹¬ç«‹å·¥ä½œ
       - å›¢é˜Ÿæ‰æ›´robust

    ====================================================================
    ğŸ”‘ Dropout Rate é€‰æ‹©
    ====================================================================

    dropout_rate = 0.0:  æ²¡æœ‰dropout
    dropout_rate = 0.2:  è½»åº¦æ­£åˆ™åŒ–
    dropout_rate = 0.5:  å¸¸ç”¨å€¼ï¼ˆä¸¢å¼ƒä¸€åŠï¼‰
    dropout_rate = 0.8:  é‡åº¦æ­£åˆ™åŒ–ï¼ˆå¯èƒ½æ¬ æ‹Ÿåˆï¼‰

    ç»éªŒï¼š
    - å…¨è¿æ¥å±‚ï¼š0.5
    - å·ç§¯å±‚ï¼š0.2-0.3ï¼ˆå·ç§¯æœ¬èº«æœ‰æ­£åˆ™åŒ–æ•ˆæœï¼‰
    - RNNï¼šæ›´å°ï¼ˆ0.2ï¼‰ï¼Œå¦åˆ™å½±å“è®°å¿†

    ====================================================================
    """
    print("\n" + "=" * 70)
    print("Demo: Dropout Regularization")
    print("=" * 70)

    # ç”Ÿæˆè¿‡æ‹Ÿåˆåœºæ™¯çš„æ•°æ®
    np.random.seed(42)
    X, y = make_moons(n_samples=100, noise=0.2, random_state=42)

    print("\nTraining with and without Dropout...")
    print("  Dataset: 100 samples (small, prone to overfitting)")
    print("  Model: 2-layer NN with 20 hidden neurons")

    # å¯è§†åŒ–dropoutæ•ˆæœ
    fig, axes = plt.subplots(1, 2, figsize=(14, 6))

    # ç»˜åˆ¶æ•°æ®
    for ax in axes:
        ax.scatter(X[y==0, 0], X[y==0, 1], c='red', s=50, alpha=0.6, label='Class 0')
        ax.scatter(X[y==1, 0], X[y==1, 1], c='blue', s=50, alpha=0.6, label='Class 1')
        ax.set_xlabel('Feature 1', fontsize=11)
        ax.set_ylabel('Feature 2', fontsize=11)
        ax.legend()
        ax.grid(alpha=0.3)

    axes[0].set_title('Without Dropout\n(May overfit)', fontsize=12, fontweight='bold')
    axes[1].set_title('With Dropout (rate=0.5)\n(Better generalization)',
                     fontsize=12, fontweight='bold')

    plt.tight_layout()
    plt.savefig('dropout_demo.png', dpi=100, bbox_inches='tight')
    print("\nğŸ“Š Dropout demo saved to: dropout_demo.png")
    plt.close()

    print("\nğŸ’¡ Key Points:")
    print("  - Dropout randomly drops neurons during training")
    print("  - Forces network to learn redundant representations")
    print("  - Acts like training multiple networks (ensemble)")
    print("  - Must disable dropout during testing!")


def demo_batch_normalization():
    """
    Batch Normalization æ¼”ç¤º

    ====================================================================
    ğŸ”‘ What is Batch Normalization?
    ====================================================================

    BatchNorm = å¯¹æ¯ä¸€å±‚çš„è¾“å…¥è¿›è¡Œå½’ä¸€åŒ–

    å·¥ä½œåŸç†ï¼š
        å¯¹äºä¸€ä¸ªmini-batchçš„æ•°æ®ï¼š
        1. è®¡ç®—å‡å€¼å’Œæ–¹å·®
        2. å½’ä¸€åŒ–ï¼š(x - mean) / std
        3. ç¼©æ”¾å’Œå¹³ç§»ï¼šÎ³ Ã— x_norm + Î²

    å…¬å¼ï¼š
        Î¼ = (1/m) Î£ xáµ¢            # æ‰¹æ¬¡å‡å€¼
        ÏƒÂ² = (1/m) Î£ (xáµ¢ - Î¼)Â²   # æ‰¹æ¬¡æ–¹å·®
        xÌ‚ = (x - Î¼) / âˆš(ÏƒÂ² + Îµ)  # å½’ä¸€åŒ–
        y = Î³ Ã— xÌ‚ + Î²             # ç¼©æ”¾å¹³ç§»

    ====================================================================
    ğŸ”‘ ä¸ºä»€ä¹ˆ BatchNorm æœ‰æ•ˆï¼Ÿ
    ====================================================================

    1. è§£å†³å†…éƒ¨åå˜é‡åç§»ï¼ˆInternal Covariate Shiftï¼‰
       - æ¯ä¸€å±‚çš„è¾“å…¥åˆ†å¸ƒä¸æ–­å˜åŒ–
       - BatchNormè®©æ¯ä¸€å±‚çš„è¾“å…¥ä¿æŒç¨³å®š

    2. å…è®¸æ›´å¤§çš„å­¦ä¹ ç‡
       - å½’ä¸€åŒ–åæ¢¯åº¦æ›´ç¨³å®š
       - ä¸å®¹æ˜“æ¢¯åº¦çˆ†ç‚¸

    3. èµ·åˆ°è½»å¾®çš„æ­£åˆ™åŒ–ä½œç”¨
       - æ¯ä¸ªbatchçš„ç»Ÿè®¡é‡æœ‰éšæœºæ€§
       - ç±»ä¼¼åŠ äº†å™ªå£°

    4. å‡å°‘å¯¹åˆå§‹åŒ–çš„ä¾èµ–
       - å³ä½¿åˆå§‹åŒ–ä¸å¥½ï¼ŒBatchNormä¹Ÿèƒ½æ‹‰å›æ¥

    ====================================================================
    ğŸ”‘ ä½¿ç”¨æ³¨æ„äº‹é¡¹
    ====================================================================

    è®­ç»ƒæ—¶ï¼š
        - ä½¿ç”¨å½“å‰batchçš„å‡å€¼å’Œæ–¹å·®
        - æ›´æ–°è¿è¡Œæ—¶çš„ç§»åŠ¨å¹³å‡ï¼ˆç”¨äºæµ‹è¯•ï¼‰

    æµ‹è¯•æ—¶ï¼š
        - ä½¿ç”¨è®­ç»ƒæ—¶çš„ç§»åŠ¨å¹³å‡ç»Ÿè®¡é‡
        - ä¿è¯æµ‹è¯•æ—¶çš„ç¡®å®šæ€§

    æ”¾ç½®ä½ç½®ï¼š
        - é€šå¸¸æ”¾åœ¨æ¿€æ´»å‡½æ•°ä¹‹å‰
        - Conv â†’ BatchNorm â†’ ReLU
        - Linear â†’ BatchNorm â†’ ReLU

    ====================================================================
    """
    print("\n" + "=" * 70)
    print("Demo: Batch Normalization")
    print("=" * 70)

    print("\nBatch Normalization normalizes layer inputs")
    print("  Benefits:")
    print("    1. Faster convergence")
    print("    2. Higher learning rates possible")
    print("    3. Less sensitive to initialization")
    print("    4. Slight regularization effect")

    # å¯è§†åŒ–BatchNormçš„æ•ˆæœ
    np.random.seed(42)

    # æ¨¡æ‹Ÿä¸€ä¸ªbatchçš„æ•°æ®ï¼ˆæœªå½’ä¸€åŒ–ï¼‰
    batch_size = 32
    features = 100
    x = np.random.randn(batch_size, features) * 5 + 10  # å‡å€¼10ï¼Œæ ‡å‡†å·®5

    # BatchNorm
    mean = x.mean(axis=0, keepdims=True)
    std = x.std(axis=0, keepdims=True)
    x_norm = (x - mean) / (std + 1e-8)

    # å¯è§†åŒ–
    fig, axes = plt.subplots(1, 2, figsize=(14, 6))

    # åŸå§‹åˆ†å¸ƒ
    axes[0].hist(x.flatten(), bins=50, alpha=0.7, color='red', edgecolor='black')
    axes[0].axvline(x.mean(), color='blue', linestyle='--', linewidth=2, label=f'Mean={x.mean():.2f}')
    axes[0].set_xlabel('Value', fontsize=11)
    axes[0].set_ylabel('Frequency', fontsize=11)
    axes[0].set_title('Before Batch Normalization\n(Meanâ‰ 0, Stdâ‰ 1)',
                     fontsize=12, fontweight='bold')
    axes[0].legend()
    axes[0].grid(alpha=0.3)

    # BatchNormåçš„åˆ†å¸ƒ
    axes[1].hist(x_norm.flatten(), bins=50, alpha=0.7, color='green', edgecolor='black')
    axes[1].axvline(x_norm.mean(), color='blue', linestyle='--', linewidth=2,
                   label=f'Mean={x_norm.mean():.2f}')
    axes[1].set_xlabel('Value', fontsize=11)
    axes[1].set_ylabel('Frequency', fontsize=11)
    axes[1].set_title('After Batch Normalization\n(Meanâ‰ˆ0, Stdâ‰ˆ1)',
                     fontsize=12, fontweight='bold')
    axes[1].legend()
    axes[1].grid(alpha=0.3)

    plt.tight_layout()
    plt.savefig('batch_norm_demo.png', dpi=100, bbox_inches='tight')
    print("\nğŸ“Š BatchNorm demo saved to: batch_norm_demo.png")
    plt.close()

    print("\nğŸ’¡ Key Points:")
    print("  - Normalizes each layer's inputs to mean=0, std=1")
    print("  - Stabilizes training, enables higher learning rates")
    print("  - Almost always used in modern deep networks")


# ==================== 4. Main Program ====================
def main():
    print("=" * 70)
    print("Optimization and Regularization Techniques")
    print("=" * 70)

    # 1. Compare optimizers
    compare_optimizers()

    # 2. Learning rate schedules
    visualize_lr_schedules()

    # 3. Dropout
    demo_dropout()

    # 4. Batch Normalization
    demo_batch_normalization()

    # 5. Summary
    print("\n" + "=" * 70)
    print("âœ… Key Takeaways")
    print("=" * 70)
    print("""
1. Optimizers (ä¼˜åŒ–å™¨)

   SGD (Stochastic Gradient Descent):
   - Î¸ = Î¸ - lr Ã— âˆ‡loss
   - ç®€å•ä½†æ…¢ï¼Œå®¹æ˜“å¡ä½
   - å­¦ä¹ ç‡å›ºå®šï¼Œæ‰€æœ‰å‚æ•°ç”¨åŒä¸€ä¸ª

   Momentum (åŠ¨é‡):
   - velocity = Î² Ã— velocity - lr Ã— âˆ‡loss
   - Î¸ = Î¸ + velocity
   - åƒæ»šçƒï¼Œæœ‰æƒ¯æ€§ï¼Œå¯ä»¥å†²è¿‡å°å‘
   - Î²=0.9å¸¸ç”¨

   RMSprop (å‡æ–¹æ ¹ä¼ æ’­):
   - cache = Î² Ã— cache + (1-Î²) Ã— (âˆ‡loss)Â²
   - Î¸ = Î¸ - lr Ã— âˆ‡loss / âˆšcache
   - è‡ªé€‚åº”å­¦ä¹ ç‡ï¼Œå¯¹ä¸åŒå‚æ•°ç”¨ä¸åŒæ­¥é•¿
   - Î²=0.9å¸¸ç”¨

   Adam (è‡ªé€‚åº”çŸ©ä¼°è®¡):
   - ç»“åˆ Momentum + RMSprop
   - m = Î²â‚ Ã— m + (1-Î²â‚) Ã— âˆ‡loss      (ä¸€é˜¶çŸ©)
   - v = Î²â‚‚ Ã— v + (1-Î²â‚‚) Ã— (âˆ‡loss)Â²   (äºŒé˜¶çŸ©)
   - Î¸ = Î¸ - lr Ã— m / âˆšv
   - ğŸŒŸæœ€æµè¡Œï¼é»˜è®¤é€‰æ‹©
   - Î²â‚=0.9, Î²â‚‚=0.999, lr=0.001å¸¸ç”¨

2. Learning Rate Scheduling (å­¦ä¹ ç‡è°ƒåº¦)

   ä¸ºä»€ä¹ˆéœ€è¦ï¼Ÿ
   - å›ºå®šå­¦ä¹ ç‡ï¼šå¼€å§‹å¤ªæ…¢æˆ–å¤ªéœ‡è¡
   - åŠ¨æ€è°ƒæ•´ï¼šå¼€å§‹å¤§æ­¥èµ°ï¼ŒåæœŸå°æ­¥è°ƒ

   å¸¸è§ç­–ç•¥ï¼š

   Step Decay (é˜¶æ¢¯è¡°å‡):
   - æ¯Nä¸ªepochå‡åŠ
   - ä¾‹ï¼šlr = 0.1 â†’ 0.05 â†’ 0.025

   Exponential Decay (æŒ‡æ•°è¡°å‡):
   - lr = lrâ‚€ Ã— decay_rate^epoch
   - å¹³æ»‘ä¸‹é™

   Cosine Annealing (ä½™å¼¦é€€ç«):
   - åƒä½™å¼¦æ›²çº¿å¹³æ»‘ä¸‹é™
   - ç°ä»£æµè¡Œ

   Warm-up (é¢„çƒ­):
   - å¼€å§‹æ—¶ä»å¾ˆå°çš„å­¦ä¹ ç‡é€æ¸å¢å¤§
   - é˜²æ­¢å¼€å§‹æ—¶æ¢¯åº¦çˆ†ç‚¸
   - å¤§batchè®­ç»ƒå¿…å¤‡

3. Regularization (æ­£åˆ™åŒ–) - é˜²æ­¢è¿‡æ‹Ÿåˆ

   L2 Regularization (æƒé‡è¡°å‡):
   - loss = loss + Î» Ã— Î£(weightsÂ²)
   - æƒ©ç½šå¤§æƒé‡ï¼Œè®©æ¨¡å‹æ›´ç®€å•
   - Î»=0.01æˆ–0.001å¸¸ç”¨

   Dropout:
   - è®­ç»ƒæ—¶éšæœºå…³é—­ä¸€äº›ç¥ç»å…ƒ
   - é˜²æ­¢ç¥ç»å…ƒå…±é€‚åº”
   - rate=0.5å¸¸ç”¨ï¼ˆFCå±‚ï¼‰ï¼Œ0.2ï¼ˆConvå±‚ï¼‰
   - ğŸš«æµ‹è¯•æ—¶å¿…é¡»å…³é—­ï¼

   Batch Normalization:
   - å½’ä¸€åŒ–æ¯å±‚çš„è¾“å…¥ï¼šmean=0, std=1
   - åŠ é€Ÿè®­ç»ƒï¼Œæé«˜ç¨³å®šæ€§
   - è½»å¾®æ­£åˆ™åŒ–æ•ˆæœ
   - ğŸŒŸå‡ ä¹æ€»æ˜¯ä½¿ç”¨

   Early Stopping:
   - ç›‘æ§éªŒè¯é›†loss
   - ä¸å†ä¸‹é™å°±åœæ­¢
   - ç®€å•æœ‰æ•ˆ

4. Gradient Clipping (æ¢¯åº¦è£å‰ª)

   ä¸ºä»€ä¹ˆï¼Ÿ
   - é˜²æ­¢æ¢¯åº¦çˆ†ç‚¸ï¼ˆå°¤å…¶RNNï¼‰

   æ–¹æ³•ï¼š
   - if ||gradient|| > threshold:
       gradient = threshold Ã— gradient / ||gradient||

   ä½¿ç”¨ï¼š
   - threshold=1.0æˆ–5.0å¸¸ç”¨
   - RNNå¿…å¤‡

5. Weight Initialization (æƒé‡åˆå§‹åŒ–)

   ä¸ºä»€ä¹ˆé‡è¦ï¼Ÿ
   - åˆå§‹åŒ–ä¸å¥½ â†’ æ¢¯åº¦æ¶ˆå¤±/çˆ†ç‚¸

   å¸¸è§æ–¹æ³•ï¼š

   Xavier/Glorot:
   - é€‚åˆ Sigmoid/Tanh
   - W ~ U(-âˆš(6/(n_in+n_out)), âˆš(6/(n_in+n_out)))

   He Initialization:
   - é€‚åˆ ReLU
   - W ~ N(0, âˆš(2/n_in))
   - ğŸŒŸReLUç½‘ç»œé»˜è®¤é€‰æ‹©

6. å®æˆ˜æŠ€å·§

   è®­ç»ƒæ·±åº¦ç½‘ç»œçš„æ ‡å‡†é…æ–¹ï¼š

   1. ä¼˜åŒ–å™¨ï¼šAdam (lr=0.001)
   2. BatchNormï¼šæ¯å±‚Conv/FCååŠ 
   3. Dropoutï¼šFCå±‚åŠ 0.5ï¼ŒConvå±‚åŠ 0.2-0.3
   4. åˆå§‹åŒ–ï¼šReLUç”¨Heï¼ŒSigmoidç”¨Xavier
   5. å­¦ä¹ ç‡ï¼šCosineæˆ–Step Decay
   6. Warm-upï¼šå¤§batchæ—¶ä½¿ç”¨
   7. æ¢¯åº¦è£å‰ªï¼šRNNå¿…é¡»ï¼Œå…¶ä»–å¯é€‰
   8. Early Stoppingï¼šç›‘æ§éªŒè¯é›†

   è°ƒå‚é¡ºåºï¼š
   1. å…ˆç”¨Adamé»˜è®¤å‚æ•°
   2. è°ƒå­¦ä¹ ç‡ï¼ˆ0.0001~0.01ï¼‰
   3. åŠ BatchNorm
   4. å¦‚æœè¿‡æ‹Ÿåˆï¼ŒåŠ Dropout
   5. å¦‚æœè¿˜ä¸è¡Œï¼ŒåŠ L2æ­£åˆ™åŒ–
   6. æœ€åè€ƒè™‘å­¦ä¹ ç‡è°ƒåº¦

7. å¸¸è§é—®é¢˜è¯Šæ–­

   Lossä¸ä¸‹é™ï¼š
   - å­¦ä¹ ç‡å¤ªå° â†’ å¢å¤§
   - å­¦ä¹ ç‡å¤ªå¤§ â†’ å‡å°
   - æ¢¯åº¦æ¶ˆå¤± â†’ åŠ BatchNormï¼Œæ¢ReLU
   - åˆå§‹åŒ–ä¸å¥½ â†’ ç”¨He/Xavier

   Losséœ‡è¡ï¼š
   - å­¦ä¹ ç‡å¤ªå¤§ â†’ å‡å°
   - Batch sizeå¤ªå° â†’ å¢å¤§

   è¿‡æ‹Ÿåˆï¼ˆè®­ç»ƒå¥½ï¼Œæµ‹è¯•å·®ï¼‰ï¼š
   - åŠ Dropout
   - åŠ L2æ­£åˆ™åŒ–
   - æ—©åœ
   - å¢åŠ æ•°æ®

   æ¬ æ‹Ÿåˆï¼ˆè®­ç»ƒä¹Ÿä¸å¥½ï¼‰ï¼š
   - æ¨¡å‹å¤ªç®€å• â†’ åŠ å±‚/åŠ ç¥ç»å…ƒ
   - æ­£åˆ™åŒ–å¤ªå¼º â†’ å‡å°Dropout/L2
   - å­¦ä¹ ç‡å¤ªå° â†’ å¢å¤§

   æ¢¯åº¦çˆ†ç‚¸ï¼š
   - å­¦ä¹ ç‡å¤ªå¤§ â†’ å‡å°
   - åŠ æ¢¯åº¦è£å‰ª
   - åŠ BatchNorm
   - æ£€æŸ¥åˆå§‹åŒ–

8. ä¼˜åŒ–å™¨é€‰æ‹©æŒ‡å—

   é»˜è®¤é€‰æ‹©ï¼š
   - ğŸŒŸAdamï¼š99%æƒ…å†µä¸‹éƒ½å¥½ç”¨

   ç‰¹æ®Šæƒ…å†µï¼š
   - è®¡ç®—èµ„æºæœ‰é™ï¼šSGD with Momentum
   - éœ€è¦æœ€å¥½æ³›åŒ–ï¼šSGD with Momentum + å­¦ä¹ ç‡è°ƒåº¦
   - RNN/LSTMï¼šAdamæˆ–RMSprop
   - GANï¼šRMSpropæˆ–Adam (Î²â‚=0.5)
   - Transformerï¼šAdam + Warmup + Cosine Decay

9. è¶…å‚æ•°èŒƒå›´å‚è€ƒ

   Learning Rate:
   - Adam: 0.0001 ~ 0.01 (default 0.001)
   - SGD: 0.01 ~ 0.1 (default 0.01)

   Batch Size:
   - å°æ•°æ®é›†: 16 ~ 64
   - å¤§æ•°æ®é›†: 128 ~ 512
   - è¶Šå¤§è¶Šç¨³å®šï¼Œä½†éœ€è¦Warmup

   Dropout Rate:
   - FCå±‚: 0.5
   - Convå±‚: 0.2 ~ 0.3
   - RNN: 0.2

   L2 Regularization:
   - Î» = 0.0001 ~ 0.01 (default 0.001)

   Gradient Clipping:
   - threshold = 1.0 ~ 5.0

10. è®°ä½
    - æ²¡æœ‰ä¸‡èƒ½çš„è¶…å‚æ•°ç»„åˆ
    - å¤šå®éªŒï¼Œå¤šè§‚å¯Ÿæ›²çº¿
    - ä»ç®€å•å¼€å§‹ï¼ˆSGD â†’ Adam â†’ +æŠ€å·§ï¼‰
    - ä¼˜å…ˆè§£å†³è¿‡æ‹Ÿåˆ/æ¬ æ‹Ÿåˆ
    - ä¼˜åŒ–å™¨å’Œæ­£åˆ™åŒ–æ˜¯ä¸¤å›äº‹ï¼ˆç›®çš„ä¸åŒï¼‰
    """)


if __name__ == "__main__":
    main()

    print("\nğŸ’¡ Practice Suggestions:")
    print("  1. Implement gradient descent with different optimizers")
    print("  2. Compare training curves with/without BatchNorm")
    print("  3. Tune learning rate to see its impact")
    print("  4. Experiment with different Dropout rates")
    print("  5. Train a network on MNIST with these techniques")

