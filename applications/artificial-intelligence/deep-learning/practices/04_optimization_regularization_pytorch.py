"""
ä¼˜åŒ–ä¸æ­£åˆ™åŒ–æŠ€æœ¯ - PyTorch å®ç°

å¯¹æ¯” NumPy ç‰ˆæœ¬ï¼š
- NumPy: æ‰‹å†™ä¼˜åŒ–å™¨ï¼Œç†è§£æ›´æ–°è§„åˆ™
- PyTorch: ä½¿ç”¨ torch.optimï¼ŒGPUåŠ é€Ÿï¼Œå·¥ä¸šå®è·µ

æœ¬æ–‡ä»¶å†…å®¹ï¼š
1. PyTorch ä¼˜åŒ–å™¨ (SGD, Momentum, RMSprop, Adam)
2. å­¦ä¹ ç‡è°ƒåº¦ (LR Scheduling)
3. æ­£åˆ™åŒ–æŠ€æœ¯ (Dropout, BatchNorm, L2)
4. æ¢¯åº¦è£å‰ª (Gradient Clipping)
5. æƒé‡åˆå§‹åŒ– (Weight Initialization)
6. å®Œæ•´è®­ç»ƒç¤ºä¾‹
"""

import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import Dataset, DataLoader, TensorDataset
import numpy as np
import matplotlib.pyplot as plt
from sklearn.datasets import make_moons, make_circles
from sklearn.model_selection import train_test_split
import time


# ==================== 1. PyTorch ä¼˜åŒ–å™¨ ====================
def demo_pytorch_optimizers():
    """
    æ¼”ç¤º PyTorch çš„å„ç§ä¼˜åŒ–å™¨

    ====================================================================
    ğŸ”‘ PyTorch vs NumPy ä¼˜åŒ–å™¨
    ====================================================================

    NumPy ç‰ˆæœ¬ï¼ˆæ‰‹å†™æ›´æ–°è§„åˆ™ï¼‰ï¼š
    ```python
    class Adam:
        def update(self, params, grads):
            # æ‰‹åŠ¨è®¡ç®—ä¸€é˜¶çŸ©ã€äºŒé˜¶çŸ©
            m = beta1 * m + (1-beta1) * grad
            v = beta2 * v + (1-beta2) * grad**2
            # æ‰‹åŠ¨åå·®ä¿®æ­£
            m_hat = m / (1 - beta1**t)
            v_hat = v / (1 - beta2**t)
            # æ‰‹åŠ¨æ›´æ–°
            param -= lr * m_hat / (sqrt(v_hat) + eps)
    ```

    PyTorch ç‰ˆæœ¬ï¼ˆä¸€è¡Œï¼‰ï¼š
    ```python
    optimizer = optim.Adam(model.parameters(), lr=0.001)
    # è®­ç»ƒå¾ªç¯ä¸­
    optimizer.zero_grad()
    loss.backward()
    optimizer.step()  # â† è‡ªåŠ¨æ›´æ–°æ‰€æœ‰å‚æ•°ï¼
    ```

    PyTorch å¸®ä½ åšäº†ä»€ä¹ˆï¼Ÿ
    - è‡ªåŠ¨ç®¡ç†æ‰€æœ‰å‚æ•°
    - è‡ªåŠ¨æ›´æ–°ï¼ˆå†…ç½®æœ€ä¼˜å®ç°ï¼‰
    - GPU åŠ é€Ÿ
    - å†…ç½®æ‰€æœ‰ç°ä»£ä¼˜åŒ–å™¨

    ====================================================================
    """
    print("=" * 70)
    print("1. PyTorch ä¼˜åŒ–å™¨æ¼”ç¤º")
    print("=" * 70)

    # åˆ›å»ºä¸€ä¸ªç®€å•æ¨¡å‹
    model = nn.Sequential(
        nn.Linear(2, 10),
        nn.ReLU(),
        nn.Linear(10, 1)
    )

    # ========== SGD ==========
    print("\n" + "-" * 70)
    print("SGD (Stochastic Gradient Descent)")
    print("-" * 70)
    print("""
æ›´æ–°è§„åˆ™ï¼š
    Î¸ = Î¸ - learning_rate Ã— gradient

ç‰¹ç‚¹ï¼š
    - æœ€åŸºç¡€çš„ä¼˜åŒ–å™¨
    - å›ºå®šå­¦ä¹ ç‡
    - å¯èƒ½éœ‡è¡ï¼Œæ”¶æ•›æ…¢
    """)

    sgd_optimizer = optim.SGD(model.parameters(), lr=0.01)
    print(f"åˆ›å»º SGD: {sgd_optimizer}")

    # ========== SGD + Momentum ==========
    print("\n" + "-" * 70)
    print("SGD with Momentum")
    print("-" * 70)
    print("""
æ›´æ–°è§„åˆ™ï¼š
    velocity = momentum Ã— velocity - learning_rate Ã— gradient
    Î¸ = Î¸ + velocity

ç‰¹ç‚¹ï¼š
    - ç´¯ç§¯å†å²æ¢¯åº¦ï¼ˆæƒ¯æ€§ï¼‰
    - åŠ é€Ÿæ”¶æ•›
    - å¯ä»¥å†²è¿‡å°å‘ï¼ˆå±€éƒ¨æœ€å°å€¼ï¼‰
    - momentum é€šå¸¸å– 0.9
    """)

    momentum_optimizer = optim.SGD(model.parameters(), lr=0.01, momentum=0.9)
    print(f"åˆ›å»º SGD+Momentum: {momentum_optimizer}")

    # ========== RMSprop ==========
    print("\n" + "-" * 70)
    print("RMSprop")
    print("-" * 70)
    print("""
æ›´æ–°è§„åˆ™ï¼š
    cache = decay Ã— cache + (1-decay) Ã— gradientÂ²
    Î¸ = Î¸ - learning_rate Ã— gradient / (âˆšcache + Îµ)

ç‰¹ç‚¹ï¼š
    - è‡ªé€‚åº”å­¦ä¹ ç‡
    - å¯¹é¢‘ç¹å˜åŒ–çš„å‚æ•°ç”¨å°å­¦ä¹ ç‡
    - å¯¹ç¨€ç–å˜åŒ–çš„å‚æ•°ç”¨å¤§å­¦ä¹ ç‡
    """)

    rmsprop_optimizer = optim.RMSprop(model.parameters(), lr=0.01, alpha=0.9)
    print(f"åˆ›å»º RMSprop: {rmsprop_optimizer}")

    # ========== Adam ==========
    print("\n" + "-" * 70)
    print("Adam (Adaptive Moment Estimation)")
    print("-" * 70)
    print("""
æ›´æ–°è§„åˆ™ï¼š
    m = Î²â‚ Ã— m + (1-Î²â‚) Ã— gradient       (ä¸€é˜¶çŸ©ï¼šåŠ¨é‡)
    v = Î²â‚‚ Ã— v + (1-Î²â‚‚) Ã— gradientÂ²      (äºŒé˜¶çŸ©ï¼šè‡ªé€‚åº”)
    m_hat = m / (1 - Î²â‚áµ—)                (åå·®ä¿®æ­£)
    v_hat = v / (1 - Î²â‚‚áµ—)
    Î¸ = Î¸ - lr Ã— m_hat / (âˆšv_hat + Îµ)

ç‰¹ç‚¹ï¼š
    - Momentum + RMSprop ç»“åˆ
    - è‡ªé€‚åº”å­¦ä¹ ç‡ + åŠ¨é‡åŠ é€Ÿ
    - é»˜è®¤å‚æ•°é€šå¸¸å°±å¾ˆå¥½ (Î²â‚=0.9, Î²â‚‚=0.999)
    - æœ€æµè¡Œçš„ä¼˜åŒ–å™¨ï¼
    """)

    adam_optimizer = optim.Adam(model.parameters(), lr=0.001, betas=(0.9, 0.999))
    print(f"åˆ›å»º Adam: {adam_optimizer}")

    # ========== AdamW ==========
    print("\n" + "-" * 70)
    print("AdamW (Adam with Weight Decay)")
    print("-" * 70)
    print("""
ç‰¹ç‚¹ï¼š
    - Adam çš„æ”¹è¿›ç‰ˆ
    - æ­£ç¡®å®ç°äº†æƒé‡è¡°å‡ï¼ˆL2æ­£åˆ™åŒ–ï¼‰
    - åœ¨ Transformer ç­‰æ¨¡å‹ä¸­è¡¨ç°æ›´å¥½
    - ç°ä»£æ·±åº¦å­¦ä¹ çš„é¦–é€‰ä¼˜åŒ–å™¨
    """)

    adamw_optimizer = optim.AdamW(model.parameters(), lr=0.001, weight_decay=0.01)
    print(f"åˆ›å»º AdamW: {adamw_optimizer}")

    print("\nğŸ’¡ é€‰æ‹©å»ºè®®:")
    print("  - å¿«é€ŸåŸå‹: Adam (é»˜è®¤é€‰æ‹©)")
    print("  - æœ€ä½³æ€§èƒ½: AdamW (Transformer ç­‰)")
    print("  - ç®€å•ä»»åŠ¡: SGD + Momentum")
    print("  - ç ”ç©¶å¯¹æ¯”: éƒ½è¯•è¯•ï¼Œçœ‹å“ªä¸ªå¥½")


def compare_optimizers_pytorch():
    """å¯¹æ¯”ä¸åŒä¼˜åŒ–å™¨åœ¨å®é™…è®­ç»ƒä¸­çš„è¡¨ç°"""
    print("\n" + "=" * 70)
    print("2. ä¼˜åŒ–å™¨æ€§èƒ½å¯¹æ¯”ï¼ˆå®é™…è®­ç»ƒï¼‰")
    print("=" * 70)

    # ========== å‡†å¤‡æ•°æ® ==========
    np.random.seed(42)
    torch.manual_seed(42)

    X, y = make_moons(n_samples=200, noise=0.2, random_state=42)
    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)

    # è½¬ä¸º PyTorch tensors
    X_train = torch.FloatTensor(X_train)
    y_train = torch.FloatTensor(y_train).unsqueeze(1)
    X_test = torch.FloatTensor(X_test)
    y_test = torch.FloatTensor(y_test).unsqueeze(1)

    train_dataset = TensorDataset(X_train, y_train)
    train_loader = DataLoader(train_dataset, batch_size=16, shuffle=True)

    print(f"\næ•°æ®é›†:")
    print(f"  è®­ç»ƒé›†: {len(X_train)} samples")
    print(f"  æµ‹è¯•é›†: {len(X_test)} samples")

    # ========== å®šä¹‰æ¨¡å‹ ==========
    def create_model():
        return nn.Sequential(
            nn.Linear(2, 16),
            nn.ReLU(),
            nn.Linear(16, 8),
            nn.ReLU(),
            nn.Linear(8, 1),
            nn.Sigmoid()
        )

    # ========== è®­ç»ƒä¸åŒä¼˜åŒ–å™¨ ==========
    optimizers_config = {
        'SGD': {'class': optim.SGD, 'kwargs': {'lr': 0.1}},
        'SGD+Momentum': {'class': optim.SGD, 'kwargs': {'lr': 0.1, 'momentum': 0.9}},
        'RMSprop': {'class': optim.RMSprop, 'kwargs': {'lr': 0.01}},
        'Adam': {'class': optim.Adam, 'kwargs': {'lr': 0.01}},
        'AdamW': {'class': optim.AdamW, 'kwargs': {'lr': 0.01, 'weight_decay': 0.01}},
    }

    criterion = nn.BCELoss()
    n_epochs = 50

    results = {}

    for name, config in optimizers_config.items():
        print(f"\nè®­ç»ƒ {name}...")

        model = create_model()
        optimizer = config['class'](model.parameters(), **config['kwargs'])

        train_losses = []
        test_losses = []

        for epoch in range(n_epochs):
            # è®­ç»ƒ
            model.train()
            epoch_loss = 0
            for batch_X, batch_y in train_loader:
                optimizer.zero_grad()
                output = model(batch_X)
                loss = criterion(output, batch_y)
                loss.backward()
                optimizer.step()
                epoch_loss += loss.item()

            train_losses.append(epoch_loss / len(train_loader))

            # æµ‹è¯•
            model.eval()
            with torch.no_grad():
                test_output = model(X_test)
                test_loss = criterion(test_output, y_test)
                test_losses.append(test_loss.item())

        results[name] = {
            'train_losses': train_losses,
            'test_losses': test_losses,
            'final_test_loss': test_losses[-1]
        }

        print(f"  æœ€ç»ˆæµ‹è¯•æŸå¤±: {test_losses[-1]:.4f}")

    # ========== å¯è§†åŒ– ==========
    visualize_optimizer_comparison(results, n_epochs)

    return results


def visualize_optimizer_comparison(results, n_epochs):
    """å¯è§†åŒ–ä¼˜åŒ–å™¨å¯¹æ¯”"""
    print("\nå¯è§†åŒ–ä¼˜åŒ–å™¨å¯¹æ¯”...")

    fig, axes = plt.subplots(1, 2, figsize=(14, 6))

    colors = ['red', 'blue', 'green', 'purple', 'orange']
    epochs_range = range(1, n_epochs + 1)

    # Plot 1: è®­ç»ƒæŸå¤±
    for (name, data), color in zip(results.items(), colors):
        axes[0].plot(epochs_range, data['train_losses'], label=name,
                    color=color, linewidth=2, alpha=0.8)

    axes[0].set_xlabel('Epoch', fontsize=11)
    axes[0].set_ylabel('Training Loss', fontsize=11)
    axes[0].set_title('Training Loss Comparison', fontsize=12, fontweight='bold')
    axes[0].legend(fontsize=9)
    axes[0].grid(alpha=0.3)

    # Plot 2: æµ‹è¯•æŸå¤±
    for (name, data), color in zip(results.items(), colors):
        axes[1].plot(epochs_range, data['test_losses'], label=name,
                    color=color, linewidth=2, alpha=0.8)

    axes[1].set_xlabel('Epoch', fontsize=11)
    axes[1].set_ylabel('Test Loss', fontsize=11)
    axes[1].set_title('Test Loss Comparison', fontsize=12, fontweight='bold')
    axes[1].legend(fontsize=9)
    axes[1].grid(alpha=0.3)

    plt.tight_layout()
    plt.savefig('pytorch_optimizer_comparison.png', dpi=100, bbox_inches='tight')
    print("ğŸ“Š ä¼˜åŒ–å™¨å¯¹æ¯”å·²ä¿å­˜: pytorch_optimizer_comparison.png")
    plt.close()


# ==================== 2. å­¦ä¹ ç‡è°ƒåº¦ ====================
def demo_lr_schedulers():
    """
    æ¼”ç¤º PyTorch çš„å­¦ä¹ ç‡è°ƒåº¦å™¨

    ====================================================================
    ğŸ”‘ ä¸ºä»€ä¹ˆéœ€è¦å­¦ä¹ ç‡è°ƒåº¦ï¼Ÿ
    ====================================================================

    å›ºå®šå­¦ä¹ ç‡é—®é¢˜ï¼š
        - å¼€å§‹ï¼šå­¦ä¹ ç‡å¤ªå¤§ â†’ éœ‡è¡
        - å¼€å§‹ï¼šå­¦ä¹ ç‡å¤ªå° â†’ å¤ªæ…¢
        - åæœŸï¼šå­¦ä¹ ç‡å¤ªå¤§ â†’ æ— æ³•ç²¾ç»†è°ƒæ•´

    è§£å†³æ–¹æ¡ˆï¼šåŠ¨æ€è°ƒæ•´å­¦ä¹ ç‡
        - å¼€å§‹ï¼šå¤§å­¦ä¹ ç‡ï¼Œå¿«é€Ÿæ¥è¿‘
        - åæœŸï¼šå°å­¦ä¹ ç‡ï¼Œç²¾ç»†è°ƒæ•´

    ====================================================================
    """
    print("\n" + "=" * 70)
    print("3. å­¦ä¹ ç‡è°ƒåº¦å™¨æ¼”ç¤º")
    print("=" * 70)

    model = nn.Linear(10, 1)
    optimizer = optim.SGD(model.parameters(), lr=0.1)

    # ========== StepLR ==========
    print("\n" + "-" * 70)
    print("StepLR - é˜¶æ¢¯è¡°å‡")
    print("-" * 70)
    print("""
æ¯éš” step_size ä¸ª epochï¼Œå­¦ä¹ ç‡ä¹˜ä»¥ gamma

ä¾‹å­: step_size=10, gamma=0.5
    epoch 0-9:   lr = 0.1
    epoch 10-19: lr = 0.05
    epoch 20-29: lr = 0.025
    """)

    scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=10, gamma=0.5)
    print(f"åˆ›å»º StepLR: {scheduler}")

    # ========== ExponentialLR ==========
    print("\n" + "-" * 70)
    print("ExponentialLR - æŒ‡æ•°è¡°å‡")
    print("-" * 70)
    print("""
æ¯ä¸ª epochï¼Œå­¦ä¹ ç‡ä¹˜ä»¥ gamma

lr = lrâ‚€ Ã— gamma^epoch
å¹³æ»‘ä¸‹é™
    """)

    scheduler = optim.lr_scheduler.ExponentialLR(optimizer, gamma=0.95)

    # ========== CosineAnnealingLR ==========
    print("\n" + "-" * 70)
    print("CosineAnnealingLR - ä½™å¼¦é€€ç«")
    print("-" * 70)
    print("""
å­¦ä¹ ç‡æŒ‰ä½™å¼¦æ›²çº¿ä¸‹é™

lr = lr_min + (lr_max - lr_min) Ã— (1 + cos(Ï€Ã—T_cur/T_max)) / 2

ç‰¹ç‚¹: å¹³æ»‘ä¸‹é™ï¼Œç°ä»£è®­ç»ƒå¸¸ç”¨
    """)

    scheduler = optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=100, eta_min=0)

    # ========== ReduceLROnPlateau ==========
    print("\n" + "-" * 70)
    print("ReduceLROnPlateau - è‡ªé€‚åº”é™ä½")
    print("-" * 70)
    print("""
å½“æŒ‡æ ‡ï¼ˆå¦‚éªŒè¯æŸå¤±ï¼‰åœæ­¢æ”¹å–„æ—¶ï¼Œé™ä½å­¦ä¹ ç‡

é€‚ç”¨åœºæ™¯ï¼šä¸çŸ¥é“ä½•æ—¶é™ä½å­¦ä¹ ç‡
è‡ªåŠ¨æ£€æµ‹plateauï¼ˆå¹³å°æœŸï¼‰
    """)

    scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min',
                                                     factor=0.5, patience=10)

    # ========== CyclicLR ==========
    print("\n" + "-" * 70)
    print("CyclicLR - å¾ªç¯å­¦ä¹ ç‡")
    print("-" * 70)
    print("""
å­¦ä¹ ç‡åœ¨ base_lr å’Œ max_lr ä¹‹é—´å¾ªç¯

æœ‰åŠ©äºè·³å‡ºå±€éƒ¨æœ€å°å€¼
Leslie Smith æå‡ºï¼ˆè¶…æ”¶æ•›ï¼‰
    """)

    scheduler = optim.lr_scheduler.CyclicLR(optimizer, base_lr=0.001,
                                           max_lr=0.1, step_size_up=2000)

    # ========== OneCycleLR ==========
    print("\n" + "-" * 70)
    print("OneCycleLR - å•å‘¨æœŸç­–ç•¥")
    print("-" * 70)
    print("""
å…ˆå¢å¤§å­¦ä¹ ç‡ï¼ˆwarm-upï¼‰ï¼Œå†å‡å°

1. Warm-up: 0 â†’ max_lr
2. Annealing: max_lr â†’ min_lr

ç°ä»£è®­ç»ƒå¸¸ç”¨ï¼Œæ”¶æ•›å¿«
    """)

    scheduler = optim.lr_scheduler.OneCycleLR(optimizer, max_lr=0.1,
                                              steps_per_epoch=100, epochs=10)

    # å¯è§†åŒ–æ‰€æœ‰è°ƒåº¦å™¨
    visualize_lr_schedulers()


def visualize_lr_schedulers():
    """å¯è§†åŒ–ä¸åŒçš„å­¦ä¹ ç‡è°ƒåº¦å™¨"""
    print("\nå¯è§†åŒ–å­¦ä¹ ç‡è°ƒåº¦ç­–ç•¥...")

    model = nn.Linear(1, 1)
    n_epochs = 100
    steps_per_epoch = 10

    schedules = {}

    # StepLR
    optimizer = optim.SGD(model.parameters(), lr=0.1)
    scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=20, gamma=0.5)
    lrs = []
    for epoch in range(n_epochs):
        lrs.append(optimizer.param_groups[0]['lr'])
        for _ in range(steps_per_epoch):
            optimizer.step()
        scheduler.step()
    schedules['StepLR'] = lrs

    # ExponentialLR
    optimizer = optim.SGD(model.parameters(), lr=0.1)
    scheduler = optim.lr_scheduler.ExponentialLR(optimizer, gamma=0.95)
    lrs = []
    for epoch in range(n_epochs):
        lrs.append(optimizer.param_groups[0]['lr'])
        for _ in range(steps_per_epoch):
            optimizer.step()
        scheduler.step()
    schedules['ExponentialLR'] = lrs

    # CosineAnnealingLR
    optimizer = optim.SGD(model.parameters(), lr=0.1)
    scheduler = optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=n_epochs, eta_min=0)
    lrs = []
    for epoch in range(n_epochs):
        lrs.append(optimizer.param_groups[0]['lr'])
        for _ in range(steps_per_epoch):
            optimizer.step()
        scheduler.step()
    schedules['CosineAnnealingLR'] = lrs

    # OneCycleLR
    optimizer = optim.SGD(model.parameters(), lr=0.1)
    scheduler = optim.lr_scheduler.OneCycleLR(optimizer, max_lr=0.1,
                                              steps_per_epoch=steps_per_epoch,
                                              epochs=n_epochs)
    lrs = []
    for epoch in range(n_epochs):
        epoch_lrs = []
        for _ in range(steps_per_epoch):
            optimizer.step()
            epoch_lrs.append(optimizer.param_groups[0]['lr'])
            scheduler.step()
        lrs.append(np.mean(epoch_lrs))
    schedules['OneCycleLR'] = lrs

    # Constant
    schedules['Constant'] = [0.1] * n_epochs

    # ç»˜å›¾
    plt.figure(figsize=(12, 7))

    colors = ['gray', 'blue', 'green', 'red', 'purple']
    epochs_range = range(n_epochs)

    for (name, lrs), color in zip(schedules.items(), colors):
        plt.plot(epochs_range, lrs, label=name, linewidth=2.5, alpha=0.8, color=color)

    plt.xlabel('Epoch', fontsize=11)
    plt.ylabel('Learning Rate', fontsize=11)
    plt.title('PyTorch Learning Rate Schedulers', fontsize=12, fontweight='bold')
    plt.legend(fontsize=10)
    plt.grid(alpha=0.3)
    plt.tight_layout()
    plt.savefig('pytorch_lr_schedulers.png', dpi=100, bbox_inches='tight')
    print("ğŸ“Š å­¦ä¹ ç‡è°ƒåº¦å™¨å·²ä¿å­˜: pytorch_lr_schedulers.png")
    plt.close()


# ==================== 3. æ­£åˆ™åŒ–æŠ€æœ¯ ====================
class ModelWithDropout(nn.Module):
    """å¸¦ Dropout çš„æ¨¡å‹"""

    def __init__(self, input_dim, hidden_dim, output_dim, dropout_rate=0.5):
        super(ModelWithDropout, self).__init__()

        self.fc1 = nn.Linear(input_dim, hidden_dim)
        self.dropout1 = nn.Dropout(dropout_rate)
        self.fc2 = nn.Linear(hidden_dim, hidden_dim)
        self.dropout2 = nn.Dropout(dropout_rate)
        self.fc3 = nn.Linear(hidden_dim, output_dim)

    def forward(self, x):
        x = torch.relu(self.fc1(x))
        x = self.dropout1(x)  # Dropout
        x = torch.relu(self.fc2(x))
        x = self.dropout2(x)  # Dropout
        x = torch.sigmoid(self.fc3(x))
        return x


class ModelWithBatchNorm(nn.Module):
    """å¸¦ Batch Normalization çš„æ¨¡å‹"""

    def __init__(self, input_dim, hidden_dim, output_dim):
        super(ModelWithBatchNorm, self).__init__()

        self.fc1 = nn.Linear(input_dim, hidden_dim)
        self.bn1 = nn.BatchNorm1d(hidden_dim)  # Batch Norm
        self.fc2 = nn.Linear(hidden_dim, hidden_dim)
        self.bn2 = nn.BatchNorm1d(hidden_dim)  # Batch Norm
        self.fc3 = nn.Linear(hidden_dim, output_dim)

    def forward(self, x):
        x = self.fc1(x)
        x = self.bn1(x)  # Batch Norm
        x = torch.relu(x)
        x = self.fc2(x)
        x = self.bn2(x)  # Batch Norm
        x = torch.relu(x)
        x = torch.sigmoid(self.fc3(x))
        return x


def demo_regularization():
    """
    æ¼”ç¤ºæ­£åˆ™åŒ–æŠ€æœ¯

    ====================================================================
    ğŸ”‘ Dropout
    ====================================================================

    ä½œç”¨ï¼š
        - è®­ç»ƒæ—¶ï¼šéšæœºå…³é—­ä¸€äº›ç¥ç»å…ƒï¼ˆè®¾ä¸º0ï¼‰
        - æµ‹è¯•æ—¶ï¼šæ‰€æœ‰ç¥ç»å…ƒå·¥ä½œ

    ä¸ºä»€ä¹ˆæœ‰æ•ˆï¼š
        - é˜²æ­¢ç¥ç»å…ƒå…±é€‚åº”ï¼ˆco-adaptationï¼‰
        - ç±»ä¼¼é›†æˆå­¦ä¹ ï¼ˆensembleï¼‰
        - æ¯ä¸ªç¥ç»å…ƒå­¦åˆ°æ›´é²æ£’çš„ç‰¹å¾

    ä½¿ç”¨ï¼š
        nn.Dropout(p=0.5)  # 50% çš„ç¥ç»å…ƒè¢«å…³é—­

    ====================================================================
    ğŸ”‘ Batch Normalization
    ====================================================================

    ä½œç”¨ï¼š
        - å½’ä¸€åŒ–æ¯ä¸ª batch çš„æ¿€æ´»å€¼
        - å‡å€¼=0ï¼Œæ–¹å·®=1

    ä¸ºä»€ä¹ˆæœ‰æ•ˆï¼š
        - ç¨³å®šè®­ç»ƒï¼ˆå‡å°‘å†…éƒ¨åå˜é‡åç§»ï¼‰
        - å…è®¸æ›´å¤§çš„å­¦ä¹ ç‡
        - è‡ªå¸¦æ­£åˆ™åŒ–æ•ˆæœ

    ä½¿ç”¨ï¼š
        nn.BatchNorm1d(num_features)  # å…¨è¿æ¥å±‚
        nn.BatchNorm2d(num_channels)  # å·ç§¯å±‚

    ====================================================================
    ğŸ”‘ L2 Regularization (Weight Decay)
    ====================================================================

    ä½œç”¨ï¼š
        - æƒ©ç½šå¤§çš„æƒé‡
        - Loss = Loss + Î» Ã— ||W||Â²

    ä¸ºä»€ä¹ˆæœ‰æ•ˆï¼š
        - é˜²æ­¢æƒé‡è¿‡å¤§
        - é¼“åŠ±ç®€å•æ¨¡å‹

    ä½¿ç”¨ï¼š
        optimizer = optim.Adam(model.parameters(), weight_decay=0.01)

    ====================================================================
    """
    print("\n" + "=" * 70)
    print("4. æ­£åˆ™åŒ–æŠ€æœ¯æ¼”ç¤º")
    print("=" * 70)

    # å‡†å¤‡æ•°æ®ï¼ˆå®¹æ˜“è¿‡æ‹Ÿåˆçš„å°æ•°æ®é›†ï¼‰
    np.random.seed(42)
    torch.manual_seed(42)

    X, y = make_moons(n_samples=100, noise=0.3, random_state=42)
    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)

    X_train = torch.FloatTensor(X_train)
    y_train = torch.FloatTensor(y_train).unsqueeze(1)
    X_test = torch.FloatTensor(X_test)
    y_test = torch.FloatTensor(y_test).unsqueeze(1)

    train_dataset = TensorDataset(X_train, y_train)
    train_loader = DataLoader(train_dataset, batch_size=16, shuffle=True)

    print(f"\nå°æ•°æ®é›†ï¼ˆå®¹æ˜“è¿‡æ‹Ÿåˆï¼‰:")
    print(f"  è®­ç»ƒé›†: {len(X_train)} samples")
    print(f"  æµ‹è¯•é›†: {len(X_test)} samples")

    # è®­ç»ƒä¸åŒæ¨¡å‹
    models_config = {
        'No Regularization': nn.Sequential(
            nn.Linear(2, 32), nn.ReLU(),
            nn.Linear(32, 32), nn.ReLU(),
            nn.Linear(32, 1), nn.Sigmoid()
        ),
        'With Dropout': ModelWithDropout(2, 32, 1, dropout_rate=0.5),
        'With BatchNorm': ModelWithBatchNorm(2, 32, 1),
    }

    criterion = nn.BCELoss()
    n_epochs = 100

    results = {}

    for name, model in models_config.items():
        print(f"\nè®­ç»ƒ {name}...")

        # L2 æ­£åˆ™åŒ– (weight_decay)
        if name == 'No Regularization':
            optimizer = optim.Adam(model.parameters(), lr=0.01)
        else:
            optimizer = optim.Adam(model.parameters(), lr=0.01, weight_decay=0.01)

        train_losses = []
        test_losses = []

        for epoch in range(n_epochs):
            # è®­ç»ƒ
            model.train()
            epoch_loss = 0
            for batch_X, batch_y in train_loader:
                optimizer.zero_grad()
                output = model(batch_X)
                loss = criterion(output, batch_y)
                loss.backward()
                optimizer.step()
                epoch_loss += loss.item()

            train_losses.append(epoch_loss / len(train_loader))

            # æµ‹è¯•
            model.eval()
            with torch.no_grad():
                test_output = model(X_test)
                test_loss = criterion(test_output, y_test)
                test_losses.append(test_loss.item())

        results[name] = {
            'train_losses': train_losses,
            'test_losses': test_losses
        }

        print(f"  æœ€ç»ˆè®­ç»ƒæŸå¤±: {train_losses[-1]:.4f}")
        print(f"  æœ€ç»ˆæµ‹è¯•æŸå¤±: {test_losses[-1]:.4f}")
        print(f"  è¿‡æ‹Ÿåˆå·®è·: {abs(train_losses[-1] - test_losses[-1]):.4f}")

    # å¯è§†åŒ–
    visualize_regularization(results, n_epochs)


def visualize_regularization(results, n_epochs):
    """å¯è§†åŒ–æ­£åˆ™åŒ–æ•ˆæœ"""
    print("\nå¯è§†åŒ–æ­£åˆ™åŒ–æ•ˆæœ...")

    fig, axes = plt.subplots(1, 2, figsize=(14, 6))

    colors = ['red', 'blue', 'green']
    epochs_range = range(1, n_epochs + 1)

    # Plot 1: è®­ç»ƒæŸå¤±
    for (name, data), color in zip(results.items(), colors):
        axes[0].plot(epochs_range, data['train_losses'], label=name,
                    color=color, linewidth=2, alpha=0.8)

    axes[0].set_xlabel('Epoch', fontsize=11)
    axes[0].set_ylabel('Training Loss', fontsize=11)
    axes[0].set_title('Training Loss (Lower = Better)', fontsize=12, fontweight='bold')
    axes[0].legend(fontsize=9)
    axes[0].grid(alpha=0.3)

    # Plot 2: æµ‹è¯•æŸå¤±
    for (name, data), color in zip(results.items(), colors):
        axes[1].plot(epochs_range, data['test_losses'], label=name,
                    color=color, linewidth=2, alpha=0.8)

    axes[1].set_xlabel('Epoch', fontsize=11)
    axes[1].set_ylabel('Test Loss', fontsize=11)
    axes[1].set_title('Test Loss (Lower = Better, Shows Overfitting)',
                     fontsize=12, fontweight='bold')
    axes[1].legend(fontsize=9)
    axes[1].grid(alpha=0.3)

    plt.tight_layout()
    plt.savefig('pytorch_regularization.png', dpi=100, bbox_inches='tight')
    print("ğŸ“Š æ­£åˆ™åŒ–æ•ˆæœå·²ä¿å­˜: pytorch_regularization.png")
    plt.close()


# ==================== 4. æ¢¯åº¦è£å‰ª ====================
def demo_gradient_clipping():
    """
    æ¼”ç¤ºæ¢¯åº¦è£å‰ª

    ====================================================================
    ğŸ”‘ ä¸ºä»€ä¹ˆéœ€è¦æ¢¯åº¦è£å‰ªï¼Ÿ
    ====================================================================

    é—®é¢˜ï¼šæ¢¯åº¦çˆ†ç‚¸
        - æ¢¯åº¦å¤ªå¤§ â†’ å‚æ•°æ›´æ–°å¤ªå¤§ â†’ å‘æ•£
        - å¸¸è§äºï¼šRNNã€LSTMã€æ·±å±‚ç½‘ç»œ

    è§£å†³ï¼šæ¢¯åº¦è£å‰ª
        - é™åˆ¶æ¢¯åº¦çš„æœ€å¤§å€¼
        - ä¿æŒæ¢¯åº¦æ–¹å‘ï¼Œåªç¼©æ”¾å¤§å°

    ====================================================================
    ğŸ”‘ ä¸¤ç§æ–¹å¼
    ====================================================================

    1. Clip by Valueï¼ˆæŒ‰å€¼è£å‰ªï¼‰
       gradient = max(min(gradient, max_value), -max_value)

    2. Clip by Normï¼ˆæŒ‰èŒƒæ•°è£å‰ªï¼‰
       if ||gradient|| > max_norm:
           gradient = gradient Ã— (max_norm / ||gradient||)

    PyTorch å®ç°ï¼ˆæ¨è Clip by Normï¼‰ï¼š
       torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)

    ====================================================================
    """
    print("\n" + "=" * 70)
    print("5. æ¢¯åº¦è£å‰ªæ¼”ç¤º")
    print("=" * 70)

    print("""
æ¢¯åº¦è£å‰ªåœ¨è®­ç»ƒå¾ªç¯ä¸­ä½¿ç”¨ï¼š

for epoch in range(n_epochs):
    for batch in dataloader:
        optimizer.zero_grad()
        loss = criterion(model(batch), target)
        loss.backward()

        # æ¢¯åº¦è£å‰ªï¼ˆåœ¨ optimizer.step() ä¹‹å‰ï¼‰
        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)

        optimizer.step()

ä½•æ—¶ä½¿ç”¨ï¼š
    - RNN/LSTMï¼šé˜²æ­¢æ¢¯åº¦çˆ†ç‚¸ï¼ˆmax_norm=1.0ï¼‰
    - Transformerï¼šç¨³å®šè®­ç»ƒï¼ˆmax_norm=1.0ï¼‰
    - æ·±å±‚ç½‘ç»œï¼šå‘ç°æ¢¯åº¦çˆ†ç‚¸æ—¶
    - GANï¼šç¨³å®šè®­ç»ƒ

å¸¸ç”¨å€¼ï¼š
    - max_norm = 1.0ï¼ˆRNN/LSTMï¼‰
    - max_norm = 5.0ï¼ˆTransformerï¼‰
    - max_norm = 10.0ï¼ˆGANï¼‰
    """)


# ==================== 5. æƒé‡åˆå§‹åŒ– ====================
def demo_weight_initialization():
    """
    æ¼”ç¤ºæƒé‡åˆå§‹åŒ–

    ====================================================================
    ğŸ”‘ ä¸ºä»€ä¹ˆæƒé‡åˆå§‹åŒ–é‡è¦ï¼Ÿ
    ====================================================================

    ç³Ÿç³•çš„åˆå§‹åŒ–ï¼š
        - å…¨0ï¼šæ‰€æœ‰ç¥ç»å…ƒå­¦åˆ°åŒæ ·çš„ä¸œè¥¿
        - å¤ªå¤§ï¼šæ¿€æ´»å€¼çˆ†ç‚¸
        - å¤ªå°ï¼šæ¿€æ´»å€¼æ¶ˆå¤±

    å¥½çš„åˆå§‹åŒ–ï¼š
        - æ‰“ç ´å¯¹ç§°æ€§
        - ä¿æŒæ¿€æ´»å€¼æ–¹å·®
        - åŠ é€Ÿæ”¶æ•›

    ====================================================================
    ğŸ”‘ å¸¸ç”¨åˆå§‹åŒ–æ–¹æ³•
    ====================================================================

    1. Xavier/Glorot Initialization
       - ç”¨äº: Sigmoid, Tanh
       - å…¬å¼: Uniform(-âˆš(6/(fan_in+fan_out)), âˆš(6/(fan_in+fan_out)))

    2. He Initialization
       - ç”¨äº: ReLU
       - å…¬å¼: Normal(0, âˆš(2/fan_in))

    3. Orthogonal Initialization
       - ç”¨äº: RNN, LSTM
       - åˆ›å»ºæ­£äº¤çŸ©é˜µ

    ====================================================================
    """
    print("\n" + "=" * 70)
    print("6. æƒé‡åˆå§‹åŒ–æ¼”ç¤º")
    print("=" * 70)

    model = nn.Sequential(
        nn.Linear(10, 20),
        nn.ReLU(),
        nn.Linear(20, 10)
    )

    # ========== Xavier/Glorot Initialization ==========
    print("\n" + "-" * 70)
    print("Xavier/Glorot Initialization (for Sigmoid/Tanh)")
    print("-" * 70)

    def init_xavier(m):
        if isinstance(m, nn.Linear):
            nn.init.xavier_uniform_(m.weight)
            if m.bias is not None:
                nn.init.zeros_(m.bias)

    model.apply(init_xavier)
    print("åº”ç”¨ Xavier åˆå§‹åŒ–")

    # ========== He Initialization ==========
    print("\n" + "-" * 70)
    print("He Initialization (for ReLU)")
    print("-" * 70)

    def init_he(m):
        if isinstance(m, nn.Linear):
            nn.init.kaiming_normal_(m.weight, mode='fan_in', nonlinearity='relu')
            if m.bias is not None:
                nn.init.zeros_(m.bias)

    model.apply(init_he)
    print("åº”ç”¨ He åˆå§‹åŒ–ï¼ˆæ¨èç”¨äºReLUï¼‰")

    # ========== Orthogonal Initialization ==========
    print("\n" + "-" * 70)
    print("Orthogonal Initialization (for RNN/LSTM)")
    print("-" * 70)

    def init_orthogonal(m):
        if isinstance(m, nn.Linear):
            nn.init.orthogonal_(m.weight)
            if m.bias is not None:
                nn.init.zeros_(m.bias)

    model.apply(init_orthogonal)
    print("åº”ç”¨ Orthogonal åˆå§‹åŒ–")

    print("""
ä½¿ç”¨å»ºè®®ï¼š
    - ReLU æ¿€æ´»å‡½æ•° â†’ He Initializationï¼ˆé»˜è®¤ï¼‰
    - Sigmoid/Tanh â†’ Xavier Initialization
    - RNN/LSTM â†’ Orthogonal Initialization
    - PyTorch é»˜è®¤ä½¿ç”¨ Kaiming (He) åˆå§‹åŒ–
    """)


# ==================== 6. å®Œæ•´è®­ç»ƒç¤ºä¾‹ ====================
def complete_training_example():
    """
    å®Œæ•´çš„è®­ç»ƒç¤ºä¾‹ï¼ˆæ•´åˆæ‰€æœ‰æŠ€æœ¯ï¼‰

    å±•ç¤ºå¦‚ä½•åœ¨å®é™…è®­ç»ƒä¸­ç»„åˆä½¿ç”¨ï¼š
    - ä¼˜åŒ–å™¨ (Adam)
    - å­¦ä¹ ç‡è°ƒåº¦ (OneCycleLR)
    - æ­£åˆ™åŒ– (Dropout + BatchNorm + Weight Decay)
    - æ¢¯åº¦è£å‰ª
    - æƒé‡åˆå§‹åŒ–
    """
    print("\n" + "=" * 70)
    print("7. å®Œæ•´è®­ç»ƒç¤ºä¾‹ï¼ˆBest Practicesï¼‰")
    print("=" * 70)

    # æ•°æ®
    np.random.seed(42)
    torch.manual_seed(42)

    X, y = make_circles(n_samples=500, noise=0.1, factor=0.5, random_state=42)
    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)

    X_train = torch.FloatTensor(X_train)
    y_train = torch.FloatTensor(y_train).unsqueeze(1)
    X_test = torch.FloatTensor(X_test)
    y_test = torch.FloatTensor(y_test).unsqueeze(1)

    train_dataset = TensorDataset(X_train, y_train)
    train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)

    # æ¨¡å‹ï¼ˆä½¿ç”¨ BatchNorm + Dropoutï¼‰
    model = nn.Sequential(
        nn.Linear(2, 64),
        nn.BatchNorm1d(64),
        nn.ReLU(),
        nn.Dropout(0.3),
        nn.Linear(64, 32),
        nn.BatchNorm1d(32),
        nn.ReLU(),
        nn.Dropout(0.3),
        nn.Linear(32, 1),
        nn.Sigmoid()
    )

    # æƒé‡åˆå§‹åŒ–ï¼ˆHeï¼‰
    def init_weights(m):
        if isinstance(m, nn.Linear):
            nn.init.kaiming_normal_(m.weight, nonlinearity='relu')
            if m.bias is not None:
                nn.init.zeros_(m.bias)

    model.apply(init_weights)

    # ä¼˜åŒ–å™¨ï¼ˆAdam + Weight Decayï¼‰
    optimizer = optim.AdamW(model.parameters(), lr=0.01, weight_decay=0.01)

    # å­¦ä¹ ç‡è°ƒåº¦ï¼ˆOneCycleLRï¼‰
    n_epochs = 50
    scheduler = optim.lr_scheduler.OneCycleLR(
        optimizer,
        max_lr=0.01,
        steps_per_epoch=len(train_loader),
        epochs=n_epochs
    )

    # æŸå¤±å‡½æ•°
    criterion = nn.BCELoss()

    print(f"\né…ç½®:")
    print(f"  æ¨¡å‹: 3-layer MLP with BatchNorm + Dropout")
    print(f"  ä¼˜åŒ–å™¨: AdamW (weight_decay=0.01)")
    print(f"  å­¦ä¹ ç‡è°ƒåº¦: OneCycleLR")
    print(f"  æ­£åˆ™åŒ–: BatchNorm + Dropout(0.3) + Weight Decay")
    print(f"  æ¢¯åº¦è£å‰ª: max_norm=1.0")
    print(f"  æƒé‡åˆå§‹åŒ–: He (Kaiming)")

    print(f"\nå¼€å§‹è®­ç»ƒ...")

    train_losses = []
    test_losses = []
    lrs = []

    for epoch in range(n_epochs):
        # è®­ç»ƒ
        model.train()
        epoch_loss = 0

        for batch_X, batch_y in train_loader:
            optimizer.zero_grad()
            output = model(batch_X)
            loss = criterion(output, batch_y)
            loss.backward()

            # æ¢¯åº¦è£å‰ª
            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)

            optimizer.step()
            scheduler.step()  # OneCycleLR æ¯ä¸ª step æ›´æ–°

            epoch_loss += loss.item()

        train_losses.append(epoch_loss / len(train_loader))
        lrs.append(optimizer.param_groups[0]['lr'])

        # æµ‹è¯•
        model.eval()
        with torch.no_grad():
            test_output = model(X_test)
            test_loss = criterion(test_output, y_test)
            test_losses.append(test_loss.item())

        if (epoch + 1) % 10 == 0:
            print(f"Epoch {epoch+1:2d}/{n_epochs} | "
                  f"Train Loss: {train_losses[-1]:.4f} | "
                  f"Test Loss: {test_losses[-1]:.4f} | "
                  f"LR: {lrs[-1]:.6f}")

    print(f"\nè®­ç»ƒå®Œæˆï¼")
    print(f"  æœ€ç»ˆè®­ç»ƒæŸå¤±: {train_losses[-1]:.4f}")
    print(f"  æœ€ç»ˆæµ‹è¯•æŸå¤±: {test_losses[-1]:.4f}")

    # å¯è§†åŒ–
    visualize_complete_training(train_losses, test_losses, lrs, n_epochs)


def visualize_complete_training(train_losses, test_losses, lrs, n_epochs):
    """å¯è§†åŒ–å®Œæ•´è®­ç»ƒè¿‡ç¨‹"""
    print("\nå¯è§†åŒ–è®­ç»ƒè¿‡ç¨‹...")

    fig, axes = plt.subplots(1, 2, figsize=(14, 6))

    epochs_range = range(1, n_epochs + 1)

    # Plot 1: æŸå¤±æ›²çº¿
    axes[0].plot(epochs_range, train_losses, 'b-', label='Train Loss', linewidth=2)
    axes[0].plot(epochs_range, test_losses, 'r-', label='Test Loss', linewidth=2)
    axes[0].set_xlabel('Epoch', fontsize=11)
    axes[0].set_ylabel('Loss', fontsize=11)
    axes[0].set_title('Training Progress', fontsize=12, fontweight='bold')
    axes[0].legend(fontsize=10)
    axes[0].grid(alpha=0.3)

    # Plot 2: å­¦ä¹ ç‡å˜åŒ–
    axes[1].plot(epochs_range, lrs, 'g-', linewidth=2)
    axes[1].set_xlabel('Epoch', fontsize=11)
    axes[1].set_ylabel('Learning Rate', fontsize=11)
    axes[1].set_title('Learning Rate Schedule (OneCycleLR)', fontsize=12, fontweight='bold')
    axes[1].grid(alpha=0.3)

    plt.tight_layout()
    plt.savefig('pytorch_complete_training.png', dpi=100, bbox_inches='tight')
    print("ğŸ“Š å®Œæ•´è®­ç»ƒè¿‡ç¨‹å·²ä¿å­˜: pytorch_complete_training.png")
    plt.close()


# ==================== ä¸»ç¨‹åº ====================
def main():
    print("=" * 70)
    print("ä¼˜åŒ–ä¸æ­£åˆ™åŒ–æŠ€æœ¯ - PyTorch å®ç°")
    print("=" * 70)

    # 1. ä¼˜åŒ–å™¨åŸºç¡€
    demo_pytorch_optimizers()

    # 2. ä¼˜åŒ–å™¨å¯¹æ¯”
    compare_optimizers_pytorch()

    # 3. å­¦ä¹ ç‡è°ƒåº¦
    demo_lr_schedulers()

    # 4. æ­£åˆ™åŒ–
    demo_regularization()

    # 5. æ¢¯åº¦è£å‰ª
    demo_gradient_clipping()

    # 6. æƒé‡åˆå§‹åŒ–
    demo_weight_initialization()

    # 7. å®Œæ•´ç¤ºä¾‹
    complete_training_example()

    # 8. æ€»ç»“
    print("\n" + "=" * 70)
    print("âœ… æ ¸å¿ƒè¦ç‚¹æ€»ç»“")
    print("=" * 70)
    print("""
1. PyTorch ä¼˜åŒ–å™¨

   å¸¸ç”¨ä¼˜åŒ–å™¨ï¼š
   - optim.SGD(params, lr, momentum)
   - optim.Adam(params, lr, betas)
   - optim.AdamW(params, lr, weight_decay)  â† æ¨è

   ä½¿ç”¨ï¼š
   optimizer = optim.Adam(model.parameters(), lr=0.001)
   optimizer.zero_grad()
   loss.backward()
   optimizer.step()

2. å­¦ä¹ ç‡è°ƒåº¦

   å¸¸ç”¨è°ƒåº¦å™¨ï¼š
   - StepLR: é˜¶æ¢¯è¡°å‡
   - CosineAnnealingLR: ä½™å¼¦é€€ç«
   - OneCycleLR: å•å‘¨æœŸï¼ˆæ¨èï¼‰
   - ReduceLROnPlateau: è‡ªé€‚åº”

   ä½¿ç”¨ï¼š
   scheduler = optim.lr_scheduler.OneCycleLR(optimizer, ...)
   scheduler.step()  # æ¯ä¸ª epoch æˆ– batch åè°ƒç”¨

3. æ­£åˆ™åŒ–æŠ€æœ¯

   Dropout:
   nn.Dropout(p=0.5)  # è®­ç»ƒæ—¶å…³é—­50%ç¥ç»å…ƒ

   Batch Normalization:
   nn.BatchNorm1d(num_features)  # å…¨è¿æ¥å±‚
   nn.BatchNorm2d(num_channels)  # å·ç§¯å±‚

   Weight Decay (L2):
   optimizer = optim.Adam(params, weight_decay=0.01)

4. æ¢¯åº¦è£å‰ª

   é˜²æ­¢æ¢¯åº¦çˆ†ç‚¸ï¼š
   torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)

   ä½¿ç”¨åœºæ™¯ï¼š
   - RNN/LSTM: max_norm=1.0
   - Transformer: max_norm=1.0-5.0

5. æƒé‡åˆå§‹åŒ–

   He (Kaiming) - for ReLU:
   nn.init.kaiming_normal_(m.weight, nonlinearity='relu')

   Xavier/Glorot - for Sigmoid/Tanh:
   nn.init.xavier_uniform_(m.weight)

   Orthogonal - for RNN:
   nn.init.orthogonal_(m.weight)

6. å®Œæ•´è®­ç»ƒæµç¨‹ï¼ˆBest Practicesï¼‰

   # 1. æ¨¡å‹ï¼ˆBatchNorm + Dropoutï¼‰
   model = nn.Sequential(
       nn.Linear(input_dim, hidden_dim),
       nn.BatchNorm1d(hidden_dim),
       nn.ReLU(),
       nn.Dropout(0.3),
       # ...
   )

   # 2. æƒé‡åˆå§‹åŒ–
   model.apply(init_weights)

   # 3. ä¼˜åŒ–å™¨ + Weight Decay
   optimizer = optim.AdamW(model.parameters(), lr=0.01, weight_decay=0.01)

   # 4. å­¦ä¹ ç‡è°ƒåº¦
   scheduler = optim.lr_scheduler.OneCycleLR(optimizer, ...)

   # 5. è®­ç»ƒå¾ªç¯
   for epoch in range(n_epochs):
       for batch in dataloader:
           optimizer.zero_grad()
           loss = criterion(model(batch), target)
           loss.backward()

           # æ¢¯åº¦è£å‰ª
           torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)

           optimizer.step()
           scheduler.step()

7. é€‰æ‹©å»ºè®®

   å¿«é€ŸåŸå‹ï¼š
   - Optimizer: Adam (é»˜è®¤å‚æ•°)
   - Scheduler: None æˆ– ReduceLROnPlateau
   - Regularization: Dropout(0.3)

   æœ€ä½³æ€§èƒ½ï¼š
   - Optimizer: AdamW (weight_decay=0.01)
   - Scheduler: OneCycleLR æˆ– CosineAnnealingLR
   - Regularization: BatchNorm + Dropout + Weight Decay

   RNN/LSTMï¼š
   - Optimizer: Adam
   - Gradient Clipping: max_norm=1.0
   - Init: Orthogonal

   Transformerï¼š
   - Optimizer: AdamW
   - Scheduler: OneCycleLR with Warmup
   - Gradient Clipping: max_norm=1.0

8. è°ƒå‚æŠ€å·§

   å­¦ä¹ ç‡ï¼š
   - å¤ªå¤§ â†’ å‘æ•£ã€éœ‡è¡
   - å¤ªå° â†’ æ”¶æ•›æ…¢
   - å¸¸ç”¨èŒƒå›´: 0.0001 ~ 0.01
   - ä½¿ç”¨ Learning Rate Finder

   Dropout Rateï¼š
   - å…¨è¿æ¥å±‚: 0.3-0.5
   - å·ç§¯å±‚: 0.1-0.3
   - RNN: 0.1-0.2

   Weight Decayï¼š
   - è½»åº¦: 0.0001
   - ä¸­åº¦: 0.001-0.01
   - é‡åº¦: 0.1

9. è°ƒè¯•æŠ€å·§

   è¿‡æ‹Ÿåˆï¼ˆTrainå¥½ï¼ŒTestå·®ï¼‰ï¼š
   â†’ å¢åŠ  Dropout
   â†’ å¢åŠ  Weight Decay
   â†’ å‡å°‘æ¨¡å‹å¤æ‚åº¦
   â†’ å¢åŠ æ•°æ®

   æ¬ æ‹Ÿåˆï¼ˆTrain Testéƒ½å·®ï¼‰ï¼š
   â†’ å¢åŠ æ¨¡å‹å¤æ‚åº¦
   â†’ å‡å°‘æ­£åˆ™åŒ–
   â†’ è®­ç»ƒæ›´ä¹…

   æ¢¯åº¦çˆ†ç‚¸ï¼š
   â†’ æ·»åŠ  Gradient Clipping
   â†’ é™ä½å­¦ä¹ ç‡
   â†’ ä½¿ç”¨ BatchNorm

   è®­ç»ƒä¸ç¨³å®šï¼š
   â†’ ä½¿ç”¨ BatchNorm
   â†’ é™ä½å­¦ä¹ ç‡
   â†’ ä½¿ç”¨ OneCycleLR with Warmup

10. PyTorch vs NumPy

    NumPy:
    - æ‰‹å†™ä¼˜åŒ–å™¨ï¼ˆç†è§£åŸç†ï¼‰
    - ä»£ç é‡å¤§
    - é€Ÿåº¦æ…¢

    PyTorch:
    - å†…ç½®ä¼˜åŒ–å™¨ï¼ˆå·¥ä¸šå®è·µï¼‰
    - ä»£ç ç®€æ´
    - GPU åŠ é€Ÿ
    - è‡ªåŠ¨å¾®åˆ†
    """)


if __name__ == "__main__":
    main()

    print("\nğŸ’¡ ç»ƒä¹ å»ºè®®:")
    print("  1. å¯¹æ¯”ä¸åŒä¼˜åŒ–å™¨åœ¨è‡ªå·±æ•°æ®ä¸Šçš„è¡¨ç°")
    print("  2. å®éªŒä¸åŒçš„å­¦ä¹ ç‡è°ƒåº¦ç­–ç•¥")
    print("  3. è°ƒæ•´ Dropout rateï¼Œè§‚å¯Ÿè¿‡æ‹Ÿåˆå˜åŒ–")
    print("  4. ç†è§£ BatchNorm å¦‚ä½•ç¨³å®šè®­ç»ƒ")
    print("  5. å®ç°å®Œæ•´è®­ç»ƒæµç¨‹ï¼ˆæ•´åˆæ‰€æœ‰æŠ€æœ¯ï¼‰")
    print("  6. ä½¿ç”¨ tensorboard å¯è§†åŒ–è®­ç»ƒè¿‡ç¨‹")
