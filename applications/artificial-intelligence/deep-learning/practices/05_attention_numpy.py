"""
Attention æœºåˆ¶ - NumPyæ‰‹å†™å®ç°
ç†è§£ Transformer çš„æ ¸å¿ƒç»„ä»¶

ä½œè€…: Zhang Wenchao
æ—¥æœŸ: 2025-11-20
"""

import numpy as np
import matplotlib.pyplot as plt


class ScaledDotProductAttention:
    """ç¼©æ”¾ç‚¹ç§¯æ³¨æ„åŠ›ï¼ˆScaled Dot-Product Attentionï¼‰

    æ ¸å¿ƒå…¬å¼ï¼šAttention(Q, K, V) = softmax(QK^T / sqrt(d_k)) * V

    å‚æ•°ï¼š
        Q (Query): æŸ¥è¯¢çŸ©é˜µï¼Œè¡¨ç¤º"æˆ‘åœ¨æ‰¾ä»€ä¹ˆ"
        K (Key): é”®çŸ©é˜µï¼Œè¡¨ç¤º"æˆ‘æœ‰ä»€ä¹ˆä¿¡æ¯"
        V (Value): å€¼çŸ©é˜µï¼Œè¡¨ç¤º"ä¿¡æ¯çš„å…·ä½“å†…å®¹"
    """

    def __init__(self):
        self.attention_weights = None  # ä¿å­˜æ³¨æ„åŠ›æƒé‡ç”¨äºå¯è§†åŒ–

    def softmax(self, x):
        """æ•°å€¼ç¨³å®šçš„ softmax"""
        exp_x = np.exp(x - np.max(x, axis=-1, keepdims=True))
        return exp_x / np.sum(exp_x, axis=-1, keepdims=True)

    def forward(self, Q, K, V, mask=None):
        """
        å‰å‘ä¼ æ’­

        å‚æ•°:
            Q: (batch_size, seq_len, d_k) - æŸ¥è¯¢
            K: (batch_size, seq_len, d_k) - é”®
            V: (batch_size, seq_len, d_v) - å€¼
            mask: (batch_size, seq_len, seq_len) - æ©ç ï¼ˆå¯é€‰ï¼‰

        è¿”å›:
            output: (batch_size, seq_len, d_v)
            attention_weights: (batch_size, seq_len, seq_len)
        """
        d_k = Q.shape[-1]

        # 1. è®¡ç®—æ³¨æ„åŠ›åˆ†æ•°ï¼šQ * K^T
        # (batch, seq_len, d_k) @ (batch, d_k, seq_len) -> (batch, seq_len, seq_len)
        scores = np.matmul(Q, K.transpose(0, 2, 1))

        # 2. ç¼©æ”¾ï¼ˆé˜²æ­¢ç‚¹ç§¯è¿‡å¤§å¯¼è‡´ softmax é¥±å’Œï¼‰
        scores = scores / np.sqrt(d_k)

        # 3. åº”ç”¨æ©ç ï¼ˆå¯é€‰ï¼Œç”¨äºé®æŒ¡æœªæ¥ä¿¡æ¯æˆ–paddingï¼‰
        if mask is not None:
            scores = scores + (mask * -1e9)

        # 4. Softmax å½’ä¸€åŒ–å¾—åˆ°æ³¨æ„åŠ›æƒé‡
        attention_weights = self.softmax(scores)
        self.attention_weights = attention_weights  # ä¿å­˜ç”¨äºå¯è§†åŒ–

        # 5. åŠ æƒæ±‚å’Œï¼šAttention * V
        output = np.matmul(attention_weights, V)

        return output, attention_weights


class MultiHeadAttention:
    """å¤šå¤´æ³¨æ„åŠ›ï¼ˆMulti-Head Attentionï¼‰

    æ ¸å¿ƒæ€æƒ³ï¼šå¹¶è¡Œè¿è¡Œå¤šä¸ªæ³¨æ„åŠ›å¤´ï¼Œä»ä¸åŒå­ç©ºé—´æ•è·ä¿¡æ¯

    MultiHead(Q,K,V) = Concat(head_1,...,head_h) * W^O
    å…¶ä¸­ head_i = Attention(Q*W^Q_i, K*W^K_i, V*W^V_i)
    """

    def __init__(self, d_model, num_heads):
        """
        å‚æ•°:
            d_model: æ¨¡å‹ç»´åº¦ï¼ˆå¦‚512ï¼‰
            num_heads: æ³¨æ„åŠ›å¤´æ•°ï¼ˆå¦‚8ï¼‰
        """
        assert d_model % num_heads == 0, "d_model å¿…é¡»èƒ½è¢« num_heads æ•´é™¤"

        self.d_model = d_model
        self.num_heads = num_heads
        self.d_k = d_model // num_heads  # æ¯ä¸ªå¤´çš„ç»´åº¦

        # åˆå§‹åŒ–æƒé‡çŸ©é˜µï¼ˆXavieråˆå§‹åŒ–ï¼‰
        scale = np.sqrt(2.0 / (d_model + self.d_k))
        self.W_Q = np.random.randn(d_model, d_model) * scale
        self.W_K = np.random.randn(d_model, d_model) * scale
        self.W_V = np.random.randn(d_model, d_model) * scale
        self.W_O = np.random.randn(d_model, d_model) * scale

        self.attention = ScaledDotProductAttention()

    def split_heads(self, x):
        """
        å°†æœ€åä¸€ç»´æ‹†åˆ†æˆ (num_heads, d_k)

        è¾“å…¥: (batch_size, seq_len, d_model)
        è¾“å‡º: (batch_size, num_heads, seq_len, d_k)
        """
        batch_size, seq_len, _ = x.shape
        # é‡å¡‘ä¸º (batch, seq_len, num_heads, d_k)
        x = x.reshape(batch_size, seq_len, self.num_heads, self.d_k)
        # è½¬ç½®ä¸º (batch, num_heads, seq_len, d_k)
        return x.transpose(0, 2, 1, 3)

    def combine_heads(self, x):
        """
        åˆå¹¶å¤šä¸ªå¤´çš„è¾“å‡º

        è¾“å…¥: (batch_size, num_heads, seq_len, d_k)
        è¾“å‡º: (batch_size, seq_len, d_model)
        """
        batch_size, _, seq_len, _ = x.shape
        # è½¬ç½®å› (batch, seq_len, num_heads, d_k)
        x = x.transpose(0, 2, 1, 3)
        # åˆå¹¶ä¸º (batch, seq_len, d_model)
        return x.reshape(batch_size, seq_len, self.d_model)

    def forward(self, Q, K, V, mask=None):
        """
        å‰å‘ä¼ æ’­

        å‚æ•°:
            Q, K, V: (batch_size, seq_len, d_model)
            mask: (batch_size, 1, 1, seq_len) æˆ– (batch_size, 1, seq_len, seq_len)

        è¿”å›:
            output: (batch_size, seq_len, d_model)
        """
        batch_size = Q.shape[0]

        # 1. çº¿æ€§æŠ•å½±åˆ° Q, K, V
        Q = np.matmul(Q, self.W_Q)  # (batch, seq_len, d_model)
        K = np.matmul(K, self.W_K)
        V = np.matmul(V, self.W_V)

        # 2. æ‹†åˆ†æˆå¤šä¸ªå¤´
        Q = self.split_heads(Q)  # (batch, num_heads, seq_len, d_k)
        K = self.split_heads(K)
        V = self.split_heads(V)

        # 3. å¯¹æ¯ä¸ªå¤´è®¡ç®—æ³¨æ„åŠ›
        # ä¸ºäº†ä½¿ç”¨ ScaledDotProductAttentionï¼Œéœ€è¦é‡å¡‘ä¸º (batch*num_heads, seq_len, d_k)
        Q_reshaped = Q.reshape(-1, Q.shape[2], Q.shape[3])
        K_reshaped = K.reshape(-1, K.shape[2], K.shape[3])
        V_reshaped = V.reshape(-1, V.shape[2], V.shape[3])

        attention_output, _ = self.attention.forward(Q_reshaped, K_reshaped, V_reshaped, mask)

        # é‡å¡‘å› (batch, num_heads, seq_len, d_k)
        attention_output = attention_output.reshape(batch_size, self.num_heads, -1, self.d_k)

        # 4. åˆå¹¶å¤šä¸ªå¤´
        output = self.combine_heads(attention_output)  # (batch, seq_len, d_model)

        # 5. æœ€ç»ˆçš„çº¿æ€§æŠ•å½±
        output = np.matmul(output, self.W_O)

        return output


class SelfAttentionExample:
    """è‡ªæ³¨æ„åŠ›ç¤ºä¾‹ï¼šç†è§£å¥å­ä¸­è¯ä¸è¯çš„å…³ç³»"""

    def __init__(self):
        pass

    def visualize_attention(self, sentence, attention_weights):
        """å¯è§†åŒ–æ³¨æ„åŠ›æƒé‡çŸ©é˜µ"""
        fig, ax = plt.subplots(figsize=(10, 8))

        im = ax.imshow(attention_weights, cmap='viridis', aspect='auto')

        # è®¾ç½®åæ ‡è½´æ ‡ç­¾
        ax.set_xticks(range(len(sentence)))
        ax.set_yticks(range(len(sentence)))
        ax.set_xticklabels(sentence, rotation=45)
        ax.set_yticklabels(sentence)

        # æ·»åŠ é¢œè‰²æ¡
        plt.colorbar(im, ax=ax)

        # åœ¨æ¯ä¸ªæ ¼å­ä¸Šæ˜¾ç¤ºæ•°å€¼
        for i in range(len(sentence)):
            for j in range(len(sentence)):
                text = ax.text(j, i, f'{attention_weights[i, j]:.2f}',
                             ha="center", va="center", color="w", fontsize=8)

        ax.set_title('Self-Attention Weight Matrix\n(Each row shows attention distribution from query to keys)')
        ax.set_xlabel('Key Position')
        ax.set_ylabel('Query Position')

        plt.tight_layout()
        plt.savefig('attention_weights.png', dpi=150, bbox_inches='tight')
        print("æ³¨æ„åŠ›å¯è§†åŒ–å·²ä¿å­˜åˆ° attention_weights.png")
        plt.close()


def demo_scaled_dot_product_attention():
    """æ¼”ç¤ºï¼šç¼©æ”¾ç‚¹ç§¯æ³¨æ„åŠ›"""
    print("=" * 60)
    print("æ¼”ç¤º 1: ç¼©æ”¾ç‚¹ç§¯æ³¨æ„åŠ› (Scaled Dot-Product Attention)")
    print("=" * 60)

    # åˆ›å»ºç®€å•ç¤ºä¾‹
    batch_size = 1
    seq_len = 4  # å¥å­é•¿åº¦
    d_k = 8      # ç‰¹å¾ç»´åº¦

    # éšæœºåˆå§‹åŒ– Q, K, V
    np.random.seed(42)
    Q = np.random.randn(batch_size, seq_len, d_k)
    K = np.random.randn(batch_size, seq_len, d_k)
    V = np.random.randn(batch_size, seq_len, d_k)

    # è®¡ç®—æ³¨æ„åŠ›
    attention = ScaledDotProductAttention()
    output, attention_weights = attention.forward(Q, K, V)

    print(f"\nè¾“å…¥ç»´åº¦:")
    print(f"  Q: {Q.shape}")
    print(f"  K: {K.shape}")
    print(f"  V: {V.shape}")

    print(f"\næ³¨æ„åŠ›æƒé‡çŸ©é˜µ (æ¯ä¸€è¡Œå’Œä¸º1):")
    print(attention_weights[0])
    print(f"\næ¯è¡Œå’Œ: {attention_weights[0].sum(axis=1)}")

    print(f"\nè¾“å‡ºç»´åº¦: {output.shape}")
    print(f"è¾“å‡ºå€¼ï¼ˆå‰2ä¸ªtokenï¼‰:")
    print(output[0, :2])


def demo_multi_head_attention():
    """æ¼”ç¤ºï¼šå¤šå¤´æ³¨æ„åŠ›"""
    print("\n" + "=" * 60)
    print("æ¼”ç¤º 2: å¤šå¤´æ³¨æ„åŠ› (Multi-Head Attention)")
    print("=" * 60)

    # å‚æ•°è®¾ç½®
    batch_size = 2
    seq_len = 5
    d_model = 64    # Transformer æ ‡å‡†é…ç½®
    num_heads = 8

    # åˆ›å»ºè¾“å…¥
    np.random.seed(42)
    Q = np.random.randn(batch_size, seq_len, d_model)
    K = np.random.randn(batch_size, seq_len, d_model)
    V = np.random.randn(batch_size, seq_len, d_model)

    # å¤šå¤´æ³¨æ„åŠ›
    mha = MultiHeadAttention(d_model, num_heads)
    output = mha.forward(Q, K, V)

    print(f"\né…ç½®:")
    print(f"  æ¨¡å‹ç»´åº¦ d_model: {d_model}")
    print(f"  æ³¨æ„åŠ›å¤´æ•° num_heads: {num_heads}")
    print(f"  æ¯ä¸ªå¤´ç»´åº¦ d_k: {mha.d_k}")

    print(f"\nè¾“å…¥ç»´åº¦: {Q.shape}")
    print(f"è¾“å‡ºç»´åº¦: {output.shape}")
    print(f"\nè¾“å‡ºç»Ÿè®¡:")
    print(f"  å‡å€¼: {output.mean():.4f}")
    print(f"  æ ‡å‡†å·®: {output.std():.4f}")


def demo_self_attention_with_meaning():
    """æ¼”ç¤ºï¼šæœ‰å®é™…æ„ä¹‰çš„è‡ªæ³¨æ„åŠ›ï¼ˆç®€åŒ–çš„è¯åµŒå…¥ï¼‰"""
    print("\n" + "=" * 60)
    print("æ¼”ç¤º 3: è‡ªæ³¨æ„åŠ›å¯è§†åŒ–ï¼ˆå¥å­ç†è§£ï¼‰")
    print("=" * 60)

    # ç®€å•çš„å¥å­å’Œè¯åµŒå…¥
    sentence = ["æˆ‘", "çˆ±", "æ·±åº¦", "å­¦ä¹ "]

    # æ‰‹åŠ¨è®¾è®¡çš„è¯å‘é‡ï¼ˆå®é™…ä¸­åº”è¯¥ç”¨è®­ç»ƒå¥½çš„embeddingï¼‰
    # è¿™é‡Œç®€åŒ–ï¼šæ¯ä¸ªè¯ç”¨ä¸€ä¸ª4ç»´å‘é‡è¡¨ç¤º
    word_embeddings = np.array([
        [1.0, 0.0, 0.0, 0.5],  # æˆ‘ï¼ˆä¸»è¯­ç‰¹å¾ï¼‰
        [0.0, 1.0, 0.5, 0.0],  # çˆ±ï¼ˆåŠ¨è¯ç‰¹å¾ï¼‰
        [0.0, 0.0, 1.0, 0.5],  # æ·±åº¦ï¼ˆå½¢å®¹è¯ï¼‰
        [0.0, 0.5, 1.0, 0.5],  # å­¦ä¹ ï¼ˆåè¯ï¼‰
    ])

    # å¢åŠ  batch ç»´åº¦
    embeddings = word_embeddings[np.newaxis, :, :]  # (1, 4, 4)

    # è‡ªæ³¨æ„åŠ›ï¼ˆQ=K=Vï¼Œå³è¾“å…¥çš„è¯å‘é‡ï¼‰
    attention = ScaledDotProductAttention()
    output, attention_weights = attention.forward(embeddings, embeddings, embeddings)

    print(f"\nå¥å­: {' '.join(sentence)}")
    print(f"\næ³¨æ„åŠ›æƒé‡çŸ©é˜µ:")
    print(attention_weights[0])

    print(f"\nè§£è¯»:")
    for i, word in enumerate(sentence):
        weights = attention_weights[0, i]
        max_idx = weights.argmax()
        print(f"  '{word}' æœ€å…³æ³¨ '{sentence[max_idx]}' (æƒé‡: {weights[max_idx]:.3f})")

    # å¯è§†åŒ–
    visualizer = SelfAttentionExample()
    visualizer.visualize_attention(sentence, attention_weights[0])


def demo_masked_attention():
    """æ¼”ç¤ºï¼šæ©ç æ³¨æ„åŠ›ï¼ˆç”¨äºè§£ç å™¨ï¼Œä¸èƒ½çœ‹åˆ°æœªæ¥ä¿¡æ¯ï¼‰"""
    print("\n" + "=" * 60)
    print("æ¼”ç¤º 4: æ©ç æ³¨æ„åŠ› (Masked Attention)")
    print("=" * 60)

    seq_len = 4
    d_k = 8

    # åˆ›å»ºè¾“å…¥
    np.random.seed(42)
    Q = np.random.randn(1, seq_len, d_k)
    K = np.random.randn(1, seq_len, d_k)
    V = np.random.randn(1, seq_len, d_k)

    # åˆ›å»ºä¸‹ä¸‰è§’æ©ç ï¼ˆé˜²æ­¢çœ‹åˆ°æœªæ¥ï¼‰
    mask = np.triu(np.ones((seq_len, seq_len)), k=1)
    mask = mask[np.newaxis, :, :]  # å¢åŠ  batch ç»´åº¦

    print("\næ©ç çŸ©é˜µï¼ˆ1è¡¨ç¤ºè¢«é®æŒ¡ï¼‰:")
    print(mask[0])

    # æ— æ©ç æ³¨æ„åŠ›
    attention = ScaledDotProductAttention()
    output_no_mask, weights_no_mask = attention.forward(Q, K, V, mask=None)

    print("\næ— æ©ç çš„æ³¨æ„åŠ›æƒé‡:")
    print(weights_no_mask[0])

    # æœ‰æ©ç æ³¨æ„åŠ›
    output_masked, weights_masked = attention.forward(Q, K, V, mask=mask)

    print("\næœ‰æ©ç çš„æ³¨æ„åŠ›æƒé‡ï¼ˆæœªæ¥ä½ç½®æƒé‡ä¸º0ï¼‰:")
    print(weights_masked[0])

    print("\nè§£è¯»:")
    print("  åœ¨è§£ç å™¨ä¸­ï¼Œæ¯ä¸ªä½ç½®åªèƒ½å…³æ³¨å½“å‰å’Œä¹‹å‰çš„ä½ç½®")
    print("  è¿™ç¡®ä¿äº†è‡ªå›å½’ç”Ÿæˆï¼šç¬¬tæ­¥åªä¾èµ–å‰t-1æ­¥çš„ä¿¡æ¯")


def demo_attention_as_database_query():
    """æ¼”ç¤ºï¼šç”¨æ•°æ®åº“æŸ¥è¯¢ç†è§£æ³¨æ„åŠ›æœºåˆ¶"""
    print("\n" + "=" * 60)
    print("æ¼”ç¤º 5: æ•°æ®åº“æŸ¥è¯¢ç±»æ¯”")
    print("=" * 60)

    print("""
æ³¨æ„åŠ›æœºåˆ¶å¯ä»¥ç±»æ¯”ä¸ºæ•°æ®åº“æŸ¥è¯¢ï¼š

1. Query (æŸ¥è¯¢): "æˆ‘æƒ³æ‰¾å…³äº'æ·±åº¦å­¦ä¹ 'çš„ä¿¡æ¯"
2. Key (é”®): æ•°æ®åº“ä¸­æ¯æ¡è®°å½•çš„ç´¢å¼•/æ ‡ç­¾
3. Value (å€¼): æ•°æ®åº“ä¸­æ¯æ¡è®°å½•çš„å®é™…å†…å®¹

Attention(Q,K,V) çš„è¿‡ç¨‹ï¼š
  Step 1: ç”¨ Query å’Œæ‰€æœ‰ Key è®¡ç®—ç›¸ä¼¼åº¦ â†’ å¾—åˆ°æ¯æ¡è®°å½•çš„ç›¸å…³æ€§åˆ†æ•°
  Step 2: Softmax å½’ä¸€åŒ– â†’ è½¬æ¢ä¸ºæ¦‚ç‡åˆ†å¸ƒï¼ˆæ³¨æ„åŠ›æƒé‡ï¼‰
  Step 3: ç”¨æƒé‡å¯¹ Value åŠ æƒæ±‚å’Œ â†’ å¾—åˆ°æœ€ç»ˆç­”æ¡ˆ

ä¾‹å­ï¼š
  Query: "æ·±åº¦å­¦ä¹ çš„åº”ç”¨"

  æ•°æ®åº“:
    Key1: "è®¡ç®—æœºè§†è§‰"     Value1: "CNNç”¨äºå›¾åƒåˆ†ç±»..."      â†’ æƒé‡0.4
    Key2: "è‡ªç„¶è¯­è¨€å¤„ç†"   Value2: "Transformerç”¨äºç¿»è¯‘..."  â†’ æƒé‡0.5
    Key3: "æ¨èç³»ç»Ÿ"       Value3: "ååŒè¿‡æ»¤ç®—æ³•..."          â†’ æƒé‡0.1

  æœ€ç»ˆè¾“å‡º = 0.4 * Value1 + 0.5 * Value2 + 0.1 * Value3
            (æ··åˆäº†å¤šä¸ªç›¸å…³ä¿¡æ¯ï¼Œæƒé‡é«˜çš„è´¡çŒ®æ›´å¤§)

ä¸ºä»€ä¹ˆå« Self-Attentionï¼Ÿ
  å½“ Q=K=V éƒ½æ¥è‡ªåŒä¸€ä¸ªè¾“å…¥åºåˆ—æ—¶ï¼Œå°±æ˜¯"è‡ªæ³¨æ„åŠ›"
  æ¯”å¦‚å¥å­"æˆ‘çˆ±æ·±åº¦å­¦ä¹ "ä¸­ï¼Œæ¯ä¸ªè¯æ—¢æ˜¯ Queryï¼Œä¹Ÿæ˜¯ Key å’Œ Value
  è¿™æ ·æ¯ä¸ªè¯éƒ½èƒ½å…³æ³¨åˆ°å¥å­ä¸­çš„å…¶ä»–è¯ï¼Œç†è§£ä¸Šä¸‹æ–‡å…³ç³»
    """)


def print_summary():
    """æ‰“å°å­¦ä¹ æ€»ç»“"""
    print("\n" + "=" * 60)
    print("å­¦ä¹ æ€»ç»“")
    print("=" * 60)

    print("""
1. æ ¸å¿ƒå…¬å¼
   Attention(Q,K,V) = softmax(QK^T / sqrt(d_k)) * V

   - QK^T: è®¡ç®—ç›¸ä¼¼åº¦
   - /sqrt(d_k): ç¼©æ”¾å› å­ï¼ˆé˜²æ­¢æ¢¯åº¦æ¶ˆå¤±ï¼‰
   - softmax: å½’ä¸€åŒ–ä¸ºæ¦‚ç‡åˆ†å¸ƒ
   - *V: åŠ æƒæ±‚å’Œ

2. å…³é”®ç»„ä»¶
   âœ“ Scaled Dot-Product Attention: åŸºç¡€æ³¨æ„åŠ›å•å…ƒ
   âœ“ Multi-Head Attention: å¹¶è¡Œå¤šä¸ªæ³¨æ„åŠ›å¤´ï¼Œä»ä¸åŒå­ç©ºé—´æ•è·ç‰¹å¾
   âœ“ Self-Attention: Q=K=Vï¼Œåºåˆ—å†…éƒ¨çš„äº¤äº’
   âœ“ Masked Attention: é˜²æ­¢çœ‹åˆ°æœªæ¥ä¿¡æ¯ï¼ˆè§£ç å™¨ç”¨ï¼‰

3. æ³¨æ„åŠ›çš„ä¼˜åŠ¿
   âœ“ å¹¶è¡Œè®¡ç®—ï¼ˆä¸åƒRNNéœ€è¦é¡ºåºå¤„ç†ï¼‰
   âœ“ é•¿è·ç¦»ä¾èµ–ï¼ˆä»»æ„ä¸¤ä¸ªä½ç½®ç›´æ¥è¿æ¥ï¼‰
   âœ“ å¯è§£é‡Šæ€§ï¼ˆå¯è§†åŒ–æ³¨æ„åŠ›æƒé‡ï¼‰

4. åœ¨ Transformer ä¸­çš„ä½œç”¨
   âœ“ Encoder: Self-Attentionï¼ˆç†è§£è¾“å…¥ï¼‰
   âœ“ Decoder: Masked Self-Attentionï¼ˆç”Ÿæˆè¾“å‡ºï¼‰+ Cross-Attentionï¼ˆå…³æ³¨ç¼–ç å™¨è¾“å‡ºï¼‰

5. ç°ä»£åº”ç”¨
   âœ“ NLP: BERTã€GPTã€T5
   âœ“ CV: Vision Transformer (ViT)
   âœ“ å¤šæ¨¡æ€: CLIPã€DALL-E

ä¸‹ä¸€æ­¥ï¼š
  â†’ å­¦ä¹ å®Œæ•´çš„ Transformer æ¶æ„ï¼ˆ06_transformer_numpy.pyï¼‰
  â†’ ç†è§£ Position Encodingï¼ˆä½ç½®ç¼–ç ï¼‰
  â†’ çœ‹ PyTorch ç‰ˆæœ¬ï¼ˆ05_attention_pytorch.pyï¼‰
    """)


if __name__ == "__main__":
    print("\n" + "ğŸ§  " + "=" * 58)
    print("  Attention æœºåˆ¶ - NumPyæ‰‹å†™å®ç°")
    print("  ç†è§£ Transformer çš„æ ¸å¿ƒ")
    print("=" * 60)

    # è¿è¡Œæ‰€æœ‰æ¼”ç¤º
    demo_scaled_dot_product_attention()
    demo_multi_head_attention()
    demo_self_attention_with_meaning()
    demo_masked_attention()
    demo_attention_as_database_query()

    # æ‰“å°æ€»ç»“
    print_summary()

    print("\nâœ… æ‰€æœ‰æ¼”ç¤ºå®Œæˆï¼")
    print("ğŸ“Š æ³¨æ„åŠ›å¯è§†åŒ–å·²ä¿å­˜åˆ° attention_weights.png")
