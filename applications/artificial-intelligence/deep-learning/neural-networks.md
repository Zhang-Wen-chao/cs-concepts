# ç¥ç»ç½‘ç»œåŸºç¡€ (Neural Networks Fundamentals)

> æ¨¡æ‹Ÿäººè„‘ç¥ç»å…ƒçš„è®¡ç®—æ¨¡å‹

## ğŸ¯ ä»€ä¹ˆæ˜¯ç¥ç»ç½‘ç»œï¼Ÿ

**ä¸€å¥è¯ç†è§£ï¼š**
ç¥ç»ç½‘ç»œ = å¤§é‡ç®€å•è®¡ç®—å•å…ƒï¼ˆç¥ç»å…ƒï¼‰è¿æ¥è€Œæˆçš„ç½‘ç»œï¼Œèƒ½å­¦ä¹ å¤æ‚çš„éçº¿æ€§å…³ç³»

**ä¸ºä»€ä¹ˆéœ€è¦ç¥ç»ç½‘ç»œï¼Ÿ**
```
ä¼ ç»Ÿæœºå™¨å­¦ä¹ ï¼ˆå¦‚é€»è¾‘å›å½’ï¼‰ï¼š
â€¢ åªèƒ½å­¦ä¹ çº¿æ€§å…³ç³»
â€¢ éœ€è¦æ‰‹å·¥è®¾è®¡ç‰¹å¾

ç¥ç»ç½‘ç»œï¼š
â€¢ èƒ½å­¦ä¹ å¤æ‚çš„éçº¿æ€§å…³ç³»
â€¢ è‡ªåŠ¨å­¦ä¹ ç‰¹å¾è¡¨ç¤º
â€¢ æ¨èç³»ç»Ÿä¸­çš„æ ¸å¿ƒæŠ€æœ¯ï¼â­ï¸
```

---

## ğŸ“– ç¥ç»å…ƒæ¨¡å‹

### ç”Ÿç‰©ç¥ç»å…ƒ vs äººå·¥ç¥ç»å…ƒ

**ç”Ÿç‰©ç¥ç»å…ƒï¼š**
```
æ ‘çªï¼ˆè¾“å…¥ï¼‰â†’ ç»†èƒä½“ï¼ˆå¤„ç†ï¼‰â†’ è½´çªï¼ˆè¾“å‡ºï¼‰

å¤šä¸ªè¾“å…¥ä¿¡å· â†’ åŠ æƒæ±‚å’Œ â†’ æ¿€æ´» â†’ è¾“å‡º
```

**äººå·¥ç¥ç»å…ƒï¼ˆæ„ŸçŸ¥æœºï¼‰ï¼š**
```
è¾“å…¥ï¼šxâ‚, xâ‚‚, xâ‚ƒ, ...
æƒé‡ï¼šwâ‚, wâ‚‚, wâ‚ƒ, ...
åç½®ï¼šb

è®¡ç®—ï¼š
1. åŠ æƒæ±‚å’Œï¼šz = wâ‚xâ‚ + wâ‚‚xâ‚‚ + wâ‚ƒxâ‚ƒ + b
2. æ¿€æ´»å‡½æ•°ï¼ša = Ïƒ(z)
3. è¾“å‡ºï¼ša
```

**å¯è§†åŒ–ï¼š**
```
    xâ‚ â”€â”€â”€â”€â”€wâ‚â”€â”€â”€â”€â”€â”
                     â”‚
    xâ‚‚ â”€â”€â”€â”€â”€wâ‚‚â”€â”€â”€â”€â”€â”¤
                     â”œâ”€â”€â†’ Î£ â”€â”€â†’ Ïƒ(z) â”€â”€â†’ è¾“å‡º
    xâ‚ƒ â”€â”€â”€â”€â”€wâ‚ƒâ”€â”€â”€â”€â”€â”¤
                     â”‚
    b  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

**ä»£ç å®ç°ï¼š**
```python
import numpy as np

def neuron(x, w, b):
    """
    å•ä¸ªç¥ç»å…ƒçš„è®¡ç®—
    x: è¾“å…¥å‘é‡ [x1, x2, x3]
    w: æƒé‡å‘é‡ [w1, w2, w3]
    b: åç½®
    """
    z = np.dot(w, x) + b  # åŠ æƒæ±‚å’Œ
    a = sigmoid(z)         # æ¿€æ´»
    return a

def sigmoid(z):
    return 1 / (1 + np.exp(-z))
```

---

## ğŸ—ï¸ ç¥ç»ç½‘ç»œç»“æ„

### åŸºæœ¬ç»„æˆ

```
è¾“å…¥å±‚    éšè—å±‚    è¾“å‡ºå±‚
  xâ‚       hâ‚        yâ‚
  xâ‚‚       hâ‚‚
  xâ‚ƒ       hâ‚ƒ
  xâ‚„       hâ‚„

å±‚ä¸å±‚ä¹‹é—´å…¨è¿æ¥ï¼ˆFully Connected / Dense Layerï¼‰
```

### å‰å‘ä¼ æ’­ (Forward Propagation)

**è®¡ç®—æµç¨‹ï¼š**
```
1. è¾“å…¥å±‚ â†’ éšè—å±‚ï¼š
   zâ½Â¹â¾ = Wâ½Â¹â¾x + bâ½Â¹â¾
   aâ½Â¹â¾ = Ïƒ(zâ½Â¹â¾)

2. éšè—å±‚ â†’ è¾“å‡ºå±‚ï¼š
   zâ½Â²â¾ = Wâ½Â²â¾aâ½Â¹â¾ + bâ½Â²â¾
   aâ½Â²â¾ = Ïƒ(zâ½Â²â¾)  â† æœ€ç»ˆè¾“å‡º

ç¬¦å·è¯´æ˜ï¼š
Wâ½Ë¡â¾: ç¬¬lå±‚çš„æƒé‡çŸ©é˜µ
bâ½Ë¡â¾: ç¬¬lå±‚çš„åç½®å‘é‡
aâ½Ë¡â¾: ç¬¬lå±‚çš„æ¿€æ´»å€¼
```

**ä¾‹å­ï¼šé¢„æµ‹æ˜¯å¦ç‚¹å‡»ï¼ˆæ¨èç³»ç»Ÿï¼‰**
```python
# è¾“å…¥ï¼šç”¨æˆ·ç‰¹å¾ + ç‰©å“ç‰¹å¾
x = np.array([
    0.5,   # ç”¨æˆ·å¹´é¾„ï¼ˆå½’ä¸€åŒ–ï¼‰
    1.0,   # ç”¨æˆ·æ€§åˆ«ï¼ˆmale=1ï¼‰
    0.3,   # ç‰©å“ä»·æ ¼ï¼ˆå½’ä¸€åŒ–ï¼‰
    0.8    # ç‰©å“è¯„åˆ†ï¼ˆå½’ä¸€åŒ–ï¼‰
])  # shape: (4,)

# ç¬¬1å±‚ï¼šè¾“å…¥å±‚(4) â†’ éšè—å±‚(3)
W1 = np.random.randn(3, 4)  # shape: (3, 4)
b1 = np.zeros(3)            # shape: (3,)
z1 = W1 @ x + b1            # shape: (3,)
a1 = sigmoid(z1)            # shape: (3,)

# ç¬¬2å±‚ï¼šéšè—å±‚(3) â†’ è¾“å‡ºå±‚(1)
W2 = np.random.randn(1, 3)  # shape: (1, 3)
b2 = np.zeros(1)            # shape: (1,)
z2 = W2 @ a1 + b2           # shape: (1,)
a2 = sigmoid(z2)            # shape: (1,)

# è¾“å‡ºï¼šç‚¹å‡»æ¦‚ç‡
print(f"ç‚¹å‡»æ¦‚ç‡: {a2[0]:.2%}")  # ä¾‹å¦‚ï¼š67%
```

---

## ğŸ¨ æ¿€æ´»å‡½æ•° (Activation Functions)

### ä¸ºä»€ä¹ˆéœ€è¦æ¿€æ´»å‡½æ•°ï¼Ÿ

```
æ²¡æœ‰æ¿€æ´»å‡½æ•°ï¼š
zâ½Â¹â¾ = Wâ½Â¹â¾x + bâ½Â¹â¾
zâ½Â²â¾ = Wâ½Â²â¾zâ½Â¹â¾ + bâ½Â²â¾
     = Wâ½Â²â¾(Wâ½Â¹â¾x + bâ½Â¹â¾) + bâ½Â²â¾
     = (Wâ½Â²â¾Wâ½Â¹â¾)x + (Wâ½Â²â¾bâ½Â¹â¾ + bâ½Â²â¾)
     = W'x + b'  â† è¿˜æ˜¯çº¿æ€§ï¼

å¤šå±‚ç½‘ç»œ = ä¸€å±‚ç½‘ç»œï¼Œæ²¡æ„ä¹‰

æœ‰æ¿€æ´»å‡½æ•°ï¼š
å¼•å…¥éçº¿æ€§ â†’ å¯ä»¥å­¦ä¹ å¤æ‚å…³ç³»
```

### å¸¸è§æ¿€æ´»å‡½æ•°

**1. Sigmoid**
```
Ïƒ(z) = 1 / (1 + eâ»á¶»)

èŒƒå›´ï¼š(0, 1)
å½¢çŠ¶ï¼šSå‹æ›²çº¿

ä¼˜ç‚¹ï¼š
â€¢ è¾“å‡ºåœ¨(0,1)ï¼Œå¯è§£é‡Šä¸ºæ¦‚ç‡
â€¢ å¹³æ»‘å¯å¯¼

ç¼ºç‚¹ï¼š
â€¢ æ¢¯åº¦æ¶ˆå¤±ï¼ˆzå¾ˆå¤§æˆ–å¾ˆå°æ—¶ï¼Œæ¢¯åº¦æ¥è¿‘0ï¼‰
â€¢ è®¡ç®—å¼€é”€å¤§ï¼ˆæŒ‡æ•°è¿ç®—ï¼‰
â€¢ è¾“å‡ºä¸æ˜¯0ä¸­å¿ƒ

ç”¨é€”ï¼š
â€¢ è¾“å‡ºå±‚ï¼ˆäºŒåˆ†ç±»ï¼‰
â€¢ æ¨èç³»ç»Ÿçš„ç‚¹å‡»ç‡é¢„ä¼° â­ï¸
```

**2. Tanhï¼ˆåŒæ›²æ­£åˆ‡ï¼‰**
```
tanh(z) = (eá¶» - eâ»á¶») / (eá¶» + eâ»á¶»)

èŒƒå›´ï¼š(-1, 1)
å½¢çŠ¶ï¼šSå‹æ›²çº¿

ä¼˜ç‚¹ï¼š
â€¢ è¾“å‡º0ä¸­å¿ƒï¼ˆæ¯”sigmoidå¥½ï¼‰
â€¢ æ¢¯åº¦æ¯”sigmoidå¤§

ç¼ºç‚¹ï¼š
â€¢ ä»ç„¶æœ‰æ¢¯åº¦æ¶ˆå¤±é—®é¢˜

ç”¨é€”ï¼š
â€¢ éšè—å±‚ï¼ˆä½†ReLUæ›´å¸¸ç”¨ï¼‰
```

**3. ReLUï¼ˆRectified Linear Unitï¼‰** â­ï¸ æœ€å¸¸ç”¨
```
ReLU(z) = max(0, z)

èŒƒå›´ï¼š[0, âˆ)
å½¢çŠ¶ï¼šæŠ˜çº¿

ä¼˜ç‚¹ï¼š
â€¢ è®¡ç®—ç®€å•ï¼ˆåªéœ€æ¯”è¾ƒï¼‰
â€¢ ç¼“è§£æ¢¯åº¦æ¶ˆå¤±
â€¢ ç¨€ç–æ¿€æ´»ï¼ˆéƒ¨åˆ†ç¥ç»å…ƒä¸º0ï¼‰
â€¢ æ”¶æ•›å¿«

ç¼ºç‚¹ï¼š
â€¢ Dead ReLUï¼ˆç¥ç»å…ƒæ°¸ä¹…æ­»äº¡ï¼‰

ç”¨é€”ï¼š
â€¢ æ·±åº¦å­¦ä¹ ä¸­æœ€å¸¸ç”¨çš„æ¿€æ´»å‡½æ•°
â€¢ æ¨èç³»ç»Ÿä¸­çš„DNN â­ï¸
```

**4. Leaky ReLU**
```
LeakyReLU(z) = max(0.01z, z)

è§£å†³Dead ReLUé—®é¢˜
è´Ÿæ•°éƒ¨åˆ†æœ‰å°æ¢¯åº¦ï¼ˆ0.01ï¼‰
```

**5. PReLU (Parametric ReLU)**
```
PReLU(z) = max(Î±z, z)

Î±æ˜¯å¯å­¦ä¹ çš„å‚æ•°
```

**å¦‚ä½•é€‰æ‹©ï¼Ÿ**
```
è¾“å‡ºå±‚ï¼š
â€¢ äºŒåˆ†ç±»ï¼šsigmoid
â€¢ å¤šåˆ†ç±»ï¼šsoftmax
â€¢ å›å½’ï¼šlinearï¼ˆä¸ç”¨æ¿€æ´»ï¼‰

éšè—å±‚ï¼š
â€¢ é»˜è®¤é€‰æ‹©ï¼šReLU â­ï¸
â€¢ å¦‚æœæœ‰Dead ReLUï¼šLeaky ReLU
â€¢ ç‰¹æ®Šéœ€æ±‚ï¼šTanh
```

---

## ğŸ”„ åå‘ä¼ æ’­ (Backpropagation)

### æ ¸å¿ƒæ€æƒ³

**é—®é¢˜ï¼š** å¦‚ä½•è°ƒæ•´æƒé‡æ¥å‡å°æŸå¤±ï¼Ÿ

**ç­”æ¡ˆï¼š** è®¡ç®—æŸå¤±å¯¹æ¯ä¸ªæƒé‡çš„æ¢¯åº¦ï¼Œç„¶åæ¢¯åº¦ä¸‹é™

```
å‰å‘ä¼ æ’­ï¼šè¾“å…¥ â†’ é¢„æµ‹
åå‘ä¼ æ’­ï¼šæŸå¤± â†’ æ¢¯åº¦ â†’ æ›´æ–°æƒé‡
```

### é“¾å¼æ³•åˆ™

```
å‡è®¾ï¼š
x â†’ zâ‚ = wx + b â†’ aâ‚ = Ïƒ(zâ‚) â†’ Loss

æ±‚ âˆ‚Loss/âˆ‚wï¼š

âˆ‚Loss/âˆ‚w = âˆ‚Loss/âˆ‚aâ‚ Ã— âˆ‚aâ‚/âˆ‚zâ‚ Ã— âˆ‚zâ‚/âˆ‚w
           â””â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”˜   â””â”€â”€â”¬â”€â”€â”˜   â””â”€â”¬â”€â”˜
             è¾“å‡ºå±‚      æ¿€æ´»å‡½æ•°    å½“å‰å±‚
             çš„æ¢¯åº¦        çš„æ¢¯åº¦     çš„æ¢¯åº¦

è¿™å°±æ˜¯é“¾å¼æ³•åˆ™ï¼
```

### åå‘ä¼ æ’­ç®—æ³•

**æ­¥éª¤ï¼š**
```
1. å‰å‘ä¼ æ’­ï¼šè®¡ç®—æ‰€æœ‰å±‚çš„è¾“å‡º
2. è®¡ç®—è¾“å‡ºå±‚çš„è¯¯å·®
3. åå‘ä¼ æ’­è¯¯å·®åˆ°æ¯ä¸€å±‚
4. è®¡ç®—æ¯ä¸€å±‚çš„æ¢¯åº¦
5. æ›´æ–°æƒé‡

ä¼ªä»£ç ï¼š
# å‰å‘ä¼ æ’­
z1 = W1 @ x + b1
a1 = relu(z1)
z2 = W2 @ a1 + b2
a2 = sigmoid(z2)
loss = cross_entropy(a2, y)

# åå‘ä¼ æ’­
dz2 = a2 - y                    # è¾“å‡ºå±‚æ¢¯åº¦
dW2 = dz2 @ a1.T                # W2çš„æ¢¯åº¦
db2 = dz2                       # b2çš„æ¢¯åº¦

da1 = W2.T @ dz2                # ä¼ å›åˆ°éšè—å±‚
dz1 = da1 * relu_derivative(z1) # éšè—å±‚æ¢¯åº¦
dW1 = dz1 @ x.T                 # W1çš„æ¢¯åº¦
db1 = dz1                       # b1çš„æ¢¯åº¦

# æ›´æ–°æƒé‡
W2 -= learning_rate * dW2
b2 -= learning_rate * db2
W1 -= learning_rate * dW1
b1 -= learning_rate * db1
```

**ä¸ºä»€ä¹ˆå«"åå‘"ä¼ æ’­ï¼Ÿ**
```
å‰å‘ï¼šè¾“å…¥ â†’ å±‚1 â†’ å±‚2 â†’ è¾“å‡º
åå‘ï¼šè¾“å‡º â† å±‚2 â† å±‚1 â† è¾“å…¥

æ¢¯åº¦ä»åå¾€å‰è®¡ç®—ï¼
```

---

## ğŸ“ è®­ç»ƒç¥ç»ç½‘ç»œ

### å®Œæ•´æµç¨‹

```python
# 1. åˆå§‹åŒ–
model = NeuralNetwork(input_size=4, hidden_size=64, output_size=1)
optimizer = Adam(learning_rate=0.001)
loss_fn = BinaryCrossEntropy()

# 2. è®­ç»ƒå¾ªç¯
for epoch in range(100):
    for batch in train_data:
        # å‰å‘ä¼ æ’­
        predictions = model.forward(batch.features)
        loss = loss_fn(predictions, batch.labels)

        # åå‘ä¼ æ’­
        gradients = model.backward(loss)

        # æ›´æ–°æƒé‡
        optimizer.update(model.parameters, gradients)

    # éªŒè¯
    val_loss = evaluate(model, val_data)
    print(f"Epoch {epoch}: train_loss={loss:.4f}, val_loss={val_loss:.4f}")
```

### Mini-batch è®­ç»ƒ

**ä¸ºä»€ä¹ˆç”¨batchï¼Ÿ**
```
æ¯ä¸ªæ ·æœ¬æ›´æ–°ï¼ˆSGDï¼‰ï¼š
â€¢ ä¼˜ç‚¹ï¼šæ›´æ–°é¢‘ç¹ï¼Œæ”¶æ•›å¯èƒ½æ›´å¿«
â€¢ ç¼ºç‚¹ï¼šå™ªå£°å¤§ï¼Œä¸ç¨³å®š
â€¢ æ— æ³•åˆ©ç”¨GPUå¹¶è¡Œ

æ‰€æœ‰æ ·æœ¬æ›´æ–°ï¼ˆBatch GDï¼‰ï¼š
â€¢ ä¼˜ç‚¹ï¼šç¨³å®š
â€¢ ç¼ºç‚¹ï¼šæ…¢ï¼Œå†…å­˜å ç”¨å¤§
â€¢ å¯èƒ½é™·å…¥å±€éƒ¨æœ€ä¼˜

Mini-batchï¼ˆæŠ˜ä¸­æ–¹æ¡ˆï¼‰â­ï¸ï¼š
â€¢ æ‰¹æ¬¡å¤§å°ï¼š32ã€64ã€128ã€256
â€¢ å…¼é¡¾é€Ÿåº¦å’Œç¨³å®šæ€§
â€¢ GPUå¹¶è¡ŒåŠ é€Ÿ
```

**Batch size çš„å½±å“ï¼š**
```
Batch size = 32:
â€¢ æ›´æ–°é¢‘ç¹
â€¢ å™ªå£°å¤§ï¼Œå¯èƒ½å¸®åŠ©é€ƒç¦»å±€éƒ¨æœ€ä¼˜
â€¢ æ³›åŒ–å¯èƒ½æ›´å¥½

Batch size = 256:
â€¢ æ›´æ–°ç¨³å®š
â€¢ æ”¶æ•›æ›´å¿«
â€¢ GPUåˆ©ç”¨ç‡é«˜

æ¨èç³»ç»Ÿå¸¸ç”¨ï¼š64-256
```

---

## ğŸ› ï¸ æ­£åˆ™åŒ–æŠ€æœ¯

### 1. Dropout

**æ ¸å¿ƒæ€æƒ³ï¼š** è®­ç»ƒæ—¶éšæœºå…³é—­ä¸€äº›ç¥ç»å…ƒ

```
è®­ç»ƒæ—¶ï¼š
â”Œâ”€â”€â”    â”Œâ”€â”€â”    â”Œâ”€â”€â”
â”‚  â”‚ â”€â”€â”€â”‚  â”‚â”€â”€â”€ â”‚  â”‚
â””â”€â”€â”˜    â””â”€â”€â”˜    â””â”€â”€â”˜
         â†“
â”Œâ”€â”€â”    â”Œâ”€â”€â”    â”Œâ”€â”€â”
â”‚  â”‚ â”€â”€â”€â”‚ â•³â”‚â”€â”€â”€ â”‚  â”‚  â† è¿™ä¸ªç¥ç»å…ƒè¢«å…³é—­
â””â”€â”€â”˜    â””â”€â”€â”˜    â””â”€â”€â”˜

æ¨ç†æ—¶ï¼š
æ‰€æœ‰ç¥ç»å…ƒéƒ½æ¿€æ´»ï¼ˆä½†è¾“å‡ºéœ€è¦ç¼©æ”¾ï¼‰
```

**ä»£ç ç¤ºä¾‹ï¼š**
```python
class Dropout:
    def __init__(self, rate=0.5):
        self.rate = rate  # å…³é—­æ¦‚ç‡

    def forward(self, x, training=True):
        if training:
            # è®­ç»ƒæ—¶ï¼šéšæœºmask
            mask = np.random.rand(*x.shape) > self.rate
            return x * mask / (1 - self.rate)
        else:
            # æ¨ç†æ—¶ï¼šä¸dropout
            return x

# ä½¿ç”¨
dropout = Dropout(rate=0.5)
x = np.array([1, 2, 3, 4])
print(dropout.forward(x, training=True))   # [0, 4, 6, 0]ï¼ˆéšæœºï¼‰
print(dropout.forward(x, training=False))  # [1, 2, 3, 4]
```

**ä¸ºä»€ä¹ˆæœ‰æ•ˆï¼Ÿ**
```
â€¢ é˜²æ­¢ç¥ç»å…ƒä¹‹é—´è¿‡åº¦ä¾èµ–
â€¢ ç›¸å½“äºè®­ç»ƒå¤šä¸ªå­ç½‘ç»œçš„é›†æˆ
â€¢ æ¨èç³»ç»ŸDNNä¸­å¹¿æ³›ä½¿ç”¨ â­ï¸

å¸¸ç”¨ç‡ï¼š
â€¢ éšè—å±‚ï¼š0.3-0.5
â€¢ è¾“å…¥å±‚ï¼š0.1-0.2ï¼ˆå¦‚æœç”¨ï¼‰
```

### 2. Batch Normalization

**æ ¸å¿ƒæ€æƒ³ï¼š** è§„èŒƒåŒ–æ¯ä¸€å±‚çš„è¾“å…¥

```
é—®é¢˜ï¼šæ·±å±‚ç½‘ç»œè®­ç»ƒæ—¶ï¼Œå„å±‚è¾“å…¥åˆ†å¸ƒä¼šå˜åŒ–
è§£å†³ï¼šåœ¨æ¯ä¸€å±‚ä¹‹å‰åšå½’ä¸€åŒ–

BN(x) = Î³ Ã— (x - Î¼) / Ïƒ + Î²

Î¼: batchçš„å‡å€¼
Ïƒ: batchçš„æ ‡å‡†å·®
Î³, Î²: å¯å­¦ä¹ çš„å‚æ•°
```

**ä¼˜ç‚¹ï¼š**
```
â€¢ åŠ é€Ÿè®­ç»ƒ
â€¢ å…è®¸æ›´å¤§çš„å­¦ä¹ ç‡
â€¢ å‡å°‘å¯¹åˆå§‹åŒ–çš„ä¾èµ–
â€¢ æœ‰è½»å¾®æ­£åˆ™åŒ–æ•ˆæœ
```

**åœ¨å“ªé‡Œç”¨ï¼Ÿ**
```
å…¨è¿æ¥å±‚ï¼š
x â†’ Linear â†’ BN â†’ ReLU â†’ ...

å·ç§¯å±‚ï¼š
x â†’ Conv â†’ BN â†’ ReLU â†’ ...
```

### 3. Early Stopping

```
ç›‘æ§éªŒè¯é›†lossï¼š
â€¢ å¦‚æœNä¸ªepochæ²¡æœ‰æ”¹å–„ â†’ åœæ­¢è®­ç»ƒ
â€¢ é˜²æ­¢åœ¨è®­ç»ƒé›†ä¸Šè¿‡åº¦ä¼˜åŒ–

ä¾‹å­ï¼š
Epoch 1: val_loss = 0.5
Epoch 2: val_loss = 0.4
Epoch 3: val_loss = 0.35
Epoch 4: val_loss = 0.36  â† å¼€å§‹ä¸Šå‡
Epoch 5: val_loss = 0.37
Epoch 6: val_loss = 0.38
â†’ åœæ­¢è®­ç»ƒï¼Œä½¿ç”¨Epoch 3çš„æ¨¡å‹
```

---

## ğŸ—ï¸ å¸¸è§ç½‘ç»œæ¶æ„

### å¤šå±‚æ„ŸçŸ¥æœº (MLP / Fully Connected)

```
ç»“æ„ï¼š
Input â†’ Dense(64) â†’ ReLU â†’ Dropout â†’
        Dense(32) â†’ ReLU â†’ Dropout â†’
        Dense(1)  â†’ Sigmoid â†’ Output

æ¨èç³»ç»Ÿä¸­çš„åŸºç¡€æ¨¡å‹ï¼š
ç”¨æˆ·ç‰¹å¾ + ç‰©å“ç‰¹å¾ â†’ MLP â†’ ç‚¹å‡»æ¦‚ç‡
```

**ä»£ç ç¤ºä¾‹ï¼š**
```python
import tensorflow as tf

def build_mlp(input_dim, hidden_dims=[64, 32], dropout_rate=0.3):
    """
    æ„å»ºMLPæ¨¡å‹
    input_dim: è¾“å…¥ç‰¹å¾ç»´åº¦
    hidden_dims: éšè—å±‚ç»´åº¦åˆ—è¡¨
    """
    model = tf.keras.Sequential()

    # è¾“å…¥å±‚
    model.add(tf.keras.layers.Input(shape=(input_dim,)))

    # éšè—å±‚
    for hidden_dim in hidden_dims:
        model.add(tf.keras.layers.Dense(hidden_dim))
        model.add(tf.keras.layers.BatchNormalization())
        model.add(tf.keras.layers.ReLU())
        model.add(tf.keras.layers.Dropout(dropout_rate))

    # è¾“å‡ºå±‚
    model.add(tf.keras.layers.Dense(1, activation='sigmoid'))

    return model

# æ¨èç³»ç»Ÿç¤ºä¾‹
model = build_mlp(input_dim=100, hidden_dims=[128, 64, 32])
model.compile(
    optimizer='adam',
    loss='binary_crossentropy',
    metrics=['auc']
)
```

---

## ğŸ“Š è®­ç»ƒæŠ€å·§

### 1. æƒé‡åˆå§‹åŒ–

**ä¸ºä»€ä¹ˆé‡è¦ï¼Ÿ**
```
åˆå§‹åŒ–ä¸å½“ï¼š
â€¢ å¤ªå°ï¼šæ¢¯åº¦æ¶ˆå¤±
â€¢ å¤ªå¤§ï¼šæ¢¯åº¦çˆ†ç‚¸
â€¢ å…¨é›¶ï¼šæ‰€æœ‰ç¥ç»å…ƒå­¦åˆ°ç›¸åŒçš„ä¸œè¥¿
```

**å¸¸ç”¨æ–¹æ³•ï¼š**
```python
# Xavier åˆå§‹åŒ–ï¼ˆç”¨äºsigmoid/tanhï¼‰
W = np.random.randn(n_in, n_out) * np.sqrt(1 / n_in)

# He åˆå§‹åŒ–ï¼ˆç”¨äºReLUï¼‰â­ï¸ æ¨è
W = np.random.randn(n_in, n_out) * np.sqrt(2 / n_in)

# åç½®åˆå§‹åŒ–
b = np.zeros(n_out)
```

### 2. å­¦ä¹ ç‡è°ƒåº¦

**å­¦ä¹ ç‡è¡°å‡ï¼š**
```
ç­–ç•¥1ï¼šæŒ‡æ•°è¡°å‡
lr = initial_lr Ã— 0.95^epoch

ç­–ç•¥2ï¼šStep Decay
æ¯Nä¸ªepochï¼Œlr = lr Ã— 0.5

ç­–ç•¥3ï¼šCosine Annealing
lr = min_lr + 0.5 Ã— (max_lr - min_lr) Ã— (1 + cos(Ï€t/T))
```

**å­¦ä¹ ç‡é¢„çƒ­ (Warmup)ï¼š**
```
å‰å‡ ä¸ªepoché€æ¸å¢å¤§å­¦ä¹ ç‡
é¿å…è®­ç»ƒåˆæœŸçš„ä¸ç¨³å®š
```

### 3. æ¢¯åº¦è£å‰ª

**é—®é¢˜ï¼š** æ¢¯åº¦çˆ†ç‚¸

**è§£å†³ï¼š**
```python
# æ¢¯åº¦è£å‰ª
max_grad_norm = 1.0
if grad_norm > max_grad_norm:
    grad = grad * (max_grad_norm / grad_norm)
```

---

## ğŸ¯ æ¨èç³»ç»Ÿä¸­çš„åº”ç”¨

### DNNæ¨¡å‹ï¼ˆDeep Neural Networksï¼‰

```
è¾“å…¥ï¼š
â€¢ ç”¨æˆ·ç‰¹å¾ï¼š[user_id, age, gender, ...]
â€¢ ç‰©å“ç‰¹å¾ï¼š[item_id, category, price, ...]
â€¢ äº¤å‰ç‰¹å¾ï¼š[user_age Ã— item_category, ...]

Embeddingå±‚ï¼š
â€¢ user_id â†’ 64ç»´å‘é‡
â€¢ item_id â†’ 64ç»´å‘é‡
â€¢ category â†’ 32ç»´å‘é‡

æ‹¼æ¥ï¼š
â€¢ concat(user_emb, item_emb, ...) â†’ 200ç»´

MLPï¼š
â€¢ Dense(128) â†’ ReLU â†’ Dropout
â€¢ Dense(64) â†’ ReLU â†’ Dropout
â€¢ Dense(32) â†’ ReLU
â€¢ Dense(1) â†’ Sigmoid

è¾“å‡ºï¼š
â€¢ ç‚¹å‡»æ¦‚ç‡ [0, 1]
```

### åŒå¡”æ¨¡å‹ï¼ˆTwo-Towerï¼‰

```
ç”¨æˆ·å¡”ï¼š
user_features â†’ MLP â†’ user_vector(64ç»´)

ç‰©å“å¡”ï¼š
item_features â†’ MLP â†’ item_vector(64ç»´)

ç›¸ä¼¼åº¦ï¼š
score = dot(user_vector, item_vector)
æˆ–
score = cosine_similarity(user_vector, item_vector)

ä¼˜ç‚¹ï¼š
â€¢ ç”¨æˆ·å’Œç‰©å“å¯ä»¥åˆ†åˆ«è®¡ç®—
â€¢ ç‰©å“å‘é‡å¯ä»¥ç¦»çº¿é¢„è®¡ç®—
â€¢ æ”¯æŒå¤§è§„æ¨¡ANNæ£€ç´¢
```

---

## ğŸ”— ä¸å…¶ä»–æ¦‚å¿µçš„è”ç³»

### ä¸æœºå™¨å­¦ä¹ 
```
ç¥ç»ç½‘ç»œæ˜¯æœºå™¨å­¦ä¹ çš„ä¸€ç§æ–¹æ³•
â€¢ é€»è¾‘å›å½’ = å•å±‚ç¥ç»ç½‘ç»œï¼ˆæ— éšè—å±‚ï¼‰
â€¢ MLP = å¤šå±‚é€»è¾‘å›å½’ + éçº¿æ€§æ¿€æ´»
```
å‚è€ƒï¼š[æœºå™¨å­¦ä¹ æ ¸å¿ƒæ¦‚å¿µ](../machine-learning/core-concepts.md)

### ä¸æ·±åº¦å­¦ä¹ 
```
æ·±åº¦å­¦ä¹  = æ·±å±‚ç¥ç»ç½‘ç»œ + å¤§æ•°æ® + GPU
â€¢ CNNï¼šå·ç§¯ç¥ç»ç½‘ç»œ
â€¢ RNNï¼šå¾ªç¯ç¥ç»ç½‘ç»œ
â€¢ Transformerï¼šæ³¨æ„åŠ›æœºåˆ¶
```

### ä¸æ¨èç³»ç»Ÿ
```
ç¥ç»ç½‘ç»œåœ¨æ¨èç³»ç»Ÿä¸­çš„åº”ç”¨ï¼š
â€¢ DNNï¼šåŸºç¡€æ¨¡å‹
â€¢ Wide & Deepï¼šç»“åˆçº¿æ€§å’Œæ·±åº¦
â€¢ DeepFMï¼šè‡ªåŠ¨ç‰¹å¾äº¤å‰
â€¢ DINï¼šæ³¨æ„åŠ›æœºåˆ¶
```
å‚è€ƒï¼š[æ¨èç³»ç»Ÿå­¦ä¹ è·¯å¾„](../../recommendation-systems/deep-learning-recsys-learning-path.md)

---

## ğŸ“ æ ¸å¿ƒè¦ç‚¹æ€»ç»“

```
1. ç¥ç»å…ƒæ¨¡å‹
   â€¢ åŠ æƒæ±‚å’Œ â†’ æ¿€æ´»å‡½æ•° â†’ è¾“å‡º
   â€¢ æ¨¡æ‹Ÿç”Ÿç‰©ç¥ç»å…ƒ

2. ç½‘ç»œç»“æ„
   â€¢ è¾“å…¥å±‚ â†’ éšè—å±‚ â†’ è¾“å‡ºå±‚
   â€¢ å‰å‘ä¼ æ’­ï¼šè®¡ç®—é¢„æµ‹
   â€¢ åå‘ä¼ æ’­ï¼šè®¡ç®—æ¢¯åº¦

3. æ¿€æ´»å‡½æ•°
   â€¢ ReLUï¼šæœ€å¸¸ç”¨ï¼ˆéšè—å±‚ï¼‰â­ï¸
   â€¢ Sigmoidï¼šäºŒåˆ†ç±»è¾“å‡ºå±‚
   â€¢ Softmaxï¼šå¤šåˆ†ç±»è¾“å‡ºå±‚

4. è®­ç»ƒæŠ€å·§
   â€¢ Dropoutï¼šé˜²æ­¢è¿‡æ‹Ÿåˆ
   â€¢ Batch Normalizationï¼šåŠ é€Ÿè®­ç»ƒ
   â€¢ Adamä¼˜åŒ–å™¨ï¼šè‡ªé€‚åº”å­¦ä¹ ç‡

5. æ¨èç³»ç»Ÿåº”ç”¨
   â€¢ DNNï¼šCTRé¢„ä¼°
   â€¢ åŒå¡”ï¼šå¬å›
   â€¢ Embeddingï¼šç±»åˆ«ç‰¹å¾
```

---

**æŒæ¡ç¥ç»ç½‘ç»œåŸºç¡€ï¼Œä½ å°±å¯ä»¥ç†è§£æ·±åº¦å­¦ä¹ æ¨èæ¨¡å‹äº†ï¼** ğŸš€

**ä¸‹ä¸€æ­¥ï¼š** å¼€å§‹å­¦ä¹ [æ¨èç³»ç»Ÿ](../../recommendation-systems/deep-learning-recsys-learning-path.md)
