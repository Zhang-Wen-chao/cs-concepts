"""
DIN (Deep Interest Network) - PyTorchå®ç°
åŸºäº Attention çš„ç”¨æˆ·å…´è¶£å»ºæ¨¡ï¼ˆé˜¿é‡Œå·´å·´ 2018ï¼‰

ä½œè€…: Zhang Wenchao
æ—¥æœŸ: 2025-11-22

====================================================================
ğŸ“– ä» DeepFM åˆ° DIN
====================================================================

DeepFM çš„é—®é¢˜ï¼š
- ç”¨æˆ·å†å²è¡Œä¸º â†’ å›ºå®šçš„ Embedding
- æ‰€æœ‰å€™é€‰ç‰©å“éƒ½ç”¨åŒä¸€ä¸ªç”¨æˆ·è¡¨ç¤º
- æ— æ³•ä½“ç°ç”¨æˆ·å…´è¶£çš„å¤šæ ·æ€§

ä¾‹å­ï¼š
ç”¨æˆ·å†å²ï¼š[æ‰‹æœº, ç”µè„‘, å°è¯´, è€³æœº, å›¾ä¹¦]
å€™é€‰1ï¼šç¬”è®°æœ¬ç”µè„‘ â†’ åº”è¯¥å…³æ³¨ [æ‰‹æœº, ç”µè„‘, è€³æœº]
å€™é€‰2ï¼šæ¨ç†å°è¯´   â†’ åº”è¯¥å…³æ³¨ [å°è¯´, å›¾ä¹¦]

ä½† DeepFM å¯¹ä¸¤ä¸ªå€™é€‰éƒ½ç”¨åŒä¸€ä¸ªç”¨æˆ·å‘é‡ï¼

====================================================================
ğŸ¯ DIN çš„æ ¸å¿ƒæ€æƒ³
====================================================================

Attention æœºåˆ¶ï¼šæ ¹æ®å€™é€‰ç‰©å“ï¼ŒåŠ¨æ€è®¡ç®—ç”¨æˆ·å†å²çš„æƒé‡

ç”¨æˆ·è¡¨ç¤º = Î£ attention_weight_i Ã— history_item_i
            â†‘ æ ¹æ®å€™é€‰ç‰©å“åŠ¨æ€è®¡ç®—

å…¬å¼ï¼š
attention_weight_i = softmax(f(candidate_item, history_item_i))

å…¶ä¸­ f æ˜¯ä¸€ä¸ªå°ç½‘ç»œï¼ˆMLPï¼‰ï¼Œå­¦ä¹ å€™é€‰å’Œå†å²çš„ç›¸å…³æ€§ã€‚

====================================================================
ğŸ—ï¸ DIN æ¶æ„
====================================================================

è¾“å…¥ï¼š
- ç”¨æˆ·ç‰¹å¾ï¼š[user_id, age, gender]
- å€™é€‰ç‰©å“ï¼š[item_id, category]
- ç”¨æˆ·å†å²ï¼š[å†å²item_1, å†å²item_2, ..., å†å²item_n]

æµç¨‹ï¼š

1ï¸âƒ£ Embedding
   ç”¨æˆ·ç‰¹å¾ â†’ user_emb
   å€™é€‰ç‰©å“ â†’ candidate_emb
   å†å²ç‰©å“ â†’ [hist_emb_1, hist_emb_2, ..., hist_emb_n]

2ï¸âƒ£ Attention Layerï¼ˆæ ¸å¿ƒï¼ï¼‰
   for each hist_emb_i:
       # è®¡ç®—å€™é€‰ç‰©å“å’Œå†å²ç‰©å“çš„ç›¸å…³æ€§
       score_i = MLP([candidate_emb, hist_emb_i, candidate_emb - hist_emb_i, candidate_emb * hist_emb_i])

   # Softmax å½’ä¸€åŒ–
   attention_weights = softmax([score_1, score_2, ..., score_n])

   # åŠ æƒæ±‚å’Œ
   user_interest = Î£ attention_weight_i Ã— hist_emb_i

3ï¸âƒ£ Concatenate
   final_input = [user_emb, candidate_emb, user_interest]

4ï¸âƒ£ MLP
   logit = MLP(final_input)

5ï¸âƒ£ Sigmoid
   click_prob = sigmoid(logit)

====================================================================
ğŸ”‘ Attention è®¡ç®—ç»†èŠ‚
====================================================================

è¾“å…¥ï¼š
- candidate_emb: (batch, emb_dim) - å€™é€‰ç‰©å“
- hist_embs: (batch, seq_len, emb_dim) - å†å²ç‰©å“åºåˆ—

è®¡ç®—ç›¸å…³æ€§ç‰¹å¾ï¼ˆ4ç§ï¼‰ï¼š
1. candidate_emb (å¤åˆ¶seq_lenæ¬¡)
2. hist_emb_i
3. candidate_emb - hist_emb_i  (å·®å€¼ï¼Œè¡¡é‡è·ç¦»)
4. candidate_emb * hist_emb_i  (é€å…ƒç´ ä¹˜ï¼Œè¡¡é‡ç›¸ä¼¼åº¦)

æ‹¼æ¥: [candidate, history, sub, mul] â†’ (batch, seq_len, 4*emb_dim)

é€šè¿‡ Attention MLP:
score_i = MLP(concat_features_i)  â†’ (batch, seq_len, 1)

Softmax:
attention_weights = softmax(scores)  â†’ (batch, seq_len, 1)

åŠ æƒæ±‚å’Œ:
user_interest = Î£ attention_weight_i Ã— hist_emb_i  â†’ (batch, emb_dim)

====================================================================
ğŸ’¡ DIN vs ä¹‹å‰çš„æ¨¡å‹
====================================================================

| æ¨¡å‹ | ç”¨æˆ·è¡¨ç¤º | ä¼˜ç¼ºç‚¹ |
|------|---------|--------|
| åŒå¡” | å›ºå®šå‘é‡ | âŒ æ— æ³•ä½“ç°å…´è¶£å¤šæ ·æ€§ |
| Wide & Deep | å›ºå®šå‘é‡ | âŒ å†å²è¡Œä¸ºå¹³å‡æ± åŒ– |
| DeepFM | å›ºå®šå‘é‡ | âŒ æ‰€æœ‰å€™é€‰ç”¨åŒä¸€è¡¨ç¤º |
| **DIN** | **åŠ¨æ€å‘é‡** | âœ… æ ¹æ®å€™é€‰åŠ¨æ€å…³æ³¨å†å²<br>âœ… ä½“ç°å…´è¶£å¤šæ ·æ€§ |

====================================================================
ğŸ§® Attention çš„å¥½å¤„
====================================================================

1. å¯è§£é‡Šæ€§ï¼š
   - å¯ä»¥çœ‹åˆ°æ¨¡å‹å…³æ³¨äº†å“ªäº›å†å²è¡Œä¸º
   - ä¾‹å¦‚ï¼šæ¨è"ç¬”è®°æœ¬"æ—¶ï¼Œå…³æ³¨äº†"æ‰‹æœº(0.5)"ã€"ç”µè„‘(0.4)"ã€"è€³æœº(0.1)"

2. æ•ˆæœæå‡ï¼š
   - é˜¿é‡Œå·´å·´è®ºæ–‡ï¼šCTR æå‡ ~10%
   - ç‰¹åˆ«é€‚åˆç”¨æˆ·å…´è¶£å¤šæ ·çš„åœºæ™¯

3. çµæ´»æ€§ï¼š
   - å†å²åºåˆ—é•¿åº¦å¯å˜
   - è‡ªåŠ¨å­¦ä¹ å“ªäº›å†å²é‡è¦

====================================================================
"""

import torch
import torch.nn as nn
import torch.nn.functional as F
import numpy as np
import matplotlib.pyplot as plt
from torch.utils.data import Dataset, DataLoader
from sklearn.metrics import roc_auc_score

# è®¾ç½®éšæœºç§å­
torch.manual_seed(42)
np.random.seed(42)

DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')


# ============ 1. æ•°æ®å‡†å¤‡ï¼ˆåŠ å…¥ç”¨æˆ·å†å²åºåˆ—ï¼‰============

class DINDataset(Dataset):
    """å¸¦ç”¨æˆ·å†å²åºåˆ—çš„ CTR æ•°æ®é›†

    æ–°å¢ï¼šç”¨æˆ·å†å²ç‰©å“åºåˆ—
    """

    def __init__(self, num_samples=10000, num_users=1000, num_items=500, max_hist_len=10):
        self.num_samples = num_samples
        self.num_users = num_users
        self.num_items = num_items
        self.max_hist_len = max_hist_len

        self.num_categories = 20

        # ç‰©å“å±æ€§
        self.item_categories = np.random.randint(0, self.num_categories, num_items)

        # ç”Ÿæˆç”¨æˆ·å†å²ï¼ˆæ¯ä¸ªç”¨æˆ·æœ‰ä¸€ä¸ªå†å²ç‰©å“åˆ—è¡¨ï¼‰
        self.user_histories = {}
        for user_id in range(num_users):
            # æ¯ä¸ªç”¨æˆ·çš„å†å²é•¿åº¦éšæœºï¼ˆ1åˆ°max_hist_lenï¼‰
            hist_len = np.random.randint(1, max_hist_len + 1)
            history = np.random.randint(0, num_items, hist_len)
            self.user_histories[user_id] = history

        # ç”Ÿæˆè®­ç»ƒæ ·æœ¬
        self.samples = []
        for _ in range(num_samples):
            user_id = np.random.randint(0, num_users)
            candidate_item = np.random.randint(0, num_items)

            # è·å–ç”¨æˆ·å†å²
            history = self.user_histories[user_id]

            # æ¨¡æ‹Ÿç‚¹å‡»è§„å¾‹ï¼šæ›´å¤æ‚çš„è§„åˆ™ï¼Œè®©æ¨¡å‹æœ‰ä¸œè¥¿å¯å­¦
            candidate_cat = self.item_categories[candidate_item]
            history_cats = self.item_categories[history]

            # è§„åˆ™1ï¼šç±»åˆ«å®Œå…¨åŒ¹é…çš„ç‰©å“æ•°é‡
            exact_match = np.sum(history_cats == candidate_cat)

            # è§„åˆ™2ï¼šç±»åˆ«æ¥è¿‘çš„ç‰©å“æ•°é‡ï¼ˆç›¸å·®1æˆ–2çš„ç±»åˆ«ä¹Ÿç®—ç›¸å…³ï¼‰
            close_match = np.sum((np.abs(history_cats - candidate_cat) <= 2))

            # è§„åˆ™3ï¼šæœ€è¿‘çš„å†å²ç‰©å“æƒé‡æ›´é«˜
            if len(history) >= 3:
                recent_match = 1.0 if candidate_cat in history_cats[-3:] else 0.0
            else:
                recent_match = 1.0 if candidate_cat in history_cats else 0.0

            # è§„åˆ™4ï¼šå€™é€‰ç‰©å“IDä¹Ÿå½±å“ç‚¹å‡»ï¼ˆæŸäº›ç‰©å“æœ¬èº«å°±çƒ­é—¨ï¼‰
            item_popularity = 1.0 if candidate_item % 10 == 0 else 0.0

            # ç»¼åˆè®¡ç®—ç‚¹å‡»æ¦‚ç‡
            click_prob = 0.05  # åŸºç¡€æ¦‚ç‡
            click_prob += (exact_match / len(history)) * 0.4  # å®Œå…¨åŒ¹é…è´¡çŒ®40%
            click_prob += (close_match / len(history)) * 0.2   # æ¥è¿‘åŒ¹é…è´¡çŒ®20%
            click_prob += recent_match * 0.2                   # æœ€è¿‘å†å²è´¡çŒ®20%
            click_prob += item_popularity * 0.15               # ç‰©å“çƒ­åº¦è´¡çŒ®15%

            label = 1 if np.random.rand() < click_prob else 0

            self.samples.append({
                'user_id': user_id,
                'candidate_item': candidate_item,
                'history': history,
                'label': label
            })

    def __len__(self):
        return self.num_samples

    def __getitem__(self, idx):
        sample = self.samples[idx]
        history = sample['history']

        # Paddingï¼šå†å²åºåˆ—ä¸è¶³max_hist_lenæ—¶ç”¨0å¡«å……
        hist_len = len(history)
        padded_history = np.zeros(self.max_hist_len, dtype=np.int64)
        padded_history[:hist_len] = history

        # Maskï¼šæ ‡è®°å“ªäº›æ˜¯çœŸå®å†å²ï¼ˆ1ï¼‰ï¼Œå“ªäº›æ˜¯paddingï¼ˆ0ï¼‰
        hist_mask = np.zeros(self.max_hist_len, dtype=np.float32)
        hist_mask[:hist_len] = 1.0

        return {
            'user_id': torch.LongTensor([sample['user_id']]),
            'candidate_item': torch.LongTensor([sample['candidate_item']]),
            'history': torch.LongTensor(padded_history),  # (max_hist_len,)
            'hist_mask': torch.FloatTensor(hist_mask),    # (max_hist_len,)
            'label': torch.FloatTensor([sample['label']])
        }


# ============ 2. Attention Layer ============

class AttentionLayer(nn.Module):
    """DIN çš„ Attention å±‚

    æ ¹æ®å€™é€‰ç‰©å“ï¼ŒåŠ¨æ€è®¡ç®—ç”¨æˆ·å†å²çš„æƒé‡
    """

    def __init__(self, embedding_dim, hidden_dim=64):
        super().__init__()

        # Attention MLP
        # è¾“å…¥ï¼š[candidate, history, candidate-history, candidate*history]
        # ç»´åº¦ï¼š4 * embedding_dim
        self.attention_mlp = nn.Sequential(
            nn.Linear(4 * embedding_dim, hidden_dim),
            nn.ReLU(),
            nn.Linear(hidden_dim, 1)  # è¾“å‡ºæ¯ä¸ªå†å²ç‰©å“çš„åˆ†æ•°
        )

    def forward(self, candidate_emb, hist_embs, hist_mask):
        """
        å‚æ•°:
            candidate_emb: (batch, emb_dim) - å€™é€‰ç‰©å“ Embedding
            hist_embs: (batch, seq_len, emb_dim) - å†å²ç‰©å“ Embedding åºåˆ—
            hist_mask: (batch, seq_len) - å†å²åºåˆ—çš„ maskï¼ˆ0=padding, 1=çœŸå®ï¼‰

        è¿”å›:
            user_interest: (batch, emb_dim) - ç”¨æˆ·å…´è¶£å‘é‡
            attention_weights: (batch, seq_len) - æ³¨æ„åŠ›æƒé‡ï¼ˆç”¨äºå¯è§†åŒ–ï¼‰
        """
        batch_size, seq_len, emb_dim = hist_embs.size()

        # 1. å°† candidate_emb æ‰©å±•åˆ°å’Œ hist_embs ç›¸åŒçš„ç»´åº¦
        candidate_emb_expand = candidate_emb.unsqueeze(1).expand(-1, seq_len, -1)  # (batch, seq_len, emb_dim)

        # 2. æ„é€  4 ç§ç›¸å…³æ€§ç‰¹å¾
        sub = candidate_emb_expand - hist_embs  # å·®å€¼
        mul = candidate_emb_expand * hist_embs  # é€å…ƒç´ ä¹˜

        # 3. æ‹¼æ¥ç‰¹å¾
        concat_features = torch.cat([
            candidate_emb_expand,  # å€™é€‰
            hist_embs,             # å†å²
            sub,                   # å·®å€¼
            mul                    # ä¹˜ç§¯
        ], dim=2)  # (batch, seq_len, 4*emb_dim)

        # 4. é€šè¿‡ Attention MLP è®¡ç®—åˆ†æ•°
        scores = self.attention_mlp(concat_features).squeeze(2)  # (batch, seq_len)

        # 5. Maskï¼šå°† padding ä½ç½®çš„åˆ†æ•°è®¾ä¸ºå¾ˆå°çš„å€¼ï¼ˆ-infï¼‰ï¼Œsoftmax åæƒé‡ä¸º0
        scores = scores.masked_fill(hist_mask == 0, -1e9)

        # 6. Softmax å½’ä¸€åŒ–
        attention_weights = F.softmax(scores, dim=1)  # (batch, seq_len)

        # 7. åŠ æƒæ±‚å’Œå¾—åˆ°ç”¨æˆ·å…´è¶£å‘é‡
        user_interest = torch.sum(
            attention_weights.unsqueeze(2) * hist_embs,  # (batch, seq_len, 1) * (batch, seq_len, emb_dim)
            dim=1  # (batch, emb_dim)
        )

        return user_interest, attention_weights


# ============ 3. DIN æ¨¡å‹ ============

class DINModel(nn.Module):
    """DIN (Deep Interest Network) å®Œæ•´æ¨¡å‹"""

    def __init__(self, num_users, num_items, embedding_dim=32, hidden_dims=[256, 128, 64]):
        super().__init__()

        # ============ Embedding å±‚ ============
        self.user_embedding = nn.Embedding(num_users, embedding_dim)
        self.item_embedding = nn.Embedding(num_items, embedding_dim)

        # ============ Attention å±‚ ============
        self.attention = AttentionLayer(embedding_dim, hidden_dim=64)

        # ============ MLP ============
        # è¾“å…¥ï¼šuser_emb + candidate_emb + user_interest
        mlp_input_dim = embedding_dim * 3

        layers = []
        input_dim = mlp_input_dim
        for hidden_dim in hidden_dims:
            layers.append(nn.Linear(input_dim, hidden_dim))
            layers.append(nn.ReLU())
            layers.append(nn.Dropout(0.2))
            input_dim = hidden_dim

        self.mlp = nn.Sequential(*layers)
        self.output = nn.Linear(hidden_dims[-1], 1)

    def forward(self, user_id, candidate_item, history, hist_mask):
        """
        å‚æ•°:
            user_id: (batch, 1) - ç”¨æˆ·ID
            candidate_item: (batch, 1) - å€™é€‰ç‰©å“ID
            history: (batch, seq_len) - å†å²ç‰©å“IDåºåˆ—
            hist_mask: (batch, seq_len) - å†å²åºåˆ—mask

        è¿”å›:
            logit: (batch,) - é¢„æµ‹çš„ logit
            attention_weights: (batch, seq_len) - æ³¨æ„åŠ›æƒé‡
        """
        # 1. Embedding
        user_emb = self.user_embedding(user_id).squeeze(1)  # (batch, emb_dim)
        candidate_emb = self.item_embedding(candidate_item).squeeze(1)  # (batch, emb_dim)
        hist_embs = self.item_embedding(history)  # (batch, seq_len, emb_dim)

        # 2. Attentionï¼šæ ¹æ®å€™é€‰ç‰©å“åŠ¨æ€è®¡ç®—ç”¨æˆ·å…´è¶£
        user_interest, attention_weights = self.attention(candidate_emb, hist_embs, hist_mask)

        # 3. æ‹¼æ¥æ‰€æœ‰ç‰¹å¾
        final_input = torch.cat([user_emb, candidate_emb, user_interest], dim=1)

        # 4. MLP
        hidden = self.mlp(final_input)
        logit = self.output(hidden).squeeze(1)

        return logit, attention_weights


# ============ 4. è®­ç»ƒ ============

def train_model(model, train_loader, val_loader, device, num_epochs=30, lr=0.001):
    """è®­ç»ƒ DIN æ¨¡å‹"""
    criterion = nn.BCEWithLogitsLoss()
    optimizer = torch.optim.Adam(model.parameters(), lr=lr)

    history = {'train_loss': [], 'train_auc': [], 'val_loss': [], 'val_auc': []}

    print("\nå¼€å§‹è®­ç»ƒ...")
    for epoch in range(num_epochs):
        model.train()
        train_loss = 0
        train_preds = []
        train_labels = []

        for batch in train_loader:
            user_id = batch['user_id'].to(device)
            candidate_item = batch['candidate_item'].to(device)
            hist_items = batch['history'].to(device)
            hist_mask = batch['hist_mask'].to(device)
            label = batch['label'].to(device).squeeze()

            optimizer.zero_grad()

            logit, _ = model(user_id, candidate_item, hist_items, hist_mask)
            loss = criterion(logit, label)

            loss.backward()
            optimizer.step()

            train_loss += loss.item()
            train_preds.extend(torch.sigmoid(logit).detach().cpu().numpy())
            train_labels.extend(label.cpu().numpy())

        train_loss /= len(train_loader)
        train_auc = roc_auc_score(train_labels, train_preds)

        model.eval()
        val_loss = 0
        val_preds = []
        val_labels = []

        with torch.no_grad():
            for batch in val_loader:
                user_id = batch['user_id'].to(device)
                candidate_item = batch['candidate_item'].to(device)
                hist_items = batch['history'].to(device)
                hist_mask = batch['hist_mask'].to(device)
                label = batch['label'].to(device).squeeze()

                logit, _ = model(user_id, candidate_item, hist_items, hist_mask)
                loss = criterion(logit, label)

                val_loss += loss.item()
                val_preds.extend(torch.sigmoid(logit).cpu().numpy())
                val_labels.extend(label.cpu().numpy())

        val_loss /= len(val_loader)
        val_auc = roc_auc_score(val_labels, val_preds)

        history['train_loss'].append(train_loss)
        history['train_auc'].append(train_auc)
        history['val_loss'].append(val_loss)
        history['val_auc'].append(val_auc)

        if (epoch + 1) % 5 == 0:
            print(f'Epoch {epoch+1}/{num_epochs}:')
            print(f'  Train Loss: {train_loss:.4f}, Train AUC: {train_auc:.4f}')
            print(f'  Val Loss: {val_loss:.4f}, Val AUC: {val_auc:.4f}')

    return history


# ============ 5. å¯è§†åŒ– Attention ============

def visualize_attention(model, dataset, device, sample_idx=0):
    """å¯è§†åŒ– Attention æƒé‡"""
    model.eval()

    sample = dataset[sample_idx]
    user_id = sample['user_id'].unsqueeze(0).to(device)
    candidate_item = sample['candidate_item'].unsqueeze(0).to(device)
    hist_items = sample['history'].unsqueeze(0).to(device)
    hist_mask = sample['hist_mask'].unsqueeze(0).to(device)

    with torch.no_grad():
        logit, attention_weights = model(user_id, candidate_item, hist_items, hist_mask)

    # è·å–çœŸå®å†å²é•¿åº¦
    hist_len = int(hist_mask.sum().item())
    history_items = hist_items.squeeze().cpu().numpy()[:hist_len]
    attention_weights = attention_weights.squeeze().cpu().numpy()[:hist_len]

    # ç»˜åˆ¶
    fig, ax = plt.subplots(figsize=(10, 4))
    bars = ax.bar(range(hist_len), attention_weights)
    ax.set_xlabel('History Item Index')
    ax.set_ylabel('Attention Weight')
    ax.set_title(f'Attention Weights (Candidate Item: {candidate_item.item()})')
    ax.set_xticks(range(hist_len))
    ax.set_xticklabels([f'Item {item}' for item in history_items], rotation=45)

    # æ ‡æ³¨æƒé‡å€¼
    for i, (bar, weight) in enumerate(zip(bars, attention_weights)):
        ax.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.01,
                f'{weight:.3f}', ha='center', va='bottom', fontsize=9)

    plt.tight_layout()
    plt.savefig('din_attention_visualization.png', dpi=150)
    print("\nAttention å¯è§†åŒ–å·²ä¿å­˜åˆ° din_attention_visualization.png")
    plt.close()


def plot_training_history(history):
    """ç»˜åˆ¶è®­ç»ƒå†å²"""
    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 4))

    ax1.plot(history['train_loss'], label='Train Loss')
    ax1.plot(history['val_loss'], label='Val Loss')
    ax1.set_xlabel('Epoch')
    ax1.set_ylabel('Loss')
    ax1.set_title('Training and Validation Loss')
    ax1.legend()
    ax1.grid(True)

    ax2.plot(history['train_auc'], label='Train AUC')
    ax2.plot(history['val_auc'], label='Val AUC')
    ax2.set_xlabel('Epoch')
    ax2.set_ylabel('AUC')
    ax2.set_title('Training and Validation AUC')
    ax2.legend()
    ax2.grid(True)

    plt.tight_layout()
    plt.savefig('din_training.png', dpi=150)
    print("\nè®­ç»ƒå†å²å·²ä¿å­˜åˆ° din_training.png")
    plt.close()


# ============ ä¸»å‡½æ•° ============

def main():
    print("\n" + "ğŸš€ " + "=" * 58)
    print("  DIN (Deep Interest Network) - PyTorchå®ç°")
    print("  åŸºäº Attention çš„ç”¨æˆ·å…´è¶£å»ºæ¨¡")
    print("=" * 60)

    print(f"\nä½¿ç”¨è®¾å¤‡: {DEVICE}")
    if torch.cuda.is_available():
        print(f"GPUå‹å·: {torch.cuda.get_device_name(0)}")

    print("\n" + "=" * 60)
    print("åˆ›å»ºæ•°æ®é›†ï¼ˆå¸¦ç”¨æˆ·å†å²åºåˆ—ï¼‰")
    print("=" * 60)

    train_dataset = DINDataset(num_samples=50000, num_users=1000, num_items=500, max_hist_len=10)  # å¢åŠ æ•°æ®
    val_dataset = DINDataset(num_samples=10000, num_users=1000, num_items=500, max_hist_len=10)

    train_loader = DataLoader(train_dataset, batch_size=256, shuffle=True)
    val_loader = DataLoader(val_dataset, batch_size=256, shuffle=False)

    print(f"è®­ç»ƒé›†å¤§å°: {len(train_dataset)}")
    print(f"éªŒè¯é›†å¤§å°: {len(val_dataset)}")
    print(f"æœ€å¤§å†å²é•¿åº¦: {train_dataset.max_hist_len}")

    train_labels = [s['label'] for s in train_dataset.samples]
    pos_ratio = sum(train_labels) / len(train_labels)
    print(f"æ­£æ ·æœ¬æ¯”ä¾‹: {pos_ratio:.2%}")

    print("\n" + "=" * 60)
    print("åˆ›å»º DIN æ¨¡å‹")
    print("=" * 60)

    model = DINModel(
        num_users=train_dataset.num_users,
        num_items=train_dataset.num_items,
        embedding_dim=16,  # é™ä½ 32 â†’ 16
        hidden_dims=[128, 64]  # å‡å°‘å±‚æ•°å’Œç»´åº¦
    ).to(DEVICE)

    print(f"æ¨¡å‹å‚æ•°é‡: {sum(p.numel() for p in model.parameters()):,}")

    print("\n" + "=" * 60)
    print("è®­ç»ƒæ¨¡å‹")
    print("=" * 60)

    history = train_model(model, train_loader, val_loader, DEVICE, num_epochs=15, lr=0.001)  # å‡å°‘åˆ°15è½®

    plot_training_history(history)

    print("\n" + "=" * 60)
    print("æµ‹è¯•é¢„æµ‹ + Attention å¯è§†åŒ–")
    print("=" * 60)

    model.eval()
    with torch.no_grad():
        for i in range(3):
            sample = val_dataset[i]

            user_id = sample['user_id'].unsqueeze(0).to(DEVICE)
            candidate_item = sample['candidate_item'].unsqueeze(0).to(DEVICE)
            hist_items = sample['history'].unsqueeze(0).to(DEVICE)
            hist_mask = sample['hist_mask'].unsqueeze(0).to(DEVICE)

            logit, attention_weights = model(user_id, candidate_item, hist_items, hist_mask)
            pred_prob = torch.sigmoid(logit).item()
            true_label = sample['label'].item()

            hist_len = int(hist_mask.sum().item())
            history_items = hist_items.squeeze().cpu().numpy()[:hist_len]
            attn_weights = attention_weights.squeeze().cpu().numpy()[:hist_len]

            print(f"\næ ·æœ¬ {i+1}:")
            print(f"  ç”¨æˆ·ID: {user_id.item()}, å€™é€‰ç‰©å“: {candidate_item.item()}")
            print(f"  å†å²ç‰©å“: {history_items}")
            print(f"  Attentionæƒé‡: {[f'{w:.3f}' for w in attn_weights]}")
            print(f"  é¢„æµ‹æ¦‚ç‡: {pred_prob:.3f}")
            print(f"  çœŸå®æ ‡ç­¾: {int(true_label)}")
            print(f"  é¢„æµ‹ç»“æœ: {'ç‚¹å‡» âœ“' if pred_prob > 0.5 else 'ä¸ç‚¹å‡» âœ—'}")

    # å¯è§†åŒ–ä¸€ä¸ªæ ·æœ¬çš„ Attention
    visualize_attention(model, val_dataset, DEVICE, sample_idx=0)

    print("\n" + "=" * 60)
    print("å­¦ä¹ æ€»ç»“")
    print("=" * 60)

    print("""
1. DIN æ ¸å¿ƒåˆ›æ–°
   âœ“ Attention æœºåˆ¶ï¼šæ ¹æ®å€™é€‰ç‰©å“åŠ¨æ€è®¡ç®—ç”¨æˆ·å…´è¶£
   âœ“ ç”¨æˆ·è¡¨ç¤ºä¸å†å›ºå®šï¼Œè€Œæ˜¯é’ˆå¯¹ä¸åŒå€™é€‰æœ‰ä¸åŒè¡¨ç¤º
   âœ“ ä½“ç°äº†ç”¨æˆ·å…´è¶£çš„å¤šæ ·æ€§

2. Attention è®¡ç®—è¿‡ç¨‹
   âœ“ ç›¸å…³æ€§ç‰¹å¾ï¼š[candidate, history, sub, mul]
   âœ“ Attention MLPï¼šè®¡ç®—æ¯ä¸ªå†å²ç‰©å“çš„åˆ†æ•°
   âœ“ Softmaxï¼šå½’ä¸€åŒ–ä¸ºæƒé‡
   âœ“ åŠ æƒæ±‚å’Œï¼šå¾—åˆ°ç”¨æˆ·å…´è¶£å‘é‡

3. ä¸ä¹‹å‰æ¨¡å‹çš„å¯¹æ¯”
   DeepFM:
   - ç”¨æˆ·è¡¨ç¤ºå›ºå®š
   - æ‰€æœ‰å€™é€‰ç‰©å“ç”¨åŒä¸€ä¸ªç”¨æˆ·å‘é‡

   DIN:
   - ç”¨æˆ·è¡¨ç¤ºåŠ¨æ€ï¼ˆæ ¹æ®å€™é€‰è®¡ç®—ï¼‰
   - ä¸åŒå€™é€‰æ¿€æ´»ä¸åŒçš„å†å²è¡Œä¸º

4. å¯è§£é‡Šæ€§
   âœ“ å¯ä»¥çœ‹åˆ°æ¨¡å‹å…³æ³¨äº†å“ªäº›å†å²è¡Œä¸º
   âœ“ æœ‰åŠ©äºç†è§£æ¨èç»“æœ
   âœ“ ä¾¿äºè°ƒè¯•å’Œä¼˜åŒ–

5. å·¥ä¸šåº”ç”¨
   âœ“ é˜¿é‡Œå·´å·´ï¼šæ·˜å®ã€å¤©çŒ«æ¨è
   âœ“ é€‚åˆç”¨æˆ·å…´è¶£å¤šæ ·çš„åœºæ™¯
   âœ“ CTR æå‡ ~10%ï¼ˆè®ºæ–‡æ•°æ®ï¼‰

6. æŠ€æœ¯è¦ç‚¹
   âœ“ Padding + Maskï¼šå¤„ç†å˜é•¿åºåˆ—
   âœ“ Attention æƒé‡å¯è§†åŒ–ï¼šå¢å¼ºå¯è§£é‡Šæ€§
   âœ“ 4ç§ç›¸å…³æ€§ç‰¹å¾ï¼šå……åˆ†å»ºæ¨¡å€™é€‰å’Œå†å²çš„å…³ç³»

7. ä¸‹ä¸€æ­¥
   â†’ DIENï¼šç”¨æˆ·å…´è¶£è¿›åŒ–ç½‘ç»œï¼ˆGRU + Attentionï¼‰
   â†’ å¤šä»»åŠ¡å­¦ä¹ ï¼šåŒæ—¶é¢„æµ‹ç‚¹å‡»å’Œè½¬åŒ–
   â†’ åºåˆ—æ¨èï¼šè€ƒè™‘ç”¨æˆ·è¡Œä¸ºçš„æ—¶é—´é¡ºåº
    """)

    print("\nâœ… DIN å­¦ä¹ å®Œæˆï¼")
    print("\næç¤º: DIN æ˜¯é˜¿é‡Œå·´å·´æ¨èç³»ç»Ÿçš„æ ¸å¿ƒæ¨¡å‹ä¹‹ä¸€")
    print("      Attention æœºåˆ¶è®©æ¨¡å‹æ›´åŠ çµæ´»å’Œå¯è§£é‡Š")


if __name__ == "__main__":
    main()
