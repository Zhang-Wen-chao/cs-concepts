# 机器学习基础实践

> 从最简单的例子开始，理解机器学习的核心概念

## 📖 学习内容

### 01 - 线性回归 (Linear Regression)
**概念：** 找一条直线拟合数据点

**实现内容：**
- 从零实现线性回归
- 理解损失函数 (MSE)
- 手动计算梯度
- 观察参数更新过程

---

### 02 - 梯度下降 (Gradient Descent)
**概念：** 如何找到最优参数

**实现内容：**
- 批量梯度下降 (BGD)
- 随机梯度下降 (SGD)
- 小批量梯度下降 (Mini-batch GD)
- 学习率的影响
- 可视化下降过程

---

### 03 - 逻辑回归 (Logistic Regression) ✅
**概念：** 从回归到分类

**实现内容：**
- Sigmoid 函数
- 交叉熵损失（vs MSE的优势）
- 二分类问题
- 决策边界可视化
- 逻辑回归的局限性（非线性数据）

**已掌握：** 2025-11-15

---

### 04 - Softmax 回归 (Softmax Regression) ✅
**概念：** 多分类问题（3 个或更多类别）

**实现内容：**
- Softmax 函数（Sigmoid 的多分类推广）
- One-hot 编码
- 多分类交叉熵损失
- 参数结构（K 个逻辑回归并行）
- 决策边界可视化（多类别）

**已掌握：** 2025-11-15

---

### 05 - 正则化 (Regularization) ✅
**概念：** 防止过拟合，提高泛化能力

**实现内容：**
- L1 正则化（Lasso）- 特征选择
- L2 正则化（Ridge）- 权重衰减
- 过拟合 vs 欠拟合
- 正则化强度的影响
- λ 参数扫描

**已掌握：** 2025-11-15

---

### 06 - 神经网络基础 (Neural Networks / MLP) ✅
**概念：** 多层感知机，从线性到非线性

**实现内容：**
- 多层网络结构
- 前向传播算法
- 反向传播算法（手动推导）
- 非线性激活函数（ReLU、Sigmoid、Tanh）
- 与逻辑回归的对比
- 隐藏层神经元数量的影响

**已掌握：** 2025-11-15

---

## 🎯 学习目标

通过这些练习，你应该能够：
- ✅ 理解监督学习的基本流程
- ✅ 掌握损失函数和梯度的概念
- ✅ 理解梯度下降的工作原理
- ✅ 能够从零实现简单的机器学习模型

## 💡 学习建议

1. **按顺序学习** - 每个概念都建立在前一个基础上
2. **修改参数实验** - 改变学习率、迭代次数，观察效果
3. **对比 sklearn** - 和标准实现对比，验证正确性
4. **动手画图** - 可视化是理解的关键

