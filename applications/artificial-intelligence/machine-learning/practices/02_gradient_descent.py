"""
æ¢¯åº¦ä¸‹é™ç®—æ³•å¯¹æ¯” (Gradient Descent Variants)

é—®é¢˜ï¼šå¯¹æ¯”ä¸‰ç§æ¢¯åº¦ä¸‹é™æ–¹æ³•çš„ä¼˜ç¼ºç‚¹
ç›®æ ‡ï¼šç†è§£ BGDã€SGDã€Mini-batch GD çš„å·®å¼‚å’Œé€‚ç”¨åœºæ™¯

æ ¸å¿ƒæ¦‚å¿µï¼š
1. BGD (Batch GD)ï¼šæ¯æ¬¡ç”¨å…¨éƒ¨æ•°æ®è®¡ç®—æ¢¯åº¦ â†’ ç¨³å®šä½†æ…¢
2. SGD (Stochastic GD)ï¼šæ¯æ¬¡ç”¨1ä¸ªæ ·æœ¬è®¡ç®—æ¢¯åº¦ â†’ å¿«ä½†æ³¢åŠ¨å¤§
3. Mini-batch GDï¼šæ¯æ¬¡ç”¨ä¸€å°æ‰¹æ ·æœ¬ â†’ æŠ˜ä¸­æ–¹æ¡ˆï¼ˆæœ€å¸¸ç”¨ï¼‰
"""

import numpy as np
import matplotlib.pyplot as plt


# ==================== 1. å‡†å¤‡æ•°æ® ====================
def generate_data(n_samples=100):
    """
    ç”Ÿæˆæ¨¡æ‹Ÿæ•°æ®ï¼šæˆ¿å­é¢ç§¯ -> æˆ¿ä»·
    çœŸå®å…³ç³»ï¼šæˆ¿ä»· = 3 * é¢ç§¯ + 2 + å™ªå£°
    """
    np.random.seed(42)
    X = np.random.uniform(1, 5, n_samples)
    y = 3 * X + 2 + np.random.normal(0, 0.5, n_samples)
    return X, y


# ==================== 2. ä¸‰ç§æ¢¯åº¦ä¸‹é™å®ç° ====================
class GradientDescentComparison:
    """å¯¹æ¯”ä¸‰ç§æ¢¯åº¦ä¸‹é™æ–¹æ³•"""

    def __init__(self, learning_rate=0.01, n_epochs=100):
        """
        å‚æ•°ï¼š
            learning_rate: å­¦ä¹ ç‡
            n_epochs: è®­ç»ƒè½®æ•°ï¼ˆepoch = éå†å®Œæ•´ä¸ªæ•°æ®é›†ä¸€æ¬¡ï¼‰
        """
        self.lr = learning_rate
        self.n_epochs = n_epochs
        self.w = None
        self.b = None

    def _compute_gradient(self, X, y, w, b):
        """
        è®¡ç®—æ¢¯åº¦ï¼ˆé€šç”¨å‡½æ•°ï¼‰

        å‚æ•°ï¼š
            X, y: æ•°æ®ï¼ˆå¯ä»¥æ˜¯å…¨éƒ¨ã€ä¸€ä¸ªæ ·æœ¬ã€æˆ–ä¸€æ‰¹æ ·æœ¬ï¼‰
            w, b: å½“å‰å‚æ•°
        è¿”å›ï¼š
            dw, db: æ¢¯åº¦
        """
        n = len(X)
        y_pred = w * X + b
        dw = (2 / n) * np.sum((y_pred - y) * X)
        db = (2 / n) * np.sum(y_pred - y)
        return dw, db

    def _compute_loss(self, X, y, w, b):
        """è®¡ç®— MSE æŸå¤±"""
        y_pred = w * X + b
        return np.mean((y_pred - y) ** 2)

    # ========== æ–¹æ³•1ï¼šæ‰¹é‡æ¢¯åº¦ä¸‹é™ (BGD) ==========
    def batch_gd(self, X, y):
        """
        æ‰¹é‡æ¢¯åº¦ä¸‹é™ï¼šæ¯æ¬¡ç”¨å…¨éƒ¨æ•°æ®è®¡ç®—æ¢¯åº¦

        ä¼˜ç‚¹ï¼š
        - æ¢¯åº¦å‡†ç¡®ï¼Œä¸‹é™ç¨³å®š
        - ç†è®ºæ”¶æ•›ä¿è¯å¼º

        ç¼ºç‚¹ï¼š
        - æ•°æ®é‡å¤§æ—¶å¾ˆæ…¢ï¼ˆæ¯æ¬¡è¿­ä»£è¦è®¡ç®—æ‰€æœ‰æ ·æœ¬ï¼‰
        - å†…å­˜å ç”¨å¤§
        """
        n_samples = len(X)
        self.w, self.b = 0.0, 0.0
        loss_history = []
        w_history, b_history = [self.w], [self.b]

        for epoch in range(self.n_epochs):
            # ç”¨å…¨éƒ¨æ•°æ®è®¡ç®—æ¢¯åº¦
            dw, db = self._compute_gradient(X, y, self.w, self.b)

            # æ›´æ–°å‚æ•°
            self.w -= self.lr * dw
            self.b -= self.lr * db

            # è®°å½•
            loss = self._compute_loss(X, y, self.w, self.b)
            loss_history.append(loss)
            w_history.append(self.w)
            b_history.append(self.b)

        return loss_history, w_history, b_history

    # ========== æ–¹æ³•2ï¼šéšæœºæ¢¯åº¦ä¸‹é™ (SGD) ==========
    def stochastic_gd(self, X, y):
        """
        éšæœºæ¢¯åº¦ä¸‹é™ï¼šæ¯æ¬¡åªç”¨1ä¸ªæ ·æœ¬è®¡ç®—æ¢¯åº¦

        ä¼˜ç‚¹ï¼š
        - æ›´æ–°é¢‘ç¹ï¼Œæ”¶æ•›å¿«
        - å†…å­˜å ç”¨å°
        - å¯èƒ½è·³å‡ºå±€éƒ¨æœ€ä¼˜

        ç¼ºç‚¹ï¼š
        - æ¢¯åº¦å™ªå£°å¤§ï¼Œæ³¢åŠ¨å‰§çƒˆ
        - ä¸ä¿è¯æ¯æ¬¡è¿­ä»£éƒ½å‡å°æŸå¤±
        - éœ€è¦è°ƒæ•´å­¦ä¹ ç‡ï¼ˆé€šå¸¸è¦æ›´å°ï¼‰
        """
        n_samples = len(X)
        self.w, self.b = 0.0, 0.0
        loss_history = []
        w_history, b_history = [self.w], [self.b]

        for epoch in range(self.n_epochs):
            # æ‰“ä¹±æ•°æ®é¡ºåºï¼ˆé‡è¦ï¼é¿å…é¡ºåºåå·®ï¼‰
            indices = np.random.permutation(n_samples)

            # éå†æ¯ä¸ªæ ·æœ¬
            for i in indices:
                # ç”¨å•ä¸ªæ ·æœ¬è®¡ç®—æ¢¯åº¦
                X_i = np.array([X[i]])
                y_i = np.array([y[i]])
                dw, db = self._compute_gradient(X_i, y_i, self.w, self.b)

                # æ›´æ–°å‚æ•°
                self.w -= self.lr * dw
                self.b -= self.lr * db

            # è®°å½•ï¼ˆæ¯ä¸ª epoch ç»“æŸåï¼‰
            loss = self._compute_loss(X, y, self.w, self.b)
            loss_history.append(loss)
            w_history.append(self.w)
            b_history.append(self.b)

        return loss_history, w_history, b_history

    # ========== æ–¹æ³•3ï¼šå°æ‰¹é‡æ¢¯åº¦ä¸‹é™ (Mini-batch GD) ==========
    def minibatch_gd(self, X, y, batch_size=10):
        """
        å°æ‰¹é‡æ¢¯åº¦ä¸‹é™ï¼šæ¯æ¬¡ç”¨ä¸€å°æ‰¹æ ·æœ¬è®¡ç®—æ¢¯åº¦

        ä¼˜ç‚¹ï¼š
        - å¹³è¡¡äº† BGD å’Œ SGD çš„ä¼˜ç¼ºç‚¹
        - å¯ä»¥åˆ©ç”¨å‘é‡åŒ–åŠ é€Ÿ
        - æ¢¯åº¦ä¼°è®¡ç›¸å¯¹å‡†ç¡®ä¸”æ›´æ–°é¢‘ç¹
        - å·¥ä¸šç•Œæœ€å¸¸ç”¨ï¼ˆæ·±åº¦å­¦ä¹ é»˜è®¤é€‰æ‹©ï¼‰

        ç¼ºç‚¹ï¼š
        - éœ€è¦è°ƒæ•´ batch_size è¿™ä¸ªè¶…å‚æ•°

        batch_size é€‰æ‹©å»ºè®®ï¼š
        - å°æ•°æ®é›†ï¼ˆ<1000ï¼‰ï¼š32-64
        - ä¸­æ•°æ®é›†ï¼ˆ1000-10ä¸‡ï¼‰ï¼š128-256
        - å¤§æ•°æ®é›†ï¼ˆ>10ä¸‡ï¼‰ï¼š256-512
        """
        n_samples = len(X)
        self.w, self.b = 0.0, 0.0
        loss_history = []
        w_history, b_history = [self.w], [self.b]

        for epoch in range(self.n_epochs):
            # æ‰“ä¹±æ•°æ®
            indices = np.random.permutation(n_samples)

            # åˆ†æ‰¹å¤„ç†
            for start_idx in range(0, n_samples, batch_size):
                # å–ä¸€ä¸ª batch
                batch_indices = indices[start_idx:start_idx + batch_size]
                X_batch = X[batch_indices]
                y_batch = y[batch_indices]

                # ç”¨ batch è®¡ç®—æ¢¯åº¦
                dw, db = self._compute_gradient(X_batch, y_batch, self.w, self.b)

                # æ›´æ–°å‚æ•°
                self.w -= self.lr * dw
                self.b -= self.lr * db

            # è®°å½•
            loss = self._compute_loss(X, y, self.w, self.b)
            loss_history.append(loss)
            w_history.append(self.w)
            b_history.append(self.b)

        return loss_history, w_history, b_history

    def predict(self, X):
        """é¢„æµ‹"""
        return self.w * X + self.b


# ==================== 3. å¯è§†åŒ–å¯¹æ¯” ====================
def visualize_comparison(X, y, results):
    """
    å¯è§†åŒ–ä¸‰ç§æ–¹æ³•çš„å¯¹æ¯”

    results: {
        'BGD': (loss_history, w_history, b_history),
        'SGD': (...),
        'Mini-batch': (...)
    }
    """
    fig = plt.figure(figsize=(18, 5))

    # å­å›¾1ï¼šæŸå¤±å‡½æ•°ä¸‹é™æ›²çº¿
    plt.subplot(1, 3, 1)
    for name, (loss_history, _, _) in results.items():
        plt.plot(loss_history, label=name, linewidth=2, alpha=0.8)
    plt.xlabel('Epoch', fontsize=12)
    plt.ylabel('Loss (MSE)', fontsize=12)
    plt.title('Loss vs Epoch (Convergence Speed)', fontsize=14, fontweight='bold')
    plt.legend(fontsize=11)
    plt.grid(True, alpha=0.3)
    plt.yscale('log')  # å¯¹æ•°åæ ‡ï¼Œæ–¹ä¾¿è§‚å¯Ÿ

    # å­å›¾2ï¼šå‚æ•°æ”¶æ•›è½¨è¿¹ï¼ˆåœ¨å‚æ•°ç©ºé—´ä¸­çš„è·¯å¾„ï¼‰
    plt.subplot(1, 3, 2)
    true_w, true_b = 3, 2  # çœŸå®å€¼
    plt.scatter(true_w, true_b, s=300, c='red', marker='*',
                label='True (w=3, b=2)', zorder=5, edgecolors='black', linewidths=2)

    for name, (_, w_history, b_history) in results.items():
        plt.plot(w_history, b_history, 'o-', label=name, alpha=0.7, markersize=4)

    plt.xlabel('Weight (w)', fontsize=12)
    plt.ylabel('Bias (b)', fontsize=12)
    plt.title('Parameter Trajectory in (w, b) Space', fontsize=14, fontweight='bold')
    plt.legend(fontsize=11)
    plt.grid(True, alpha=0.3)

    # å­å›¾3ï¼šæœ€ç»ˆæ‹Ÿåˆæ•ˆæœ
    plt.subplot(1, 3, 3)
    plt.scatter(X, y, alpha=0.5, label='Data', s=20)

    colors = ['red', 'green', 'blue']
    for (name, (_, w_history, b_history)), color in zip(results.items(), colors):
        final_w, final_b = w_history[-1], b_history[-1]
        X_line = np.linspace(X.min(), X.max(), 100)
        y_line = final_w * X_line + final_b
        plt.plot(X_line, y_line, color=color, linewidth=2,
                label=f'{name}: y={final_w:.2f}x+{final_b:.2f}', alpha=0.8)

    plt.xlabel('X', fontsize=12)
    plt.ylabel('y', fontsize=12)
    plt.title('Final Fitted Lines', fontsize=14, fontweight='bold')
    plt.legend(fontsize=10)
    plt.grid(True, alpha=0.3)

    plt.tight_layout()
    plt.savefig('gradient_descent_comparison.png', dpi=100)
    print("\nğŸ“Š å¯¹æ¯”å›¾å·²ä¿å­˜åˆ°: gradient_descent_comparison.png")
    plt.show()


# ==================== 4. ä¸»ç¨‹åº ====================
def main():
    print("=" * 70)
    print("æ¢¯åº¦ä¸‹é™ç®—æ³•å¯¹æ¯”ï¼šBGD vs SGD vs Mini-batch GD")
    print("=" * 70)

    # ç”Ÿæˆæ•°æ®
    print("\nğŸ“Š ç”Ÿæˆæ•°æ®ï¼ˆ100ä¸ªæ ·æœ¬ï¼ŒçœŸå®å…³ç³»ï¼šy = 3x + 2ï¼‰")
    X, y = generate_data(100)

    # è®­ç»ƒå‚æ•°
    learning_rate = 0.01
    n_epochs = 50

    print(f"\nâš™ï¸  è®­ç»ƒå‚æ•°ï¼š")
    print(f"   å­¦ä¹ ç‡: {learning_rate}")
    print(f"   è®­ç»ƒè½®æ•°: {n_epochs}")
    print(f"   Mini-batch å¤§å°: 10")

    # è®­ç»ƒä¸‰ç§æ–¹æ³•
    results = {}

    print("\n" + "-" * 70)
    print("ğŸ”µ æ–¹æ³•1ï¼šæ‰¹é‡æ¢¯åº¦ä¸‹é™ (BGD)")
    print("   ç­–ç•¥ï¼šæ¯æ¬¡ç”¨å…¨éƒ¨100ä¸ªæ ·æœ¬è®¡ç®—æ¢¯åº¦")
    model_bgd = GradientDescentComparison(learning_rate, n_epochs)
    loss_bgd, w_bgd, b_bgd = model_bgd.batch_gd(X, y)
    results['BGD'] = (loss_bgd, w_bgd, b_bgd)
    print(f"   æœ€ç»ˆç»“æœï¼šw={model_bgd.w:.4f}, b={model_bgd.b:.4f}, loss={loss_bgd[-1]:.4f}")

    print("\n" + "-" * 70)
    print("ğŸŸ¢ æ–¹æ³•2ï¼šéšæœºæ¢¯åº¦ä¸‹é™ (SGD)")
    print("   ç­–ç•¥ï¼šæ¯æ¬¡åªç”¨1ä¸ªæ ·æœ¬è®¡ç®—æ¢¯åº¦")
    model_sgd = GradientDescentComparison(learning_rate, n_epochs)
    loss_sgd, w_sgd, b_sgd = model_sgd.stochastic_gd(X, y)
    results['SGD'] = (loss_sgd, w_sgd, b_sgd)
    print(f"   æœ€ç»ˆç»“æœï¼šw={model_sgd.w:.4f}, b={model_sgd.b:.4f}, loss={loss_sgd[-1]:.4f}")

    print("\n" + "-" * 70)
    print("ğŸŸ¡ æ–¹æ³•3ï¼šå°æ‰¹é‡æ¢¯åº¦ä¸‹é™ (Mini-batch GD)")
    print("   ç­–ç•¥ï¼šæ¯æ¬¡ç”¨10ä¸ªæ ·æœ¬è®¡ç®—æ¢¯åº¦")
    model_minibatch = GradientDescentComparison(learning_rate, n_epochs)
    loss_minibatch, w_minibatch, b_minibatch = model_minibatch.minibatch_gd(X, y, batch_size=10)
    results['Mini-batch'] = (loss_minibatch, w_minibatch, b_minibatch)
    print(f"   æœ€ç»ˆç»“æœï¼šw={model_minibatch.w:.4f}, b={model_minibatch.b:.4f}, loss={loss_minibatch[-1]:.4f}")

    # å¯¹æ¯”åˆ†æ
    print("\n" + "=" * 70)
    print("ğŸ“Š å¯¹æ¯”åˆ†æ")
    print("=" * 70)
    print(f"\næ”¶æ•›é€Ÿåº¦ï¼ˆæœ€ç»ˆæŸå¤±ï¼‰ï¼š")
    print(f"  BGD:        {loss_bgd[-1]:.6f}")
    print(f"  SGD:        {loss_sgd[-1]:.6f}")
    print(f"  Mini-batch: {loss_minibatch[-1]:.6f}")

    print(f"\nå‚æ•°å‡†ç¡®åº¦ï¼ˆçœŸå®å€¼ w=3, b=2ï¼‰ï¼š")
    print(f"  BGD:        w={model_bgd.w:.4f}, b={model_bgd.b:.4f}")
    print(f"  SGD:        w={model_sgd.w:.4f}, b={model_sgd.b:.4f}")
    print(f"  Mini-batch: w={model_minibatch.w:.4f}, b={model_minibatch.b:.4f}")

    # å¯è§†åŒ–
    print("\nğŸ¨ ç”Ÿæˆå¯è§†åŒ–å›¾è¡¨...")
    visualize_comparison(X, y, results)

    # æ€»ç»“
    print("\n" + "=" * 70)
    print("âœ… æ ¸å¿ƒç»“è®º")
    print("=" * 70)
    print("""
1. BGD (æ‰¹é‡æ¢¯åº¦ä¸‹é™)
   âœ“ æœ€ç¨³å®šï¼Œæ¢¯åº¦æœ€å‡†ç¡®
   âœ— å¤§æ•°æ®é›†æ—¶å¾ˆæ…¢
   ğŸ’¡ é€‚ç”¨ï¼šå°æ•°æ®é›†ã€éœ€è¦ç²¾ç¡®æ”¶æ•›

2. SGD (éšæœºæ¢¯åº¦ä¸‹é™)
   âœ“ æ›´æ–°æœ€å¿«ï¼Œå†…å­˜å ç”¨å°
   âœ— æ³¢åŠ¨å¤§ï¼Œä¸ç¨³å®š
   ğŸ’¡ é€‚ç”¨ï¼šè¶…å¤§æ•°æ®é›†ã€åœ¨çº¿å­¦ä¹ 

3. Mini-batch GD (å°æ‰¹é‡æ¢¯åº¦ä¸‹é™) â­ æ¨è
   âœ“ å¹³è¡¡é€Ÿåº¦å’Œç¨³å®šæ€§
   âœ“ å¯ä»¥åˆ©ç”¨GPUå¹¶è¡ŒåŠ é€Ÿ
   âœ“ æ·±åº¦å­¦ä¹ é»˜è®¤é€‰æ‹©
   ğŸ’¡ é€‚ç”¨ï¼šå‡ ä¹æ‰€æœ‰åœºæ™¯ï¼ˆå·¥ä¸šç•Œæ ‡å‡†ï¼‰

å…³é”®å»ºè®®ï¼š
- æ•°æ®é›† <1000ï¼šç”¨ BGD æˆ– Mini-batch(32-64)
- æ•°æ®é›† >10ä¸‡ï¼šå¿…é¡»ç”¨ Mini-batch(128-512) æˆ– SGD
- æ·±åº¦å­¦ä¹ ï¼šå§‹ç»ˆç”¨ Mini-batch GD
    """)

    print("=" * 70)


# ==================== 5. å®éªŒåŒº ====================
def experiment_batch_size():
    """
    å®éªŒï¼šä¸åŒ batch size çš„å½±å“

    è§‚å¯Ÿä» SGD (batch=1) åˆ° BGD (batch=100) çš„è¿‡æ¸¡
    """
    print("\n" + "=" * 70)
    print("ğŸ§ª å®éªŒï¼šä¸åŒ Batch Size çš„å½±å“")
    print("=" * 70)

    X, y = generate_data(100)
    learning_rate = 0.01
    n_epochs = 50

    batch_sizes = [1, 5, 10, 20, 50, 100]
    results = {}

    for bs in batch_sizes:
        model = GradientDescentComparison(learning_rate, n_epochs)
        if bs == 100:
            # batch_size = 100 = æ•°æ®æ€»æ•°ï¼Œç­‰åŒäº BGD
            loss_history, w_history, b_history = model.batch_gd(X, y)
            name = f'Batch={bs} (BGD)'
        else:
            loss_history, w_history, b_history = model.minibatch_gd(X, y, batch_size=bs)
            name = f'Batch={bs}'

        results[name] = (loss_history, w_history, b_history)
        print(f"  {name:15s}: æœ€ç»ˆæŸå¤±={loss_history[-1]:.6f}, w={model.w:.4f}, b={model.b:.4f}")

    # å¯è§†åŒ–
    plt.figure(figsize=(14, 5))

    # æŸå¤±æ›²çº¿
    plt.subplot(1, 2, 1)
    for name, (loss_history, _, _) in results.items():
        plt.plot(loss_history, label=name, linewidth=2, alpha=0.7)
    plt.xlabel('Epoch')
    plt.ylabel('Loss (MSE)')
    plt.title('Loss vs Epoch for Different Batch Sizes')
    plt.legend()
    plt.grid(True, alpha=0.3)
    plt.yscale('log')

    # å‚æ•°è½¨è¿¹
    plt.subplot(1, 2, 2)
    plt.scatter(3, 2, s=300, c='red', marker='*', label='True (3, 2)', zorder=5)
    for name, (_, w_history, b_history) in results.items():
        plt.plot(w_history, b_history, 'o-', label=name, alpha=0.7, markersize=3)
    plt.xlabel('Weight (w)')
    plt.ylabel('Bias (b)')
    plt.title('Parameter Trajectory')
    plt.legend()
    plt.grid(True, alpha=0.3)

    plt.tight_layout()
    plt.savefig('batch_size_comparison.png', dpi=100)
    print("\nğŸ“Š Batch size å¯¹æ¯”å›¾å·²ä¿å­˜åˆ°: batch_size_comparison.png")
    plt.show()

    print("\nğŸ’¡ è§‚å¯Ÿï¼š")
    print("  - Batch size è¶Šå° â†’ æ›´æ–°è¶Šé¢‘ç¹ï¼Œä½†æ³¢åŠ¨è¶Šå¤§")
    print("  - Batch size è¶Šå¤§ â†’ æ›´ç¨³å®šï¼Œä½†æ›´æ–°è¶Šæ…¢")
    print("  - Mini-batch (10-50) â†’ æœ€ä½³å¹³è¡¡ç‚¹")


if __name__ == "__main__":
    # ä¸»å®éªŒ
    main()

    # å¯é€‰ï¼šBatch size å®éªŒï¼ˆå–æ¶ˆæ³¨é‡Šè¿è¡Œï¼‰
    experiment_batch_size()

    print("\nğŸ’¡ æç¤ºï¼š")
    print("  - å–æ¶ˆ experiment_batch_size() çš„æ³¨é‡Šï¼Œæ¢ç´¢ batch size çš„å½±å“")
    print("  - å°è¯•ä¿®æ”¹å­¦ä¹ ç‡ï¼Œè§‚å¯Ÿä¸‰ç§æ–¹æ³•çš„è¡¨ç°å·®å¼‚")
    print("  - æ€è€ƒï¼šä¸ºä»€ä¹ˆæ·±åº¦å­¦ä¹ å‡ ä¹æ€»æ˜¯ç”¨ Mini-batch GDï¼Ÿ")
