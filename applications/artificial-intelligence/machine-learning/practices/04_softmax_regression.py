"""
Softmax å›å½’ (Softmax Regression / Multinomial Logistic Regression)

é—®é¢˜ï¼šå¦‚ä½•å¤„ç†å¤šåˆ†ç±»é—®é¢˜ï¼ˆ3 ä¸ªæˆ–æ›´å¤šç±»åˆ«ï¼‰ï¼Ÿ
ç›®æ ‡ï¼šå°†äºŒåˆ†ç±»çš„é€»è¾‘å›å½’æ‰©å±•åˆ°å¤šåˆ†ç±»

æ ¸å¿ƒæ¦‚å¿µï¼š
1. Softmax å‡½æ•°ï¼šå°† K ä¸ªç±»åˆ«çš„å¾—åˆ†è½¬æ¢ä¸ºæ¦‚ç‡åˆ†å¸ƒ
   p_k = e^(z_k) / Î£(e^(z_j))  å…¶ä¸­ Î£p_k = 1
2. äº¤å‰ç†µæŸå¤±ï¼ˆå¤šåˆ†ç±»ç‰ˆæœ¬ï¼‰ï¼š
   Loss = -Î£ y_k * log(p_k)  (y_k æ˜¯ one-hot ç¼–ç )
3. å†³ç­–ï¼šé€‰æ‹©æ¦‚ç‡æœ€å¤§çš„ç±»åˆ« argmax(p_k)

é€»è¾‘å›å½’ vs Softmax å›å½’ï¼š
- é€»è¾‘å›å½’ï¼š2 åˆ†ç±»ï¼ŒSigmoidï¼Œè¾“å‡º 1 ä¸ªæ¦‚ç‡
- Softmax å›å½’ï¼šK åˆ†ç±»ï¼ŒSoftmaxï¼Œè¾“å‡º K ä¸ªæ¦‚ç‡
"""

import numpy as np
import matplotlib.pyplot as plt
from sklearn.datasets import make_classification
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler


# ==================== 1. æ ¸å¿ƒå‡½æ•° ====================
def softmax(z):
    """
    Softmax å‡½æ•°ï¼šå°† K ä¸ªå¾—åˆ†è½¬æ¢ä¸ºæ¦‚ç‡åˆ†å¸ƒ

    è¾“å…¥ï¼šz = [z1, z2, ..., zK]  (K ä¸ªç±»åˆ«çš„å¾—åˆ†)
    è¾“å‡ºï¼šp = [p1, p2, ..., pK]  (K ä¸ªç±»åˆ«çš„æ¦‚ç‡ï¼Œå’Œä¸º 1)

    å…¬å¼ï¼šp_k = e^(z_k) / Î£(e^(z_j))

    ========================================================================
    ğŸ”‘ Softmax vs Sigmoid çš„å…³ç³»
    ========================================================================

    ã€Sigmoid (äºŒåˆ†ç±»)ã€‘
    ä¸¤ä¸ªç±»åˆ«ï¼š0 å’Œ 1
    p(y=1) = 1 / (1 + e^(-z))
    p(y=0) = 1 - p(y=1)

    ã€Softmax (å¤šåˆ†ç±»)ã€‘
    K ä¸ªç±»åˆ«ï¼š0, 1, 2, ..., K-1
    p(y=k) = e^(z_k) / Î£(e^(z_j))

    å½“ K=2 æ—¶ï¼ŒSoftmax é€€åŒ–ä¸º Sigmoidï¼

    æ¨å¯¼ï¼š
    p(y=1) = e^(z1) / (e^(z0) + e^(z1))
           = 1 / (1 + e^(z0-z1))
           = 1 / (1 + e^(-(z1-z0)))  â† è¿™å°±æ˜¯ Sigmoid!

    ç»“è®ºï¼šSigmoid æ˜¯ Softmax çš„ç‰¹ä¾‹ï¼ˆK=2ï¼‰

    ========================================================================
    ğŸ”‘ Softmax çš„æ€§è´¨
    ========================================================================

    1. è¾“å‡ºèŒƒå›´ï¼šæ¯ä¸ª p_k âˆˆ (0, 1)
    2. æ¦‚ç‡å’Œä¸º 1ï¼šÎ£ p_k = 1
    3. å•è°ƒæ€§ï¼šz_k è¶Šå¤§ï¼Œp_k è¶Šå¤§
    4. ç›¸å¯¹å¤§å°ï¼šä¸ä»…çœ‹ç»å¯¹å€¼ï¼Œè¿˜çœ‹ç›¸å¯¹å·®å¼‚

    ã€æ•°å€¼ç¨³å®šæ€§æŠ€å·§ã€‘
    ç›´æ¥è®¡ç®— e^(z_k) å¯èƒ½æº¢å‡ºï¼ˆz å¾ˆå¤§æ—¶ï¼‰

    è§£å†³æ–¹æ¡ˆï¼šå‡å»æœ€å¤§å€¼
    p_k = e^(z_k - max(z)) / Î£(e^(z_j - max(z)))

    ä¸ºä»€ä¹ˆæœ‰æ•ˆï¼Ÿ
    åˆ†å­åˆ†æ¯åŒæ—¶é™¤ä»¥ e^(max(z))ï¼Œç»“æœä¸å˜
    ä½†é¿å…äº† e^(å¤§æ•°) çš„æº¢å‡º

    ========================================================================
    """
    # æ•°å€¼ç¨³å®šæ€§ï¼šå‡å»æœ€å¤§å€¼
    z_shifted = z - np.max(z, axis=-1, keepdims=True)
    exp_z = np.exp(z_shifted)
    return exp_z / np.sum(exp_z, axis=-1, keepdims=True)


def categorical_cross_entropy(y_true, y_pred, epsilon=1e-15):
    """
    å¤šåˆ†ç±»äº¤å‰ç†µæŸå¤±

    è¾“å…¥ï¼š
        y_true: one-hot ç¼–ç çš„çœŸå®æ ‡ç­¾ï¼Œshape (n_samples, n_classes)
                ä¾‹å¦‚ï¼š[[1,0,0], [0,1,0], [0,0,1]]
        y_pred: é¢„æµ‹æ¦‚ç‡ï¼Œshape (n_samples, n_classes)
                ä¾‹å¦‚ï¼š[[0.7,0.2,0.1], [0.1,0.8,0.1], ...]

    å…¬å¼ï¼š
        Loss = -1/n * Î£Î£ y_ik * log(p_ik)
        å…¶ä¸­ i æ˜¯æ ·æœ¬ç´¢å¼•ï¼Œk æ˜¯ç±»åˆ«ç´¢å¼•

    ========================================================================
    ğŸ”‘ ç†è§£å¤šåˆ†ç±»äº¤å‰ç†µ
    ========================================================================

    ã€ç›´è§‰ç†è§£ã€‘
    å¯¹äºæ¯ä¸ªæ ·æœ¬ï¼Œåªæœ‰ä¸€ä¸ªç±»åˆ«æ˜¯æ­£ç¡®çš„ï¼ˆy_ik = 1ï¼‰
    å…¶ä»–ç±»åˆ« y_ik = 0ï¼Œå¯¹æŸå¤±æ²¡æœ‰è´¡çŒ®

    ä¾‹å­ï¼šçœŸå®æ ‡ç­¾æ˜¯ç±»åˆ« 1 (one-hot: [0, 1, 0])
    Loss = -(0*log(p0) + 1*log(p1) + 0*log(p2))
         = -log(p1)

    åªå…³å¿ƒæ­£ç¡®ç±»åˆ«çš„é¢„æµ‹æ¦‚ç‡ï¼

    ã€äºŒåˆ†ç±»äº¤å‰ç†µ vs å¤šåˆ†ç±»äº¤å‰ç†µã€‘

    äºŒåˆ†ç±»ï¼ˆK=2ï¼‰ï¼š
    Loss = -[y*log(p) + (1-y)*log(1-p)]

    å¤šåˆ†ç±»ï¼ˆK>2ï¼‰ï¼š
    Loss = -Î£ y_k * log(p_k)

    å½“ K=2 æ—¶ï¼Œä¸¤è€…ç­‰ä»·ï¼

    ã€ä¸ºä»€ä¹ˆç”¨ one-hot ç¼–ç ï¼Ÿã€‘
    ç±»åˆ«æ˜¯ç¦»æ•£çš„ï¼Œæ²¡æœ‰å¤§å°å…³ç³»
    - ä¸èƒ½ç”¨ 0, 1, 2 è¡¨ç¤ºç±»åˆ«ï¼ˆä¼šæš—ç¤º 2 > 1 > 0ï¼‰
    - ç”¨ one-hotï¼š[1,0,0], [0,1,0], [0,0,1]
    - æ¯ä¸ªç±»åˆ«éƒ½æ˜¯ç‹¬ç«‹çš„ç»´åº¦

    ========================================================================
    """
    # è£å‰ªé¢„æµ‹å€¼ï¼Œé¿å… log(0)
    y_pred = np.clip(y_pred, epsilon, 1 - epsilon)

    # è®¡ç®—äº¤å‰ç†µ
    # åªæœ‰æ­£ç¡®ç±»åˆ«ï¼ˆy_true=1ï¼‰çš„ä½ç½®æ‰æœ‰è´¡çŒ®
    loss = -np.mean(np.sum(y_true * np.log(y_pred), axis=1))
    return loss


def to_one_hot(y, n_classes):
    """
    å°†ç±»åˆ«æ ‡ç­¾è½¬æ¢ä¸º one-hot ç¼–ç 

    è¾“å…¥ï¼šy = [0, 2, 1]  (ç±»åˆ«ç´¢å¼•)
    è¾“å‡ºï¼š[[1, 0, 0],
          [0, 0, 1],
          [0, 1, 0]]  (one-hot å‘é‡)
    """
    one_hot = np.zeros((y.shape[0], n_classes))
    one_hot[np.arange(y.shape[0]), y] = 1
    return one_hot


# ==================== 2. Softmax å›å½’ç±» ====================
class SoftmaxRegression:
    """ä»é›¶å®ç° Softmax å›å½’ï¼ˆå¤šåˆ†ç±»é€»è¾‘å›å½’ï¼‰"""

    def __init__(self, learning_rate=0.01, n_epochs=1000, batch_size=32):
        """
        å‚æ•°ï¼š
            learning_rate: å­¦ä¹ ç‡
            n_epochs: è®­ç»ƒè½®æ•°
            batch_size: æ‰¹é‡å¤§å°
        """
        self.lr = learning_rate
        self.n_epochs = n_epochs
        self.batch_size = batch_size
        self.weights = None  # shape: (n_features, n_classes)
        self.bias = None     # shape: (n_classes,)
        self.loss_history = []

    def fit(self, X, y):
        """
        è®­ç»ƒæ¨¡å‹

        X: shape (n_samples, n_features)
        y: shape (n_samples,)  ç±»åˆ«ç´¢å¼• [0, 1, 2, ...]

        ====================================================================
        ğŸ”‘ Softmax å›å½’çš„å‚æ•°ç»“æ„
        ====================================================================

        é€»è¾‘å›å½’ï¼ˆK=2ï¼‰ï¼š
        - æƒé‡ï¼šw âˆˆ R^d  (d ä¸ªç‰¹å¾)
        - åç½®ï¼šb âˆˆ R    (1 ä¸ªå€¼)
        - è¾“å‡ºï¼šz = wx + b  (1 ä¸ªå¾—åˆ†)

        Softmax å›å½’ï¼ˆK ç±»ï¼‰ï¼š
        - æƒé‡ï¼šW âˆˆ R^(dÃ—K)  (æ¯ä¸ªç±»åˆ«ä¸€ç»„æƒé‡)
        - åç½®ï¼šb âˆˆ R^K      (æ¯ä¸ªç±»åˆ«ä¸€ä¸ªåç½®)
        - è¾“å‡ºï¼šZ = XW + b   (K ä¸ªå¾—åˆ†)

        å¯ä»¥ç†è§£ä¸ºï¼šK ä¸ªé€»è¾‘å›å½’å¹¶è¡Œè¿è¡Œï¼

        ä¾‹å­ï¼š3 ä¸ªç‰¹å¾ï¼Œ4 ä¸ªç±»åˆ«
        W = [w0_class0, w0_class1, w0_class2, w0_class3]  â† ç‰¹å¾ 0
            [w1_class0, w1_class1, w1_class2, w1_class3]  â† ç‰¹å¾ 1
            [w2_class0, w2_class1, w2_class2, w2_class3]  â† ç‰¹å¾ 2

        ====================================================================
        """
        n_samples, n_features = X.shape
        self.n_classes = len(np.unique(y))

        # åˆå§‹åŒ–å‚æ•°
        self.weights = np.zeros((n_features, self.n_classes))
        self.bias = np.zeros(self.n_classes)

        # è½¬æ¢ä¸º one-hot ç¼–ç 
        y_one_hot = to_one_hot(y, self.n_classes)

        # Mini-batch æ¢¯åº¦ä¸‹é™
        for epoch in range(self.n_epochs):
            # æ‰“ä¹±æ•°æ®
            indices = np.random.permutation(n_samples)

            # åˆ†æ‰¹è®­ç»ƒ
            for start_idx in range(0, n_samples, self.batch_size):
                batch_indices = indices[start_idx:start_idx + self.batch_size]
                X_batch = X[batch_indices]
                y_batch = y_one_hot[batch_indices]

                # ========== å‰å‘ä¼ æ’­ ==========
                # 1. çº¿æ€§ç»„åˆï¼šZ = XW + b
                #    X_batch: (batch_size, n_features)
                #    weights: (n_features, n_classes)
                #    Z: (batch_size, n_classes)
                Z = np.dot(X_batch, self.weights) + self.bias

                # 2. Softmax æ¿€æ´»
                #    å°† K ä¸ªå¾—åˆ†è½¬æ¢ä¸ºæ¦‚ç‡
                y_pred = softmax(Z)

                # ========== è®¡ç®—æ¢¯åº¦ ==========
                # æ¢¯åº¦æ¨å¯¼ï¼ˆç±»ä¼¼é€»è¾‘å›å½’ï¼‰ï¼š
                # âˆ‚Loss/âˆ‚W = X^T Â· (y_pred - y_true) / batch_size
                # âˆ‚Loss/âˆ‚b = sum(y_pred - y_true) / batch_size
                #
                # ç¥å¥‡çš„æ˜¯ï¼šå½¢å¼å’Œé€»è¾‘å›å½’å®Œå…¨ä¸€æ ·ï¼
                # åªæ˜¯ä»æ ‡é‡å˜æˆäº†å‘é‡/çŸ©é˜µ

                batch_size_actual = len(X_batch)
                error = y_pred - y_batch  # shape: (batch_size, n_classes)

                dW = np.dot(X_batch.T, error) / batch_size_actual
                db = np.sum(error, axis=0) / batch_size_actual

                # ========== æ›´æ–°å‚æ•° ==========
                self.weights -= self.lr * dW
                self.bias -= self.lr * db

            # è®°å½•æŸå¤±ï¼ˆæ¯ 10 ä¸ª epochï¼‰
            if epoch % 10 == 0:
                Z_all = np.dot(X, self.weights) + self.bias
                y_pred_all = softmax(Z_all)
                loss = categorical_cross_entropy(y_one_hot, y_pred_all)
                self.loss_history.append(loss)

    def predict_proba(self, X):
        """é¢„æµ‹æ¯ä¸ªç±»åˆ«çš„æ¦‚ç‡"""
        Z = np.dot(X, self.weights) + self.bias
        return softmax(Z)

    def predict(self, X):
        """é¢„æµ‹ç±»åˆ«ï¼ˆé€‰æ‹©æ¦‚ç‡æœ€å¤§çš„ï¼‰"""
        probabilities = self.predict_proba(X)
        return np.argmax(probabilities, axis=1)

    def score(self, X, y):
        """è®¡ç®—å‡†ç¡®ç‡"""
        y_pred = self.predict(X)
        return np.mean(y_pred == y)


# ==================== 3. æ•°æ®ç”Ÿæˆ ====================
def generate_multiclass_data(n_samples=600, n_classes=3, n_features=2, random_state=42):
    """
    ç”Ÿæˆå¤šåˆ†ç±»æ•°æ®

    è¿”å›ï¼š
        X: ç‰¹å¾ï¼Œshape (n_samples, n_features)
        y: ç±»åˆ«æ ‡ç­¾ï¼Œshape (n_samples,)

    æ³¨æ„ï¼šsklearn çš„é™åˆ¶
    - n_classes * n_clusters_per_class â‰¤ 2^n_informative
    - n_informative â‰¤ n_features
    - å¦‚æœ n_classes > 2^n_featuresï¼Œéœ€è¦å¢åŠ  n_features
    """
    # æ ¹æ®ç±»åˆ«æ•°åŠ¨æ€è°ƒæ•´å‚æ•°
    # ç¡®ä¿ 2^n_informative >= n_classes
    required_informative = int(np.ceil(np.log2(n_classes)))

    # å¦‚æœéœ€è¦çš„ n_informative è¶…è¿‡ n_featuresï¼Œå¢åŠ  n_features
    if required_informative > n_features:
        n_features = required_informative
        print(f"  âš ï¸  å¢åŠ ç‰¹å¾æ•°åˆ° {n_features}ï¼ˆç±»åˆ«æ•° {n_classes} éœ€è¦è‡³å°‘ {required_informative} ä¸ªä¿¡æ¯ç‰¹å¾ï¼‰")

    n_informative = min(required_informative, n_features)

    X, y = make_classification(
        n_samples=n_samples,
        n_features=n_features,
        n_informative=n_informative,
        n_redundant=0,
        n_clusters_per_class=1,
        n_classes=n_classes,
        class_sep=1.5,
        random_state=random_state
    )
    return X, y


# ==================== 4. å¯è§†åŒ– ====================
def plot_decision_boundary_multiclass(model, X, y, title="Decision Boundary"):
    """
    ç»˜åˆ¶å¤šåˆ†ç±»å†³ç­–è¾¹ç•Œ

    å¯¹äº Softmax å›å½’ï¼Œå†³ç­–è¾¹ç•Œæ˜¯çº¿æ€§çš„
    K ä¸ªç±»åˆ«ä¼šæœ‰ K ä¸ªåŒºåŸŸ
    """
    # è®¾ç½®ç½‘æ ¼
    x_min, x_max = X[:, 0].min() - 1, X[:, 0].max() + 1
    y_min, y_max = X[:, 1].min() - 1, X[:, 1].max() + 1
    xx, yy = np.meshgrid(
        np.linspace(x_min, x_max, 200),
        np.linspace(y_min, y_max, 200)
    )

    # é¢„æµ‹ç½‘æ ¼ç‚¹çš„ç±»åˆ«
    Z = model.predict(np.c_[xx.ravel(), yy.ravel()])
    Z = Z.reshape(xx.shape)

    # ç»˜åˆ¶å†³ç­–è¾¹ç•Œ
    plt.contourf(xx, yy, Z, alpha=0.3, cmap='viridis', levels=model.n_classes-1)

    # ç»˜åˆ¶æ•°æ®ç‚¹
    colors = ['blue', 'red', 'green', 'purple', 'orange']
    markers = ['o', 's', '^', 'D', 'v']

    for class_idx in range(model.n_classes):
        mask = (y == class_idx)
        plt.scatter(
            X[mask, 0], X[mask, 1],
            c=colors[class_idx % len(colors)],
            marker=markers[class_idx % len(markers)],
            s=50,
            edgecolors='k',
            label=f'Class {class_idx}',
            alpha=0.7
        )

    plt.xlabel('Feature 1', fontsize=12)
    plt.ylabel('Feature 2', fontsize=12)
    plt.title(title, fontsize=14, fontweight='bold')
    plt.legend(fontsize=10)
    plt.grid(True, alpha=0.3)


def visualize_softmax():
    """å¯è§†åŒ– Softmax å‡½æ•°çš„è¡Œä¸º"""
    fig, axes = plt.subplots(1, 2, figsize=(14, 5))

    # å·¦å›¾ï¼š3 ä¸ªç±»åˆ«çš„ Softmax
    z1 = np.linspace(-3, 3, 100)
    z2 = 0  # å›ºå®š
    z3 = 0  # å›ºå®š

    probs = []
    for z1_val in z1:
        z = np.array([z1_val, z2, z3])
        p = softmax(z.reshape(1, -1))[0]
        probs.append(p)
    probs = np.array(probs)

    axes[0].plot(z1, probs[:, 0], label='p(class 0)', linewidth=2, color='blue')
    axes[0].plot(z1, probs[:, 1], label='p(class 1)', linewidth=2, color='red')
    axes[0].plot(z1, probs[:, 2], label='p(class 2)', linewidth=2, color='green')
    axes[0].axhline(y=1/3, color='gray', linestyle='--', alpha=0.5)
    axes[0].set_xlabel('zâ‚ (score of class 0)', fontsize=12)
    axes[0].set_ylabel('Probability', fontsize=12)
    axes[0].set_title('Softmax: zâ‚ varies, zâ‚‚=zâ‚ƒ=0', fontsize=13, fontweight='bold')
    axes[0].legend(fontsize=10)
    axes[0].grid(True, alpha=0.3)
    axes[0].set_ylim(0, 1)

    # å³å›¾ï¼šæ¦‚ç‡å’Œå§‹ç»ˆä¸º 1
    axes[1].plot(z1, probs.sum(axis=1), linewidth=3, color='purple')
    axes[1].axhline(y=1, color='red', linestyle='--', linewidth=2, label='Sum = 1')
    axes[1].set_xlabel('zâ‚', fontsize=12)
    axes[1].set_ylabel('Sum of Probabilities', fontsize=12)
    axes[1].set_title('Softmax Property: Î£p = 1', fontsize=13, fontweight='bold')
    axes[1].legend(fontsize=10)
    axes[1].grid(True, alpha=0.3)
    axes[1].set_ylim(0.95, 1.05)

    plt.tight_layout()
    plt.savefig('softmax_function.png', dpi=100)
    print("ğŸ“Š Softmax å‡½æ•°å›¾å·²ä¿å­˜åˆ°: softmax_function.png")
    plt.show()


# ==================== 5. ä¸»ç¨‹åº ====================
def main():
    print("=" * 70)
    print("Softmax å›å½’ (Softmax Regression) - å¤šåˆ†ç±»ä»»åŠ¡")
    print("=" * 70)

    # ========== 1. å¯è§†åŒ– Softmax å‡½æ•° ==========
    print("\n" + "=" * 70)
    print("ğŸ“ˆ ç¬¬ä¸€æ­¥ï¼šç†è§£ Softmax å‡½æ•°")
    print("=" * 70)
    print("""
Softmax æ˜¯ Sigmoid çš„å¤šåˆ†ç±»æ¨å¹¿ï¼š
- Sigmoid: 2 ç±» â†’ è¾“å‡º 1 ä¸ªæ¦‚ç‡
- Softmax: K ç±» â†’ è¾“å‡º K ä¸ªæ¦‚ç‡ï¼ˆå’Œä¸º 1ï¼‰

å…³é”®æ€§è´¨ï¼š
1. æ‰€æœ‰æ¦‚ç‡å’Œä¸º 1
2. æŸä¸ªç±»åˆ«å¾—åˆ†è¶Šé«˜ï¼Œå…¶æ¦‚ç‡è¶Šå¤§
3. æ¦‚ç‡æ˜¯ç›¸å¯¹çš„ï¼ˆçœ‹æ‰€æœ‰ç±»åˆ«çš„ç›¸å¯¹å¤§å°ï¼‰
    """)
    visualize_softmax()

    # ========== 2. ç”Ÿæˆå¤šåˆ†ç±»æ•°æ® ==========
    print("\n" + "=" * 70)
    print("ğŸ“Š ç¬¬äºŒæ­¥ï¼šç”Ÿæˆå¤šåˆ†ç±»æ•°æ®ï¼ˆ3 ç±»ï¼‰")
    print("=" * 70)

    n_classes = 3
    X, y = generate_multiclass_data(n_samples=600, n_classes=n_classes)

    # æ ‡å‡†åŒ–ç‰¹å¾ï¼ˆå¯é€‰ï¼Œä½†é€šå¸¸æœ‰å¸®åŠ©ï¼‰
    scaler = StandardScaler()
    X = scaler.fit_transform(X)

    X_train, X_test, y_train, y_test = train_test_split(
        X, y, test_size=0.3, random_state=42
    )

    print(f"è®­ç»ƒé›†å¤§å°: {len(X_train)}")
    print(f"æµ‹è¯•é›†å¤§å°: {len(X_test)}")
    for i in range(n_classes):
        print(f"ç±»åˆ« {i} æ ·æœ¬æ•°: {np.sum(y_train == i)}")

    # ========== 3. è®­ç»ƒæ¨¡å‹ ==========
    print("\n" + "=" * 70)
    print("ğŸ”§ ç¬¬ä¸‰æ­¥ï¼šè®­ç»ƒ Softmax å›å½’æ¨¡å‹")
    print("=" * 70)

    model = SoftmaxRegression(learning_rate=0.1, n_epochs=200, batch_size=32)
    print(f"è¶…å‚æ•°ï¼šå­¦ä¹ ç‡={model.lr}, è®­ç»ƒè½®æ•°={model.n_epochs}, æ‰¹é‡å¤§å°={model.batch_size}")
    print("\nè®­ç»ƒä¸­...")
    model.fit(X_train, y_train)

    # ========== 4. è¯„ä¼°æ¨¡å‹ ==========
    print("\n" + "=" * 70)
    print("ğŸ“Š ç¬¬å››æ­¥ï¼šè¯„ä¼°æ¨¡å‹æ€§èƒ½")
    print("=" * 70)

    train_acc = model.score(X_train, y_train)
    test_acc = model.score(X_test, y_test)

    print(f"\nè®­ç»ƒé›†å‡†ç¡®ç‡: {train_acc:.4f} ({train_acc * 100:.2f}%)")
    print(f"æµ‹è¯•é›†å‡†ç¡®ç‡: {test_acc:.4f} ({test_acc * 100:.2f}%)")

    # æŸ¥çœ‹å‚æ•°å½¢çŠ¶
    print(f"\nå­¦ä¹ åˆ°çš„å‚æ•°å½¢çŠ¶ï¼š")
    print(f"  æƒé‡ W: {model.weights.shape}  (n_features Ã— n_classes)")
    print(f"  åç½® b: {model.bias.shape}    (n_classes,)")

    # é¢„æµ‹ç¤ºä¾‹
    print(f"\né¢„æµ‹ç¤ºä¾‹ï¼ˆå‰ 3 ä¸ªæµ‹è¯•æ ·æœ¬ï¼‰ï¼š")
    sample_probs = model.predict_proba(X_test[:3])
    sample_preds = model.predict(X_test[:3])

    for i in range(3):
        print(f"\n  æ ·æœ¬ {i+1}:")
        print(f"    çœŸå®ç±»åˆ«: {y_test[i]}")
        print(f"    é¢„æµ‹ç±»åˆ«: {sample_preds[i]}")
        print(f"    å„ç±»æ¦‚ç‡: {sample_probs[i]}")
        print(f"    (Class 0: {sample_probs[i][0]:.3f}, "
              f"Class 1: {sample_probs[i][1]:.3f}, "
              f"Class 2: {sample_probs[i][2]:.3f})")

    # ========== 5. å¯è§†åŒ–ç»“æœ ==========
    print("\n" + "=" * 70)
    print("ğŸ¨ ç¬¬äº”æ­¥ï¼šå¯è§†åŒ–å†³ç­–è¾¹ç•Œå’Œè®­ç»ƒè¿‡ç¨‹")
    print("=" * 70)

    fig = plt.figure(figsize=(16, 5))

    # å­å›¾1ï¼šè®­ç»ƒé›†å†³ç­–è¾¹ç•Œ
    plt.subplot(1, 3, 1)
    plot_decision_boundary_multiclass(
        model, X_train, y_train,
        title=f'Training Set\nAccuracy: {train_acc:.2%}'
    )

    # å­å›¾2ï¼šæµ‹è¯•é›†å†³ç­–è¾¹ç•Œ
    plt.subplot(1, 3, 2)
    plot_decision_boundary_multiclass(
        model, X_test, y_test,
        title=f'Test Set\nAccuracy: {test_acc:.2%}'
    )

    # å­å›¾3ï¼šæŸå¤±æ›²çº¿
    plt.subplot(1, 3, 3)
    plt.plot(range(0, model.n_epochs, 10), model.loss_history,
             linewidth=2, color='blue', marker='o', markersize=4)
    plt.xlabel('Epoch', fontsize=12)
    plt.ylabel('Cross-Entropy Loss', fontsize=12)
    plt.title('Training Loss vs Epoch', fontsize=14, fontweight='bold')
    plt.grid(True, alpha=0.3)

    plt.tight_layout()
    plt.savefig('softmax_regression_result.png', dpi=100)
    print("\nğŸ“Š ç»“æœå›¾å·²ä¿å­˜åˆ°: softmax_regression_result.png")
    plt.show()

    # ========== 6. å¯¹æ¯” sklearn ==========
    print("\n" + "=" * 70)
    print("ğŸ”¬ ç¬¬å…­æ­¥ï¼šä¸ sklearn å¯¹æ¯”éªŒè¯")
    print("=" * 70)

    from sklearn.linear_model import LogisticRegression as SklearnLR

    sklearn_model = SklearnLR(multi_class='multinomial', solver='lbfgs',
                              max_iter=1000, random_state=42)
    sklearn_model.fit(X_train, y_train)

    sklearn_train_acc = sklearn_model.score(X_train, y_train)
    sklearn_test_acc = sklearn_model.score(X_test, y_test)

    print(f"\nSklearn Softmax å›å½’ï¼ˆå¤šåˆ†ç±»é€»è¾‘å›å½’ï¼‰ï¼š")
    print(f"  è®­ç»ƒé›†å‡†ç¡®ç‡: {sklearn_train_acc:.4f} ({sklearn_train_acc * 100:.2f}%)")
    print(f"  æµ‹è¯•é›†å‡†ç¡®ç‡: {sklearn_test_acc:.4f} ({sklearn_test_acc * 100:.2f}%)")
    print(f"  æƒé‡å½¢çŠ¶: {sklearn_model.coef_.shape}")

    print(f"\nå¯¹æ¯”ç»“æœï¼š")
    print(f"  å‡†ç¡®ç‡å·®å¼‚ï¼ˆæµ‹è¯•é›†ï¼‰: {abs(test_acc - sklearn_test_acc):.4f}")
    print(f"  âœ… å®ç°åŸºæœ¬æ­£ç¡®ï¼")


# ==================== 6. å®éªŒåŒº ====================
def experiment_num_classes():
    """
    å®éªŒï¼šä¸åŒç±»åˆ«æ•°çš„å½±å“

    è§‚å¯Ÿ 2 ç±»ã€3 ç±»ã€4 ç±»ã€5 ç±»çš„è¡¨ç°
    """
    print("\n" + "=" * 70)
    print("ğŸ§ª å®éªŒï¼šä¸åŒç±»åˆ«æ•°çš„å½±å“")
    print("=" * 70)

    results = []

    for n_classes in [2, 3, 4, 5]:
        print(f"\nè®­ç»ƒ {n_classes} ç±»åˆ†ç±»å™¨...")

        # ç”Ÿæˆæ•°æ®
        X, y = generate_multiclass_data(n_samples=600, n_classes=n_classes)
        scaler = StandardScaler()
        X = scaler.fit_transform(X)
        X_train, X_test, y_train, y_test = train_test_split(
            X, y, test_size=0.3, random_state=42
        )

        # è®­ç»ƒ
        model = SoftmaxRegression(learning_rate=0.1, n_epochs=200)
        model.fit(X_train, y_train)

        # è¯„ä¼°
        train_acc = model.score(X_train, y_train)
        test_acc = model.score(X_test, y_test)

        results.append({
            'n_classes': n_classes,
            'train_acc': train_acc,
            'test_acc': test_acc,
            'n_params': model.weights.size + model.bias.size
        })

        print(f"  æµ‹è¯•å‡†ç¡®ç‡: {test_acc:.4f}")
        print(f"  å‚æ•°æ€»æ•°: {results[-1]['n_params']}")

    # å¯è§†åŒ–
    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 5))

    n_classes_list = [r['n_classes'] for r in results]
    test_accs = [r['test_acc'] for r in results]
    n_params_list = [r['n_params'] for r in results]

    # å‡†ç¡®ç‡ vs ç±»åˆ«æ•°
    ax1.plot(n_classes_list, test_accs, 'o-', linewidth=2, markersize=10, color='blue')
    ax1.set_xlabel('Number of Classes', fontsize=12)
    ax1.set_ylabel('Test Accuracy', fontsize=12)
    ax1.set_title('Accuracy vs Number of Classes', fontsize=13, fontweight='bold')
    ax1.grid(True, alpha=0.3)
    ax1.set_xticks(n_classes_list)

    # å‚æ•°æ•°é‡ vs ç±»åˆ«æ•°
    ax2.plot(n_classes_list, n_params_list, 'o-', linewidth=2, markersize=10, color='red')
    ax2.set_xlabel('Number of Classes', fontsize=12)
    ax2.set_ylabel('Number of Parameters', fontsize=12)
    ax2.set_title('Parameters vs Number of Classes', fontsize=13, fontweight='bold')
    ax2.grid(True, alpha=0.3)
    ax2.set_xticks(n_classes_list)

    plt.tight_layout()
    plt.savefig('softmax_num_classes.png', dpi=100)
    print("\nğŸ“Š ç±»åˆ«æ•°å¯¹æ¯”å›¾å·²ä¿å­˜åˆ°: softmax_num_classes.png")
    plt.show()

    print("\nğŸ’¡ è§‚å¯Ÿï¼š")
    print("  - ç±»åˆ«è¶Šå¤šï¼Œé—®é¢˜è¶Šéš¾ï¼Œå‡†ç¡®ç‡å¯èƒ½ä¸‹é™")
    print("  - å‚æ•°æ•°é‡çº¿æ€§å¢é•¿ï¼šn_params = (n_features + 1) Ã— n_classes")


# ==================== 7. æ€»ç»“ ====================
def print_summary():
    print("\n" + "=" * 70)
    print("âœ… æ ¸å¿ƒè¦ç‚¹æ€»ç»“")
    print("=" * 70)
    print("""
1. Softmax å›å½’ = å¤šåˆ†ç±»é€»è¾‘å›å½’
   - äºŒåˆ†ç±»ï¼šSigmoid
   - å¤šåˆ†ç±»ï¼šSoftmaxï¼ˆSigmoid çš„æ¨å¹¿ï¼‰

2. Softmax å‡½æ•°
   å…¬å¼ï¼šp_k = e^(z_k) / Î£(e^(z_j))
   æ€§è´¨ï¼š
   - è¾“å‡º K ä¸ªæ¦‚ç‡ï¼Œå’Œä¸º 1
   - å•è°ƒï¼šz_k è¶Šå¤§ï¼Œp_k è¶Šå¤§
   - ç›¸å¯¹ï¼šçœ‹æ‰€æœ‰ç±»åˆ«çš„ç›¸å¯¹å¤§å°

3. å‚æ•°ç»“æ„
   - æƒé‡ï¼šW âˆˆ R^(dÃ—K)  æ¯ä¸ªç±»åˆ«ä¸€ç»„æƒé‡
   - åç½®ï¼šb âˆˆ R^K      æ¯ä¸ªç±»åˆ«ä¸€ä¸ªåç½®
   - å¯ä»¥ç†è§£ä¸º K ä¸ªé€»è¾‘å›å½’å¹¶è¡Œ

4. One-hot ç¼–ç 
   - ç±»åˆ«æ˜¯ç¦»æ•£çš„ï¼Œç”¨ one-hot è¡¨ç¤º
   - [1,0,0], [0,1,0], [0,0,1]
   - æ¯ä¸ªç±»åˆ«ç‹¬ç«‹ï¼Œæ— å¤§å°å…³ç³»

5. äº¤å‰ç†µæŸå¤±ï¼ˆå¤šåˆ†ç±»ï¼‰
   Loss = -Î£ y_k * log(p_k)
   - åªæœ‰æ­£ç¡®ç±»åˆ«ï¼ˆy_k=1ï¼‰æœ‰è´¡çŒ®
   - ç­‰ä»·äº -log(p_æ­£ç¡®ç±»åˆ«)

6. æ¢¯åº¦å…¬å¼ï¼ˆç¥å¥‡çš„ç®€æ´ï¼‰
   âˆ‚Loss/âˆ‚W = X^T Â· (y_pred - y_true)
   âˆ‚Loss/âˆ‚b = sum(y_pred - y_true)
   - å½¢å¼å’Œé€»è¾‘å›å½’å®Œå…¨ä¸€æ ·ï¼
   - åªæ˜¯ä»æ ‡é‡å˜æˆäº†å‘é‡/çŸ©é˜µ

7. å†³ç­–è¾¹ç•Œ
   - Softmax å›å½’æ˜¯çº¿æ€§åˆ†ç±»å™¨
   - K ä¸ªç±»åˆ«çš„è¾¹ç•Œéƒ½æ˜¯çº¿æ€§çš„
   - æ— æ³•å¤„ç†éçº¿æ€§æ•°æ®ï¼ˆéœ€è¦ç¥ç»ç½‘ç»œï¼‰

8. åº”ç”¨åœºæ™¯
   âœ“ å›¾åƒåˆ†ç±»ï¼ˆæ‰‹å†™æ•°å­—è¯†åˆ«ï¼‰
   âœ“ æ–‡æœ¬åˆ†ç±»ï¼ˆæ–°é—»ä¸»é¢˜ï¼‰
   âœ“ å¤šæ ‡ç­¾åˆ†ç±»ï¼ˆç‰©ä½“æ£€æµ‹ï¼‰
   âœ“ ä»»ä½•å¤šåˆ†ç±»é—®é¢˜

9. ä¸ç¥ç»ç½‘ç»œçš„å…³ç³»
   - Softmax å›å½’ = å•å±‚ç¥ç»ç½‘ç»œ + Softmax è¾“å‡º
   - æ·±åº¦ç¥ç»ç½‘ç»œçš„æœ€åä¸€å±‚é€šå¸¸ç”¨ Softmax
   - ç†è§£ Softmax æ˜¯ç†è§£åˆ†ç±»ç½‘ç»œçš„åŸºç¡€
    """)

    print("=" * 70)
    print("ğŸ¯ ä¸‹ä¸€æ­¥å­¦ä¹ å»ºè®®")
    print("=" * 70)
    print("""
1. æ­£åˆ™åŒ–ï¼ˆL1/L2 Regularizationï¼‰
   - é˜²æ­¢è¿‡æ‹Ÿåˆ
   - ç‰¹å¾é€‰æ‹©
   - å¢å¼ºæ³›åŒ–èƒ½åŠ›

2. ç¥ç»ç½‘ç»œåŸºç¡€ï¼ˆå¤šå±‚æ„ŸçŸ¥æœº MLPï¼‰
   - å¤šå±‚ç»“æ„
   - éçº¿æ€§æ¿€æ´»å‡½æ•°
   - åå‘ä¼ æ’­ç®—æ³•

3. è¯„ä¼°æŒ‡æ ‡æ·±å…¥
   - æ··æ·†çŸ©é˜µï¼ˆå¤šåˆ†ç±»ï¼‰
   - Macro/Micro å¹³å‡
   - ROC æ›²çº¿ï¼ˆå¤šåˆ†ç±»ç‰ˆæœ¬ï¼‰

4. ä¼˜åŒ–ç®—æ³•
   - Momentum
   - Adam
   - å­¦ä¹ ç‡è°ƒåº¦
    """)


if __name__ == "__main__":
    # ä¸»å®éªŒ
    main()

    # é¢å¤–å®éªŒï¼ˆå–æ¶ˆæ³¨é‡Šè¿è¡Œï¼‰
    experiment_num_classes()

    # æ€»ç»“
    print_summary()

    print("\nğŸ’¡ ç»ƒä¹ å»ºè®®ï¼š")
    print("  1. å°è¯• 4 ç±»æˆ– 5 ç±»åˆ†ç±»ï¼Œè§‚å¯Ÿå‡†ç¡®ç‡å˜åŒ–")
    print("  2. ä¿®æ”¹å­¦ä¹ ç‡ï¼Œçœ‹å¯¹è®­ç»ƒçš„å½±å“")
    print("  3. å¯¹æ¯”ä¸åŒ batch size çš„æ•ˆæœ")
    print("  4. æ€è€ƒï¼šä¸ºä»€ä¹ˆå‚æ•°æ•°é‡æ˜¯ (n_features + 1) Ã— n_classesï¼Ÿ")
