# 性能评估 (Performance Evaluation)

> 如何科学地衡量和优化计算机性能

## 🎯 核心概念

**性能 = 完成任务的速度**

```
关键问题：
  • 如何定义性能？
  • 如何测量性能？
  • 如何比较不同系统？
  • 优化的上限在哪里？
```

---

## 1️⃣ 性能指标

### 执行时间 (Execution Time)

**最直接的性能指标：完成任务需要多长时间**

```
性能 = 1 / 执行时间

示例：
  程序 A: 10 秒
  程序 B: 5 秒

  B 的性能是 A 的: 10/5 = 2 倍
```

### 两种执行时间

#### 1. 墙上时间 (Wall-Clock Time / Elapsed Time)
```
从程序开始到结束的总时间

包括：
  • CPU 执行时间
  • I/O 等待时间
  • 其他进程占用时间
  • 操作系统开销

测量：
  time ./program
  → real 0m10.234s  ← 这是墙上时间
```

#### 2. CPU 时间 (CPU Time)
```
CPU 实际执行该程序的时间

分为：
  • 用户 CPU 时间：执行用户代码
  • 系统 CPU 时间：执行系统调用

测量：
  time ./program
  → user 0m8.123s   ← 用户 CPU 时间
  → sys  0m0.456s   ← 系统 CPU 时间
```

**什么时候用哪个？**
```
CPU 时间：
  ✅ 评估程序本身性能
  ✅ 比较算法效率
  ✅ 编译器优化效果

墙上时间：
  ✅ 用户体验（真实感受）
  ✅ 系统整体性能
  ✅ 包含 I/O 的应用
```

---

## 2️⃣ 性能方程

### CPU 时间的组成

```
CPU 时间 = 指令数 × CPI × 时钟周期

其中：
  • 指令数 (Instruction Count, IC)
      → 程序执行了多少条指令
      → 受算法和编译器影响

  • CPI (Cycles Per Instruction)
      → 每条指令平均需要多少时钟周期
      → 受 CPU 架构和指令类型影响

  • 时钟周期 (Clock Cycle Time)
      → 每个周期的时间 = 1 / 频率
      → 受 CPU 频率影响
```

### 详细推导

```
CPU 时间 = 指令数 × CPI × 时钟周期时间

因为：时钟周期时间 = 1 / CPU 频率

所以：CPU 时间 = (指令数 × CPI) / CPU 频率

CPU 时钟周期数 = 指令数 × CPI

因此：CPU 时间 = CPU 时钟周期数 / CPU 频率
```

### 实际例子

```
程序 A:
  • 指令数: 100 亿条
  • CPI: 2.0
  • CPU 频率: 2 GHz

CPU 时间 = (100 × 10^9) × 2.0 / (2 × 10^9)
         = 100 秒

优化选项：
  1. 减少指令数 → 更好的算法、编译器优化
  2. 降低 CPI → 更好的 CPU 架构、减少 Cache miss
  3. 提高频率 → 更先进的制程（但有功耗墙）
```

---

## 3️⃣ CPI 详解

### CPI 不是常数！

**不同指令的 CPI 不同**

```
典型指令 CPI（简化流水线）:
┌───────────────┬────────┐
│  指令类型     │  CPI   │
├───────────────┼────────┤
│ ALU 指令      │   1    │
│ Load 指令     │   2    │  (需要访存)
│ Store 指令    │   2    │
│ 分支指令      │ 1-4    │  (取决于预测)
│ 浮点乘法      │   4    │
│ 浮点除法      │  20    │
└───────────────┴────────┘
```

### 平均 CPI 计算

```
平均 CPI = Σ (CPI_i × 指令比例_i)

示例：
  ALU 指令: 50%, CPI = 1
  Load:     20%, CPI = 2
  Store:    15%, CPI = 2
  分支:     15%, CPI = 2

平均 CPI = 0.5×1 + 0.2×2 + 0.15×2 + 0.15×2
         = 0.5 + 0.4 + 0.3 + 0.3
         = 1.5
```

### 影响 CPI 的因素

```
1. 流水线停顿 (Stall):
   • 数据冒险 → +1-3 cycles
   • 控制冒险 → +2-4 cycles
   • 结构冒险 → +1 cycle

2. Cache 缺失:
   • L1 miss → +10 cycles
   • L2 miss → +50 cycles
   • L3 miss → +200 cycles

3. 分支预测失败:
   • 每次失败 → +10-20 cycles

4. TLB 缺失:
   • 页表查询 → +100 cycles
```

---

## 4️⃣ Amdahl 定律

### 加速比的上限

**核心思想：程序的总加速比受限于不能并行化的部分**

```
加速比 = 优化前时间 / 优化后时间

Amdahl 定律：
         1
S = ────────────────
    (1-P) + P/N

其中：
  S = 总加速比
  P = 可并行部分占比
  N = 并行度（核心数）
  (1-P) = 串行部分占比
```

### 图形理解

```
原始程序执行时间：
┌─────────────────────────────────────────────┐
│ 串行部分(20%) │  并行部分(80%)             │
└───────────────┴─────────────────────────────┘

4核并行后：
┌───────────────┬───────────┐
│ 串行部分(20%) │并行(80/4) │
└───────────────┴───────────┘

加速比 = 1 / (0.2 + 0.8/4) = 1 / 0.4 = 2.5 倍

理论最大加速比（无限核心）：
  lim (N→∞) S = 1 / (1-P) = 1 / 0.2 = 5 倍
```

### 实际例子

```
Web 服务器：
  • 串行部分：请求解析、日志记录 (10%)
  • 并行部分：处理请求 (90%)

核心数   加速比    效率
──────────────────────────
  1      1.00     100%
  2      1.82      91%
  4      3.08      77%
  8      4.71      59%
 16      6.40      40%
 32      7.53      24%
 ∞     10.00      ---

结论：即使 90% 可并行，32核也只有 7.5 倍加速！
```

### Amdahl 定律的启示

```
1. 优化热点代码最重要
   • 优化占 1% 的代码 → 最多 1% 提升
   • 优化占 80% 的代码 → 可能 80% 提升

2. 并行不是万能的
   • 串行部分限制了加速比
   • 需要减少串行部分

3. 多核收益递减
   • 从 1 核到 2 核：接近 2 倍
   • 从 16 核到 32 核：收益很小
```

---

## 5️⃣ 性能测试方法

### 基准测试 (Benchmarks)

#### 1. SPEC CPU
```
标准性能评估公司 (SPEC) 的 CPU 基准测试

SPEC CPU 2017:
  • SPECspeed: 单线程性能
  • SPECrate: 多线程吞吐量

包含实际应用：
  • 编译器 (gcc)
  • 数据库
  • 视频编码
  • 科学计算
```

#### 2. CoreMark
```
嵌入式 CPU 基准测试

测试内容：
  • 列表操作
  • 矩阵运算
  • 状态机
  • CRC 计算

结果：CoreMark/MHz
  → 归一化到频率，比较架构效率
```

#### 3. LINPACK
```
高性能计算基准测试（超级计算机排名）

测试：求解线性方程组
结果：FLOPS (浮点运算次数/秒)

Top500 超算排名就用 LINPACK
```

### 微基准测试 (Microbenchmarks)

**测试特定硬件特性**

```c
// 测量 L1 Cache 延迟
void measure_l1_latency() {
    int arr[1024];  // 适合 L1
    for (int i = 0; i < 1000000; i++) {
        int idx = (i * 64) % 1024;  // 随机访问
        sum += arr[idx];
    }
}

// 测量内存带宽
void measure_memory_bandwidth() {
    int *arr = malloc(100 * 1024 * 1024);  // 100MB
    for (int i = 0; i < 100*1024*1024; i++) {
        arr[i] = i;  // 顺序写入
    }
}
```

### 真实应用测试

```
系统性能：
  • 编译 Linux 内核时间
  • 数据库 TPS (Transactions Per Second)
  • 游戏帧率 (FPS)

Web 服务器：
  • QPS (Queries Per Second)
  • 响应延迟 (Latency)
  • P99 延迟（99% 请求的延迟）
```

---

## 6️⃣ 性能分析工具

### Linux perf

```bash
# 统计基本性能指标
perf stat ./program

Performance counter stats:
  10234.56 msec task-clock        # 0.998 CPUs utilized
  1,234,567,890 cycles            # 2.4 GHz
  2,345,678,901 instructions      # 1.9 IPC
  123,456,789 cache-misses        # 5% miss rate
  12,345,678 branch-misses        # 2% of all branches

# 采样分析热点函数
perf record ./program
perf report

# 查看 CPU 流水线效率
perf stat -e cycles,instructions,stalled-cycles-frontend,stalled-cycles-backend ./program
```

### Intel VTune

```
性能分析套件：
  • Hotspots: 热点函数分析
  • Memory Access: 内存访问分析
  • Microarchitecture: CPU 流水线分析
  • Threading: 多线程效率分析

可视化：
  • 时间线视图
  • 自上而下分析 (Top-Down)
  • 汇编级分析
```

### 自上而下分析方法 (Top-Down Analysis)

```
Intel 的 CPU 性能分析方法：

Level 1: 程序在哪里花时间？
  • Retiring (有效工作): 40%
  • Bad Speculation (分支预测错误): 10%
  • Frontend Bound (取指/译码瓶颈): 20%
  • Backend Bound (执行瓶颈): 30%

Level 2: Backend Bound 细分
  • Memory Bound (内存瓶颈): 25%
      → L1 Bound
      → L2 Bound
      → L3 Bound / DRAM Bound
  • Core Bound (执行单元瓶颈): 5%

逐层深入，定位瓶颈！
```

---

## 7️⃣ 性能优化策略

### 优化优先级

```
1. 算法优化
   O(n²) → O(n log n)
   可能带来 10-1000 倍提升！

2. 数据结构优化
   链表 → 数组（Cache 友好）
   可能带来 2-10 倍提升

3. 编译器优化
   -O0 → -O3
   可能带来 2-5 倍提升

4. 多线程并行
   单核 → 多核
   理想 N 倍，实际 0.5N-0.8N 倍

5. SIMD 优化
   标量 → 向量化
   理想 4-8 倍，实际 2-4 倍

6. 微架构优化
   Cache 对齐、减少分支
   可能带来 10-30% 提升
```

### 优化案例

#### 案例 1：矩阵乘法

```c
// 版本 1：朴素实现（慢）
for (int i = 0; i < N; i++)
    for (int j = 0; j < N; j++)
        for (int k = 0; k < N; k++)
            C[i][j] += A[i][k] * B[k][j];

性能：1000×1000 矩阵，2.5 秒

// 版本 2：循环交换（Cache 友好）
for (int i = 0; i < N; i++)
    for (int k = 0; k < N; k++)
        for (int j = 0; j < N; j++)
            C[i][j] += A[i][k] * B[k][j];

性能：0.8 秒（3 倍提升）

// 版本 3：分块 (Tiling / Loop Blocking)
for (int ii = 0; ii < N; ii += BLOCK)
    for (int jj = 0; jj < N; jj += BLOCK)
        for (int kk = 0; kk < N; kk += BLOCK)
            for (int i = ii; i < ii+BLOCK; i++)
                for (int j = jj; j < jj+BLOCK; j++)
                    for (int k = kk; k < kk+BLOCK; k++)
                        C[i][j] += A[i][k] * B[k][j];

性能：0.3 秒（8 倍提升）

// 版本 4：BLAS 库（高度优化）
cblas_dgemm(..., A, B, C);

性能：0.05 秒（50 倍提升！）
```

**分块优化（Loop Tiling）原理详解：**

```
问题核心：大矩阵数据无法全部放入 Cache

示例：1000×1000 矩阵（每个元素 4 字节 = 4MB）
  • L1 Cache: ~32KB  → 只能放 8×8 个元素的子矩阵
  • L2 Cache: ~256KB → 只能放 256×256 的子矩阵
  • L3 Cache: ~8MB   → 可以放两个完整矩阵

原始版本的问题：
┌─────────────────────────────────────────────┐
│ for i in [0..1000):                         │
│   for j in [0..1000):                       │
│     for k in [0..1000):                     │
│       C[i][j] += A[i][k] * B[k][j]          │
│                                             │
│ 访问模式：                                  │
│   A[i][k]: 按行访问（顺序，Cache 友好）     │
│   B[k][j]: 按列访问（跳跃，Cache miss 多）  │
│                                             │
│ 每次访问 B[k][j]，可能跨越多个 Cache line   │
│ → Cache miss 率高 → 频繁从内存读取         │
└─────────────────────────────────────────────┘

分块版本（BLOCK = 64）：
┌─────────────────────────────────────────────┐
│ 把矩阵分成 64×64 的小块                     │
│                                             │
│ 矩阵 A:           矩阵 B:                   │
│ ┌──┬──┬──┬──┐    ┌──┬──┬──┬──┐            │
│ │A0│A1│A2│A3│    │B0│B1│B2│B3│            │
│ ├──┼──┼──┼──┤    ├──┼──┼──┼──┤            │
│ │A4│A5│A6│A7│    │B4│B5│B6│B7│            │
│ └──┴──┴──┴──┘    └──┴──┴──┴──┘            │
│                                             │
│ 每个块 64×64×4 = 16KB                       │
│ → 可以完整放入 L1 Cache (32KB)             │
│                                             │
│ 计算 C 的一个块时：                         │
│   1. 加载 A 的一个块到 Cache                │
│   2. 加载 B 的一个块到 Cache                │
│   3. 在 Cache 中完成所有计算                │
│   4. 写回 C 的结果                          │
│                                             │
│ 关键：块内计算时，数据都在 Cache 中！       │
│ → Cache miss 率大幅降低                     │
└─────────────────────────────────────────────┘

类比理解：
  • 原始方法：在仓库（内存）和工作台（CPU）之间
              每需要一个零件就跑一次仓库
              → 大量时间浪费在路上

  • 分块方法：一次从仓库搬一箱零件到工作台（Cache）
              用完这箱再去搬下一箱
              → 减少往返次数，工作效率高

性能提升原因：
  • Cache miss 率：从 30% 降到 5%
  • 内存访问次数：减少 6 倍
  • 数据重用：同一数据在 Cache 中被多次使用
```

**BLAS 库为何能达到 50 倍性能提升？**

```
BLAS (Basic Linear Algebra Subprograms) 不仅用了分块，
还采用了多层优化：

1. 多级分块 (Multi-level Tiling)
   • L1 分块: 32KB 块
   • L2 分块: 256KB 块
   • L3 分块: 8MB 块
   → 充分利用各级 Cache

2. SIMD 向量化
   • AVX2: 一次处理 8 个 float
   • AVX-512: 一次处理 16 个 float
   → 指令级并行

3. 多线程并行
   • 利用所有 CPU 核心
   • 细粒度任务分配
   → 线程级并行

4. 汇编级优化
   • 手写关键路径汇编
   • 寄存器分配优化
   • 指令流水线优化

5. 针对特定 CPU 优化
   • Intel MKL: 为 Intel CPU 优化
   • OpenBLAS: 通用优化
   • 运行时检测 CPU 特性

6. 预取 (Prefetch)
   • 提前加载下一块数据
   • 隐藏内存延迟

7. 减少 TLB miss
   • 使用大页 (Huge Pages)
   • 优化内存布局

综合效果：
  • 理论峰值性能: 假设 2.4 GHz × 8 SIMD × 2 (FMA) = 38.4 GFLOPS
  • BLAS 实际性能: ~30 GFLOPS (80% 峰值)
  • 朴素实现: ~0.8 GFLOPS (2% 峰值)
  → 接近 50 倍差距！

这就是为什么：
  • 数值计算要用 NumPy/SciPy（底层用 BLAS）
  • 机器学习用 cuBLAS（GPU 版）
  • 自己写很难达到库的性能
```

#### 案例 2：排序后处理

```c
// 问题：处理满足条件的元素
int data[N];
for (int i = 0; i < N; i++) {
    if (data[i] < threshold) {  // 不可预测分支
        process(data[i]);
    }
}

// 优化：先排序
sort(data, N);
int split = lower_bound(data, threshold);
for (int i = 0; i < split; i++) {
    process(data[i]);  // 没有分支！
}

性能提升：
  • 未排序：分支预测 50%，性能差
  • 排序后：分支预测 100%，性能好
  • 即使加上排序时间，仍然更快！
```

---

## 8️⃣ 性能权衡

### 能效比 (Performance per Watt)

```
性能不是唯一指标，功耗也很重要

桌面 CPU (Intel i9):
  • 性能: 10,000 分
  • 功耗: 125W
  • 能效: 80 分/瓦

移动 CPU (Apple M2):
  • 性能: 8,000 分
  • 功耗: 20W
  • 能效: 400 分/瓦

M2 能效是 i9 的 5 倍！
```

### 成本与性能

```
服务器选择：

方案 A: 高端 CPU
  • 单核性能: 100
  • 核心数: 8
  • 总性能: 800
  • 价格: $1000
  • 性价比: 0.8 性能/美元

方案 B: 中端 CPU
  • 单核性能: 60
  • 核心数: 16
  • 总性能: 960
  • 价格: $600
  • 性价比: 1.6 性能/美元

对并行应用，方案 B 更优！
```

---

## 9️⃣ 实际应用指南

### 性能分析流程

```
1. 建立基准
   → 测量当前性能
   → 记录指标（时间、吞吐量等）

2. 定位瓶颈
   → 使用 profiler 找热点
   → 分析性能计数器

3. 提出假设
   → 瓶颈是 Cache miss？
   → 还是分支预测？

4. 实施优化
   → 修改代码
   → 保持可读性

5. 验证效果
   → 重新测量
   → 确保正确性

6. 重复
   → 优化下一个瓶颈
```

### 性能调优原则

```
1. 先测量，再优化
   ❌ "这里应该慢" → ✅ "profiler 显示这里慢"

2. 优化热点
   80/20 法则：80% 时间花在 20% 代码上

3. 保持简单
   可读性 > 微小的性能提升

4. 编译器很聪明
   简单清晰的代码，编译器更容易优化

5. 实测为准
   理论分析可能错误，实测才是真理
```

### 常见性能陷阱

```
❌ 过早优化
   → 先保证正确性，再优化

❌ 微优化
   → 优化不重要的部分（占 1% 的代码）

❌ 忽略 I/O
   → CPU 再快，等磁盘也没用

❌ 盲目并行
   → 串行部分、同步开销限制加速比

❌ 忽略真实工作负载
   → 合成基准测试 ≠ 真实应用
```

---

## 🔗 与其他概念的联系

### 与算法
- **时间复杂度** - 决定指令数的增长率
- **空间复杂度** - 影响 Cache 性能

参考：`fundamentals/algorithms/complexity-analysis.md`

### 与操作系统
- **进程调度** - 影响多核性能
- **虚拟内存** - TLB 和页表开销

参考：`systems/operating-systems/`

### 与编译器
- **优化级别** - 影响指令数和 CPI
- **向量化** - 提升 SIMD 性能

---

## 📚 深入学习

### 推荐资源
- *Computer Architecture: A Quantitative Approach* - 第1章、第4章
- Brendan Gregg 的性能分析书籍
- Agner Fog 的优化手册

### 实践项目
- 优化实际应用（如图像处理、矩阵运算）
- 分析开源软件性能瓶颈
- 编写性能测试工具

### 性能优化检查清单

```
□ 算法是否最优？
□ 数据结构是否 Cache 友好？
□ 是否有不必要的内存分配？
□ 循环是否可以展开/向量化？
□ 分支是否可预测？
□ 是否利用了多核？
□ 是否避免了伪共享？
□ I/O 是否异步？
□ 是否使用了优化库（BLAS、FFT等）？
□ 编译器优化是否开启？
```

---

**性能优化是科学，不是艺术 - 用数据说话！** 📊
